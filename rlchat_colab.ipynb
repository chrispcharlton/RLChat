{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rlchat_colab.ipynb","provenance":[{"file_id":"1iwozjyZP19mzhaqK53YFohTDsudLIc6u","timestamp":1570505285383}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"O_22D4xVakP2","colab_type":"text"},"source":["\n","\n","1. Ensure in GPU runtime -> Edit -> Notebook settings\n","2. Mount your own gdrive\n","3. cd to root of proj\n","4. run.py file at project root with args  \n","\n"]},{"cell_type":"code","metadata":{"id":"6idAjO9yZEVf","colab_type":"code","outputId":"3e4b0282-ae24-41a5-d7d9-7e9bd2069ace","executionInfo":{"status":"ok","timestamp":1572146790986,"user_tz":-780,"elapsed":2788,"user":{"displayName":"Dylan Webb","photoUrl":"","userId":"17339420517359398809"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ip7F5_y3ZLbZ","colab_type":"code","outputId":"8a719dbf-27bd-48f8-ff3a-ae19655c8a0f","executionInfo":{"status":"ok","timestamp":1572146792485,"user_tz":-780,"elapsed":631,"user":{"displayName":"Dylan Webb","photoUrl":"","userId":"17339420517359398809"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Per where your project is in gdrive\n","%cd /content/gdrive/'My Drive'/RLChat \n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/RLChat\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1FsnVZP-Zfjj","colab_type":"code","outputId":"c4e9b159-7a94-47be-c7e5-62188895e426","executionInfo":{"status":"ok","timestamp":1571556705343,"user_tz":-780,"elapsed":3026736,"user":{"displayName":"Chester Holt","photoUrl":"","userId":"05242144140561707601"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run.py train seq2seq"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Added valid_freq.json to dataset\n","Added valid_rare.json to dataset\n","Added test_freq.json to dataset\n","Added train.json to dataset\n","Added test_rare.json to dataset\n","0 pairs trimmed, 158603 remain\n","Start preparing training data ...\n","Counting words...\n","Counted words: 41279\n","input_variable: tensor([[  470,   174,   970,    47,    13,     4,    49, 17098,   452,  3879,\n","          2286,   134,    53, 22827, 10473,     8,   352,  4531,     2],\n","        [  380,    10,    15,    24,    86,   315,    10,    14,    12,    11,\n","          1030,    32,  1050,  1404,    16, 10060,    47,     2,     0],\n","        [  672,    10,    89,     3,     4,     8,    28,     4,    49,   269,\n","            50,    16,  7985,     8,     2,     0,     0,     0,     0],\n","        [  233,    10,  4843,  9540,    42,  3125,    12,  1354,    24,   167,\n","           157,    15,    12,     2,     0,     0,     0,     0,     0],\n","        [  128,  1038,    32,  1065,    16,  7103,     3,  2871,    74,    16,\n","          1078,    12,     2,     0,     0,     0,     0,     0,     0]],\n","       device='cuda:0')\n","lengths: tensor([19, 18, 15, 14, 13], device='cuda:0')\n","target_variable: tensor([[  216,   517,   332,    27,    27],\n","        [    8,   243,  2645,   120,   135],\n","        [   15,    32,    27,   121,   969],\n","        [   24,   188,   334, 21451,    15],\n","        [  315,   333,   174,  2583,   281],\n","        [   42,    35,    12,   135,    27],\n","        [  242,  2102,    27,   165,   284],\n","        [  216,    37,   334,     5,   112],\n","        [ 2299,    28,     5, 10718,   512],\n","        [  153,   106,  7806,   715,   872],\n","        [   16,  3504,  1260,    10,   142],\n","        [  402,     5,    12,   517,     4],\n","        [   72,  5852,    70,    13,   216],\n","        [   10,  2065,    50,  1161,   354],\n","        [  360, 39304,     4, 21560,   153],\n","        [  250,     2,     8,   289,    16],\n","        [  200,     0,     2,    74,  8573],\n","        [  421,     0,     0,   441,    18],\n","        [  300,     0,     0,    68,   112],\n","        [    4,     0,     0,   162,    12],\n","        [ 1001,     0,     0,     2,     2],\n","        [   12,     0,     0,     0,     0],\n","        [   34,     0,     0,     0,     0],\n","        [    2,     0,     0,     0,     0]], device='cuda:0')\n","mask: tensor([[ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True, False,  True,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False,  True,  True],\n","        [ True, False, False, False, False],\n","        [ True, False, False, False, False],\n","        [ True, False, False, False, False]], device='cuda:0')\n","max_target_len: 24\n","Building encoder and decoder ...\n","Models built and ready to go!\n","Building optimizers ...\n","Starting Training!\n","Initializing ...\n","Training...\n","Iteration: 1; Percent complete: 0.0%; Average loss: 10.6270\n","Iteration: 2; Percent complete: 0.1%; Average loss: 10.5874\n","Iteration: 3; Percent complete: 0.1%; Average loss: 10.5424\n","Iteration: 4; Percent complete: 0.1%; Average loss: 10.4502\n","Iteration: 5; Percent complete: 0.1%; Average loss: 10.3269\n","Iteration: 6; Percent complete: 0.1%; Average loss: 10.0703\n","Iteration: 7; Percent complete: 0.2%; Average loss: 9.7815\n","Iteration: 8; Percent complete: 0.2%; Average loss: 9.2324\n","Iteration: 9; Percent complete: 0.2%; Average loss: 8.8191\n","Iteration: 10; Percent complete: 0.2%; Average loss: 8.5407\n","Iteration: 11; Percent complete: 0.3%; Average loss: 8.2637\n","Iteration: 12; Percent complete: 0.3%; Average loss: 7.9708\n","Iteration: 13; Percent complete: 0.3%; Average loss: 7.8101\n","Iteration: 14; Percent complete: 0.4%; Average loss: 7.4876\n","Iteration: 15; Percent complete: 0.4%; Average loss: 7.2619\n","Iteration: 16; Percent complete: 0.4%; Average loss: 7.0666\n","Iteration: 17; Percent complete: 0.4%; Average loss: 7.0057\n","Iteration: 18; Percent complete: 0.4%; Average loss: 6.8790\n","Iteration: 19; Percent complete: 0.5%; Average loss: 6.5826\n","Iteration: 20; Percent complete: 0.5%; Average loss: 6.6312\n","Iteration: 21; Percent complete: 0.5%; Average loss: 6.4650\n","Iteration: 22; Percent complete: 0.5%; Average loss: 6.5063\n","Iteration: 23; Percent complete: 0.6%; Average loss: 6.4356\n","Iteration: 24; Percent complete: 0.6%; Average loss: 6.3735\n","Iteration: 25; Percent complete: 0.6%; Average loss: 6.3114\n","Iteration: 26; Percent complete: 0.7%; Average loss: 6.3012\n","Iteration: 27; Percent complete: 0.7%; Average loss: 6.2791\n","Iteration: 28; Percent complete: 0.7%; Average loss: 6.3889\n","Iteration: 29; Percent complete: 0.7%; Average loss: 6.3238\n","Iteration: 30; Percent complete: 0.8%; Average loss: 6.5712\n","Iteration: 31; Percent complete: 0.8%; Average loss: 6.5964\n","Iteration: 32; Percent complete: 0.8%; Average loss: 6.5392\n","Iteration: 33; Percent complete: 0.8%; Average loss: 6.6503\n","Iteration: 34; Percent complete: 0.9%; Average loss: 6.6416\n","Iteration: 35; Percent complete: 0.9%; Average loss: 6.4711\n","Iteration: 36; Percent complete: 0.9%; Average loss: 6.5130\n","Iteration: 37; Percent complete: 0.9%; Average loss: 6.5633\n","Iteration: 38; Percent complete: 0.9%; Average loss: 6.5005\n","Iteration: 39; Percent complete: 1.0%; Average loss: 6.4635\n","Iteration: 40; Percent complete: 1.0%; Average loss: 6.4216\n","Iteration: 41; Percent complete: 1.0%; Average loss: 6.5711\n","Iteration: 42; Percent complete: 1.1%; Average loss: 6.2702\n","Iteration: 43; Percent complete: 1.1%; Average loss: 6.4853\n","Iteration: 44; Percent complete: 1.1%; Average loss: 6.4526\n","Iteration: 45; Percent complete: 1.1%; Average loss: 6.5515\n","Iteration: 46; Percent complete: 1.1%; Average loss: 6.3990\n","Iteration: 47; Percent complete: 1.2%; Average loss: 6.3516\n","Iteration: 48; Percent complete: 1.2%; Average loss: 6.4327\n","Iteration: 49; Percent complete: 1.2%; Average loss: 6.4654\n","Iteration: 50; Percent complete: 1.2%; Average loss: 6.5082\n","Iteration: 51; Percent complete: 1.3%; Average loss: 6.4761\n","Iteration: 52; Percent complete: 1.3%; Average loss: 6.6707\n","Iteration: 53; Percent complete: 1.3%; Average loss: 6.4544\n","Iteration: 54; Percent complete: 1.4%; Average loss: 6.5456\n","Iteration: 55; Percent complete: 1.4%; Average loss: 6.4657\n","Iteration: 56; Percent complete: 1.4%; Average loss: 6.4491\n","Iteration: 57; Percent complete: 1.4%; Average loss: 6.3814\n","Iteration: 58; Percent complete: 1.5%; Average loss: 6.2092\n","Iteration: 59; Percent complete: 1.5%; Average loss: 6.3718\n","Iteration: 60; Percent complete: 1.5%; Average loss: 6.3808\n","Iteration: 61; Percent complete: 1.5%; Average loss: 6.2993\n","Iteration: 62; Percent complete: 1.6%; Average loss: 6.3852\n","Iteration: 63; Percent complete: 1.6%; Average loss: 6.4233\n","Iteration: 64; Percent complete: 1.6%; Average loss: 6.2303\n","Iteration: 65; Percent complete: 1.6%; Average loss: 6.4638\n","Iteration: 66; Percent complete: 1.7%; Average loss: 6.3522\n","Iteration: 67; Percent complete: 1.7%; Average loss: 6.4499\n","Iteration: 68; Percent complete: 1.7%; Average loss: 6.3215\n","Iteration: 69; Percent complete: 1.7%; Average loss: 6.3133\n","Iteration: 70; Percent complete: 1.8%; Average loss: 6.3172\n","Iteration: 71; Percent complete: 1.8%; Average loss: 6.1649\n","Iteration: 72; Percent complete: 1.8%; Average loss: 6.3410\n","Iteration: 73; Percent complete: 1.8%; Average loss: 6.2766\n","Iteration: 74; Percent complete: 1.8%; Average loss: 6.1437\n","Iteration: 75; Percent complete: 1.9%; Average loss: 6.3599\n","Iteration: 76; Percent complete: 1.9%; Average loss: 6.2547\n","Iteration: 77; Percent complete: 1.9%; Average loss: 6.4131\n","Iteration: 78; Percent complete: 1.9%; Average loss: 6.1772\n","Iteration: 79; Percent complete: 2.0%; Average loss: 6.0994\n","Iteration: 80; Percent complete: 2.0%; Average loss: 6.3439\n","Iteration: 81; Percent complete: 2.0%; Average loss: 6.1859\n","Iteration: 82; Percent complete: 2.1%; Average loss: 6.2360\n","Iteration: 83; Percent complete: 2.1%; Average loss: 6.2712\n","Iteration: 84; Percent complete: 2.1%; Average loss: 6.1524\n","Iteration: 85; Percent complete: 2.1%; Average loss: 6.1869\n","Iteration: 86; Percent complete: 2.1%; Average loss: 6.1615\n","Iteration: 87; Percent complete: 2.2%; Average loss: 6.2685\n","Iteration: 88; Percent complete: 2.2%; Average loss: 6.2497\n","Iteration: 89; Percent complete: 2.2%; Average loss: 6.3100\n","Iteration: 90; Percent complete: 2.2%; Average loss: 6.2192\n","Iteration: 91; Percent complete: 2.3%; Average loss: 6.2512\n","Iteration: 92; Percent complete: 2.3%; Average loss: 6.2616\n","Iteration: 93; Percent complete: 2.3%; Average loss: 6.1303\n","Iteration: 94; Percent complete: 2.4%; Average loss: 6.0330\n","Iteration: 95; Percent complete: 2.4%; Average loss: 6.2882\n","Iteration: 96; Percent complete: 2.4%; Average loss: 6.1263\n","Iteration: 97; Percent complete: 2.4%; Average loss: 6.1456\n","Iteration: 98; Percent complete: 2.5%; Average loss: 6.3128\n","Iteration: 99; Percent complete: 2.5%; Average loss: 6.1888\n","Iteration: 100; Percent complete: 2.5%; Average loss: 6.1373\n","Iteration: 101; Percent complete: 2.5%; Average loss: 6.3445\n","Iteration: 102; Percent complete: 2.5%; Average loss: 6.2414\n","Iteration: 103; Percent complete: 2.6%; Average loss: 6.2658\n","Iteration: 104; Percent complete: 2.6%; Average loss: 6.2151\n","Iteration: 105; Percent complete: 2.6%; Average loss: 6.2035\n","Iteration: 106; Percent complete: 2.6%; Average loss: 6.3046\n","Iteration: 107; Percent complete: 2.7%; Average loss: 6.3151\n","Iteration: 108; Percent complete: 2.7%; Average loss: 6.0379\n","Iteration: 109; Percent complete: 2.7%; Average loss: 6.1514\n","Iteration: 110; Percent complete: 2.8%; Average loss: 5.9422\n","Iteration: 111; Percent complete: 2.8%; Average loss: 6.2322\n","Iteration: 112; Percent complete: 2.8%; Average loss: 6.1254\n","Iteration: 113; Percent complete: 2.8%; Average loss: 6.1319\n","Iteration: 114; Percent complete: 2.9%; Average loss: 6.0869\n","Iteration: 115; Percent complete: 2.9%; Average loss: 6.2717\n","Iteration: 116; Percent complete: 2.9%; Average loss: 6.1098\n","Iteration: 117; Percent complete: 2.9%; Average loss: 6.1605\n","Iteration: 118; Percent complete: 2.9%; Average loss: 6.1911\n","Iteration: 119; Percent complete: 3.0%; Average loss: 6.1573\n","Iteration: 120; Percent complete: 3.0%; Average loss: 6.2780\n","Iteration: 121; Percent complete: 3.0%; Average loss: 6.0115\n","Iteration: 122; Percent complete: 3.0%; Average loss: 6.0348\n","Iteration: 123; Percent complete: 3.1%; Average loss: 6.0453\n","Iteration: 124; Percent complete: 3.1%; Average loss: 6.0726\n","Iteration: 125; Percent complete: 3.1%; Average loss: 6.0823\n","Iteration: 126; Percent complete: 3.1%; Average loss: 6.2008\n","Iteration: 127; Percent complete: 3.2%; Average loss: 6.1262\n","Iteration: 128; Percent complete: 3.2%; Average loss: 6.2567\n","Iteration: 129; Percent complete: 3.2%; Average loss: 5.9209\n","Iteration: 130; Percent complete: 3.2%; Average loss: 6.2355\n","Iteration: 131; Percent complete: 3.3%; Average loss: 6.1024\n","Iteration: 132; Percent complete: 3.3%; Average loss: 6.0397\n","Iteration: 133; Percent complete: 3.3%; Average loss: 6.0687\n","Iteration: 134; Percent complete: 3.4%; Average loss: 5.9793\n","Iteration: 135; Percent complete: 3.4%; Average loss: 6.1123\n","Iteration: 136; Percent complete: 3.4%; Average loss: 6.0515\n","Iteration: 137; Percent complete: 3.4%; Average loss: 6.0748\n","Iteration: 138; Percent complete: 3.5%; Average loss: 6.1093\n","Iteration: 139; Percent complete: 3.5%; Average loss: 6.0147\n","Iteration: 140; Percent complete: 3.5%; Average loss: 6.0294\n","Iteration: 141; Percent complete: 3.5%; Average loss: 6.1140\n","Iteration: 142; Percent complete: 3.5%; Average loss: 6.0362\n","Iteration: 143; Percent complete: 3.6%; Average loss: 6.1379\n","Iteration: 144; Percent complete: 3.6%; Average loss: 6.1738\n","Iteration: 145; Percent complete: 3.6%; Average loss: 6.0330\n","Iteration: 146; Percent complete: 3.6%; Average loss: 6.1958\n","Iteration: 147; Percent complete: 3.7%; Average loss: 6.0055\n","Iteration: 148; Percent complete: 3.7%; Average loss: 6.2304\n","Iteration: 149; Percent complete: 3.7%; Average loss: 6.1151\n","Iteration: 150; Percent complete: 3.8%; Average loss: 6.0464\n","Iteration: 151; Percent complete: 3.8%; Average loss: 6.1510\n","Iteration: 152; Percent complete: 3.8%; Average loss: 6.1687\n","Iteration: 153; Percent complete: 3.8%; Average loss: 5.9133\n","Iteration: 154; Percent complete: 3.9%; Average loss: 6.0326\n","Iteration: 155; Percent complete: 3.9%; Average loss: 6.2020\n","Iteration: 156; Percent complete: 3.9%; Average loss: 5.9355\n","Iteration: 157; Percent complete: 3.9%; Average loss: 5.9105\n","Iteration: 158; Percent complete: 4.0%; Average loss: 6.2028\n","Iteration: 159; Percent complete: 4.0%; Average loss: 6.1247\n","Iteration: 160; Percent complete: 4.0%; Average loss: 5.9710\n","Iteration: 161; Percent complete: 4.0%; Average loss: 6.0928\n","Iteration: 162; Percent complete: 4.0%; Average loss: 6.0359\n","Iteration: 163; Percent complete: 4.1%; Average loss: 5.9419\n","Iteration: 164; Percent complete: 4.1%; Average loss: 6.0213\n","Iteration: 165; Percent complete: 4.1%; Average loss: 6.1119\n","Iteration: 166; Percent complete: 4.2%; Average loss: 6.1796\n","Iteration: 167; Percent complete: 4.2%; Average loss: 6.0673\n","Iteration: 168; Percent complete: 4.2%; Average loss: 5.9830\n","Iteration: 169; Percent complete: 4.2%; Average loss: 5.9098\n","Iteration: 170; Percent complete: 4.2%; Average loss: 6.1211\n","Iteration: 171; Percent complete: 4.3%; Average loss: 6.1184\n","Iteration: 172; Percent complete: 4.3%; Average loss: 6.0511\n","Iteration: 173; Percent complete: 4.3%; Average loss: 5.9743\n","Iteration: 174; Percent complete: 4.3%; Average loss: 5.9277\n","Iteration: 175; Percent complete: 4.4%; Average loss: 5.9267\n","Iteration: 176; Percent complete: 4.4%; Average loss: 6.0194\n","Iteration: 177; Percent complete: 4.4%; Average loss: 5.7859\n","Iteration: 178; Percent complete: 4.5%; Average loss: 5.8777\n","Iteration: 179; Percent complete: 4.5%; Average loss: 5.9561\n","Iteration: 180; Percent complete: 4.5%; Average loss: 5.9120\n","Iteration: 181; Percent complete: 4.5%; Average loss: 5.8969\n","Iteration: 182; Percent complete: 4.5%; Average loss: 5.9341\n","Iteration: 183; Percent complete: 4.6%; Average loss: 5.9203\n","Iteration: 184; Percent complete: 4.6%; Average loss: 5.9457\n","Iteration: 185; Percent complete: 4.6%; Average loss: 5.9926\n","Iteration: 186; Percent complete: 4.7%; Average loss: 5.9002\n","Iteration: 187; Percent complete: 4.7%; Average loss: 6.0430\n","Iteration: 188; Percent complete: 4.7%; Average loss: 5.8240\n","Iteration: 189; Percent complete: 4.7%; Average loss: 5.9106\n","Iteration: 190; Percent complete: 4.8%; Average loss: 5.9404\n","Iteration: 191; Percent complete: 4.8%; Average loss: 5.8439\n","Iteration: 192; Percent complete: 4.8%; Average loss: 5.8769\n","Iteration: 193; Percent complete: 4.8%; Average loss: 6.1780\n","Iteration: 194; Percent complete: 4.9%; Average loss: 6.0171\n","Iteration: 195; Percent complete: 4.9%; Average loss: 6.0010\n","Iteration: 196; Percent complete: 4.9%; Average loss: 5.8967\n","Iteration: 197; Percent complete: 4.9%; Average loss: 5.7757\n","Iteration: 198; Percent complete: 5.0%; Average loss: 5.7294\n","Iteration: 199; Percent complete: 5.0%; Average loss: 5.7885\n","Iteration: 200; Percent complete: 5.0%; Average loss: 5.7987\n","Iteration: 201; Percent complete: 5.0%; Average loss: 5.8187\n","Iteration: 202; Percent complete: 5.1%; Average loss: 5.8464\n","Iteration: 203; Percent complete: 5.1%; Average loss: 5.7659\n","Iteration: 204; Percent complete: 5.1%; Average loss: 5.9815\n","Iteration: 205; Percent complete: 5.1%; Average loss: 5.8610\n","Iteration: 206; Percent complete: 5.1%; Average loss: 5.8383\n","Iteration: 207; Percent complete: 5.2%; Average loss: 5.7520\n","Iteration: 208; Percent complete: 5.2%; Average loss: 5.8306\n","Iteration: 209; Percent complete: 5.2%; Average loss: 5.8304\n","Iteration: 210; Percent complete: 5.2%; Average loss: 5.8507\n","Iteration: 211; Percent complete: 5.3%; Average loss: 5.7148\n","Iteration: 212; Percent complete: 5.3%; Average loss: 5.7260\n","Iteration: 213; Percent complete: 5.3%; Average loss: 5.8463\n","Iteration: 214; Percent complete: 5.3%; Average loss: 5.8402\n","Iteration: 215; Percent complete: 5.4%; Average loss: 5.7989\n","Iteration: 216; Percent complete: 5.4%; Average loss: 5.9300\n","Iteration: 217; Percent complete: 5.4%; Average loss: 5.9589\n","Iteration: 218; Percent complete: 5.5%; Average loss: 5.8273\n","Iteration: 219; Percent complete: 5.5%; Average loss: 5.8406\n","Iteration: 220; Percent complete: 5.5%; Average loss: 5.9153\n","Iteration: 221; Percent complete: 5.5%; Average loss: 5.8815\n","Iteration: 222; Percent complete: 5.5%; Average loss: 5.8513\n","Iteration: 223; Percent complete: 5.6%; Average loss: 5.8576\n","Iteration: 224; Percent complete: 5.6%; Average loss: 5.7144\n","Iteration: 225; Percent complete: 5.6%; Average loss: 5.8372\n","Iteration: 226; Percent complete: 5.7%; Average loss: 5.8797\n","Iteration: 227; Percent complete: 5.7%; Average loss: 5.6945\n","Iteration: 228; Percent complete: 5.7%; Average loss: 5.7017\n","Iteration: 229; Percent complete: 5.7%; Average loss: 5.9231\n","Iteration: 230; Percent complete: 5.8%; Average loss: 5.7915\n","Iteration: 231; Percent complete: 5.8%; Average loss: 5.8757\n","Iteration: 232; Percent complete: 5.8%; Average loss: 5.8267\n","Iteration: 233; Percent complete: 5.8%; Average loss: 5.6501\n","Iteration: 234; Percent complete: 5.9%; Average loss: 5.7313\n","Iteration: 235; Percent complete: 5.9%; Average loss: 5.7354\n","Iteration: 236; Percent complete: 5.9%; Average loss: 5.7233\n","Iteration: 237; Percent complete: 5.9%; Average loss: 5.5653\n","Iteration: 238; Percent complete: 5.9%; Average loss: 5.7671\n","Iteration: 239; Percent complete: 6.0%; Average loss: 6.0435\n","Iteration: 240; Percent complete: 6.0%; Average loss: 5.8440\n","Iteration: 241; Percent complete: 6.0%; Average loss: 5.6604\n","Iteration: 242; Percent complete: 6.0%; Average loss: 5.5375\n","Iteration: 243; Percent complete: 6.1%; Average loss: 5.5164\n","Iteration: 244; Percent complete: 6.1%; Average loss: 5.8678\n","Iteration: 245; Percent complete: 6.1%; Average loss: 5.6450\n","Iteration: 246; Percent complete: 6.2%; Average loss: 5.4908\n","Iteration: 247; Percent complete: 6.2%; Average loss: 5.7757\n","Iteration: 248; Percent complete: 6.2%; Average loss: 5.8680\n","Iteration: 249; Percent complete: 6.2%; Average loss: 5.6162\n","Iteration: 250; Percent complete: 6.2%; Average loss: 5.7659\n","Iteration: 251; Percent complete: 6.3%; Average loss: 5.6521\n","Iteration: 252; Percent complete: 6.3%; Average loss: 5.7438\n","Iteration: 253; Percent complete: 6.3%; Average loss: 5.7673\n","Iteration: 254; Percent complete: 6.3%; Average loss: 5.8594\n","Iteration: 255; Percent complete: 6.4%; Average loss: 5.7505\n","Iteration: 256; Percent complete: 6.4%; Average loss: 5.7872\n","Iteration: 257; Percent complete: 6.4%; Average loss: 5.6783\n","Iteration: 258; Percent complete: 6.5%; Average loss: 5.6934\n","Iteration: 259; Percent complete: 6.5%; Average loss: 5.6783\n","Iteration: 260; Percent complete: 6.5%; Average loss: 5.6893\n","Iteration: 261; Percent complete: 6.5%; Average loss: 5.6790\n","Iteration: 262; Percent complete: 6.6%; Average loss: 5.8380\n","Iteration: 263; Percent complete: 6.6%; Average loss: 5.6224\n","Iteration: 264; Percent complete: 6.6%; Average loss: 5.4764\n","Iteration: 265; Percent complete: 6.6%; Average loss: 5.7550\n","Iteration: 266; Percent complete: 6.7%; Average loss: 5.8809\n","Iteration: 267; Percent complete: 6.7%; Average loss: 5.5281\n","Iteration: 268; Percent complete: 6.7%; Average loss: 5.9723\n","Iteration: 269; Percent complete: 6.7%; Average loss: 5.6159\n","Iteration: 270; Percent complete: 6.8%; Average loss: 5.6861\n","Iteration: 271; Percent complete: 6.8%; Average loss: 5.8232\n","Iteration: 272; Percent complete: 6.8%; Average loss: 5.6907\n","Iteration: 273; Percent complete: 6.8%; Average loss: 5.7184\n","Iteration: 274; Percent complete: 6.9%; Average loss: 5.6742\n","Iteration: 275; Percent complete: 6.9%; Average loss: 5.7307\n","Iteration: 276; Percent complete: 6.9%; Average loss: 5.6834\n","Iteration: 277; Percent complete: 6.9%; Average loss: 5.8319\n","Iteration: 278; Percent complete: 7.0%; Average loss: 5.6537\n","Iteration: 279; Percent complete: 7.0%; Average loss: 5.6510\n","Iteration: 280; Percent complete: 7.0%; Average loss: 5.6566\n","Iteration: 281; Percent complete: 7.0%; Average loss: 5.6990\n","Iteration: 282; Percent complete: 7.0%; Average loss: 5.6606\n","Iteration: 283; Percent complete: 7.1%; Average loss: 5.7290\n","Iteration: 284; Percent complete: 7.1%; Average loss: 5.5621\n","Iteration: 285; Percent complete: 7.1%; Average loss: 5.7209\n","Iteration: 286; Percent complete: 7.1%; Average loss: 5.4763\n","Iteration: 287; Percent complete: 7.2%; Average loss: 5.6198\n","Iteration: 288; Percent complete: 7.2%; Average loss: 5.5722\n","Iteration: 289; Percent complete: 7.2%; Average loss: 5.5009\n","Iteration: 290; Percent complete: 7.2%; Average loss: 5.7922\n","Iteration: 291; Percent complete: 7.3%; Average loss: 5.6124\n","Iteration: 292; Percent complete: 7.3%; Average loss: 5.4848\n","Iteration: 293; Percent complete: 7.3%; Average loss: 5.5439\n","Iteration: 294; Percent complete: 7.3%; Average loss: 5.8725\n","Iteration: 295; Percent complete: 7.4%; Average loss: 5.6279\n","Iteration: 296; Percent complete: 7.4%; Average loss: 5.4730\n","Iteration: 297; Percent complete: 7.4%; Average loss: 5.6452\n","Iteration: 298; Percent complete: 7.4%; Average loss: 5.6061\n","Iteration: 299; Percent complete: 7.5%; Average loss: 5.4967\n","Iteration: 300; Percent complete: 7.5%; Average loss: 5.5613\n","Iteration: 301; Percent complete: 7.5%; Average loss: 5.6571\n","Iteration: 302; Percent complete: 7.5%; Average loss: 5.7714\n","Iteration: 303; Percent complete: 7.6%; Average loss: 5.6749\n","Iteration: 304; Percent complete: 7.6%; Average loss: 5.6165\n","Iteration: 305; Percent complete: 7.6%; Average loss: 5.5546\n","Iteration: 306; Percent complete: 7.6%; Average loss: 5.5422\n","Iteration: 307; Percent complete: 7.7%; Average loss: 5.6950\n","Iteration: 308; Percent complete: 7.7%; Average loss: 5.4592\n","Iteration: 309; Percent complete: 7.7%; Average loss: 5.4451\n","Iteration: 310; Percent complete: 7.8%; Average loss: 5.6801\n","Iteration: 311; Percent complete: 7.8%; Average loss: 5.5137\n","Iteration: 312; Percent complete: 7.8%; Average loss: 5.6483\n","Iteration: 313; Percent complete: 7.8%; Average loss: 5.7834\n","Iteration: 314; Percent complete: 7.8%; Average loss: 5.7405\n","Iteration: 315; Percent complete: 7.9%; Average loss: 5.4394\n","Iteration: 316; Percent complete: 7.9%; Average loss: 5.5789\n","Iteration: 317; Percent complete: 7.9%; Average loss: 5.6140\n","Iteration: 318; Percent complete: 8.0%; Average loss: 5.4065\n","Iteration: 319; Percent complete: 8.0%; Average loss: 5.4476\n","Iteration: 320; Percent complete: 8.0%; Average loss: 5.4961\n","Iteration: 321; Percent complete: 8.0%; Average loss: 5.4646\n","Iteration: 322; Percent complete: 8.1%; Average loss: 5.7581\n","Iteration: 323; Percent complete: 8.1%; Average loss: 5.7028\n","Iteration: 324; Percent complete: 8.1%; Average loss: 5.5012\n","Iteration: 325; Percent complete: 8.1%; Average loss: 5.4785\n","Iteration: 326; Percent complete: 8.2%; Average loss: 5.4407\n","Iteration: 327; Percent complete: 8.2%; Average loss: 5.5144\n","Iteration: 328; Percent complete: 8.2%; Average loss: 5.5706\n","Iteration: 329; Percent complete: 8.2%; Average loss: 5.7542\n","Iteration: 330; Percent complete: 8.2%; Average loss: 5.5293\n","Iteration: 331; Percent complete: 8.3%; Average loss: 5.4260\n","Iteration: 332; Percent complete: 8.3%; Average loss: 5.6539\n","Iteration: 333; Percent complete: 8.3%; Average loss: 5.5189\n","Iteration: 334; Percent complete: 8.3%; Average loss: 5.4565\n","Iteration: 335; Percent complete: 8.4%; Average loss: 5.5360\n","Iteration: 336; Percent complete: 8.4%; Average loss: 5.5289\n","Iteration: 337; Percent complete: 8.4%; Average loss: 5.4235\n","Iteration: 338; Percent complete: 8.5%; Average loss: 5.4390\n","Iteration: 339; Percent complete: 8.5%; Average loss: 5.6638\n","Iteration: 340; Percent complete: 8.5%; Average loss: 5.4845\n","Iteration: 341; Percent complete: 8.5%; Average loss: 5.3810\n","Iteration: 342; Percent complete: 8.6%; Average loss: 5.3475\n","Iteration: 343; Percent complete: 8.6%; Average loss: 5.5163\n","Iteration: 344; Percent complete: 8.6%; Average loss: 5.4476\n","Iteration: 345; Percent complete: 8.6%; Average loss: 5.2766\n","Iteration: 346; Percent complete: 8.6%; Average loss: 5.4704\n","Iteration: 347; Percent complete: 8.7%; Average loss: 5.5607\n","Iteration: 348; Percent complete: 8.7%; Average loss: 5.5091\n","Iteration: 349; Percent complete: 8.7%; Average loss: 5.3871\n","Iteration: 350; Percent complete: 8.8%; Average loss: 5.0901\n","Iteration: 351; Percent complete: 8.8%; Average loss: 5.4798\n","Iteration: 352; Percent complete: 8.8%; Average loss: 5.6852\n","Iteration: 353; Percent complete: 8.8%; Average loss: 5.4202\n","Iteration: 354; Percent complete: 8.8%; Average loss: 5.5090\n","Iteration: 355; Percent complete: 8.9%; Average loss: 5.3425\n","Iteration: 356; Percent complete: 8.9%; Average loss: 5.4545\n","Iteration: 357; Percent complete: 8.9%; Average loss: 5.4683\n","Iteration: 358; Percent complete: 8.9%; Average loss: 5.2783\n","Iteration: 359; Percent complete: 9.0%; Average loss: 5.3664\n","Iteration: 360; Percent complete: 9.0%; Average loss: 5.3856\n","Iteration: 361; Percent complete: 9.0%; Average loss: 5.4492\n","Iteration: 362; Percent complete: 9.0%; Average loss: 5.4533\n","Iteration: 363; Percent complete: 9.1%; Average loss: 5.4468\n","Iteration: 364; Percent complete: 9.1%; Average loss: 5.4315\n","Iteration: 365; Percent complete: 9.1%; Average loss: 5.4636\n","Iteration: 366; Percent complete: 9.2%; Average loss: 5.4683\n","Iteration: 367; Percent complete: 9.2%; Average loss: 5.4694\n","Iteration: 368; Percent complete: 9.2%; Average loss: 5.3092\n","Iteration: 369; Percent complete: 9.2%; Average loss: 5.5211\n","Iteration: 370; Percent complete: 9.2%; Average loss: 5.4865\n","Iteration: 371; Percent complete: 9.3%; Average loss: 5.4487\n","Iteration: 372; Percent complete: 9.3%; Average loss: 5.5410\n","Iteration: 373; Percent complete: 9.3%; Average loss: 5.3324\n","Iteration: 374; Percent complete: 9.3%; Average loss: 5.2972\n","Iteration: 375; Percent complete: 9.4%; Average loss: 5.4342\n","Iteration: 376; Percent complete: 9.4%; Average loss: 5.4209\n","Iteration: 377; Percent complete: 9.4%; Average loss: 5.4891\n","Iteration: 378; Percent complete: 9.4%; Average loss: 5.3622\n","Iteration: 379; Percent complete: 9.5%; Average loss: 5.4666\n","Iteration: 380; Percent complete: 9.5%; Average loss: 5.1491\n","Iteration: 381; Percent complete: 9.5%; Average loss: 5.2985\n","Iteration: 382; Percent complete: 9.6%; Average loss: 5.2668\n","Iteration: 383; Percent complete: 9.6%; Average loss: 5.5097\n","Iteration: 384; Percent complete: 9.6%; Average loss: 5.5148\n","Iteration: 385; Percent complete: 9.6%; Average loss: 5.3727\n","Iteration: 386; Percent complete: 9.7%; Average loss: 5.3049\n","Iteration: 387; Percent complete: 9.7%; Average loss: 5.3372\n","Iteration: 388; Percent complete: 9.7%; Average loss: 5.4644\n","Iteration: 389; Percent complete: 9.7%; Average loss: 5.4528\n","Iteration: 390; Percent complete: 9.8%; Average loss: 5.1832\n","Iteration: 391; Percent complete: 9.8%; Average loss: 5.3505\n","Iteration: 392; Percent complete: 9.8%; Average loss: 5.2330\n","Iteration: 393; Percent complete: 9.8%; Average loss: 5.4252\n","Iteration: 394; Percent complete: 9.8%; Average loss: 5.2469\n","Iteration: 395; Percent complete: 9.9%; Average loss: 5.2621\n","Iteration: 396; Percent complete: 9.9%; Average loss: 5.3998\n","Iteration: 397; Percent complete: 9.9%; Average loss: 5.3267\n","Iteration: 398; Percent complete: 10.0%; Average loss: 5.3761\n","Iteration: 399; Percent complete: 10.0%; Average loss: 5.3441\n","Iteration: 400; Percent complete: 10.0%; Average loss: 5.3994\n","Iteration: 401; Percent complete: 10.0%; Average loss: 5.4745\n","Iteration: 402; Percent complete: 10.1%; Average loss: 5.2752\n","Iteration: 403; Percent complete: 10.1%; Average loss: 5.2335\n","Iteration: 404; Percent complete: 10.1%; Average loss: 5.2833\n","Iteration: 405; Percent complete: 10.1%; Average loss: 5.3125\n","Iteration: 406; Percent complete: 10.2%; Average loss: 5.1663\n","Iteration: 407; Percent complete: 10.2%; Average loss: 5.1658\n","Iteration: 408; Percent complete: 10.2%; Average loss: 5.4168\n","Iteration: 409; Percent complete: 10.2%; Average loss: 5.3521\n","Iteration: 410; Percent complete: 10.2%; Average loss: 5.1628\n","Iteration: 411; Percent complete: 10.3%; Average loss: 5.4204\n","Iteration: 412; Percent complete: 10.3%; Average loss: 5.1986\n","Iteration: 413; Percent complete: 10.3%; Average loss: 5.2748\n","Iteration: 414; Percent complete: 10.3%; Average loss: 5.3107\n","Iteration: 415; Percent complete: 10.4%; Average loss: 5.3695\n","Iteration: 416; Percent complete: 10.4%; Average loss: 5.6590\n","Iteration: 417; Percent complete: 10.4%; Average loss: 5.3503\n","Iteration: 418; Percent complete: 10.4%; Average loss: 5.3156\n","Iteration: 419; Percent complete: 10.5%; Average loss: 5.3796\n","Iteration: 420; Percent complete: 10.5%; Average loss: 5.2753\n","Iteration: 421; Percent complete: 10.5%; Average loss: 5.4325\n","Iteration: 422; Percent complete: 10.5%; Average loss: 5.1701\n","Iteration: 423; Percent complete: 10.6%; Average loss: 5.2185\n","Iteration: 424; Percent complete: 10.6%; Average loss: 5.4481\n","Iteration: 425; Percent complete: 10.6%; Average loss: 5.2773\n","Iteration: 426; Percent complete: 10.7%; Average loss: 5.2874\n","Iteration: 427; Percent complete: 10.7%; Average loss: 5.1583\n","Iteration: 428; Percent complete: 10.7%; Average loss: 5.2249\n","Iteration: 429; Percent complete: 10.7%; Average loss: 5.4153\n","Iteration: 430; Percent complete: 10.8%; Average loss: 5.2902\n","Iteration: 431; Percent complete: 10.8%; Average loss: 5.1570\n","Iteration: 432; Percent complete: 10.8%; Average loss: 5.0383\n","Iteration: 433; Percent complete: 10.8%; Average loss: 5.4125\n","Iteration: 434; Percent complete: 10.8%; Average loss: 5.5129\n","Iteration: 435; Percent complete: 10.9%; Average loss: 5.3590\n","Iteration: 436; Percent complete: 10.9%; Average loss: 5.4559\n","Iteration: 437; Percent complete: 10.9%; Average loss: 5.2087\n","Iteration: 438; Percent complete: 10.9%; Average loss: 5.1055\n","Iteration: 439; Percent complete: 11.0%; Average loss: 5.5023\n","Iteration: 440; Percent complete: 11.0%; Average loss: 5.3780\n","Iteration: 441; Percent complete: 11.0%; Average loss: 5.3347\n","Iteration: 442; Percent complete: 11.1%; Average loss: 5.4080\n","Iteration: 443; Percent complete: 11.1%; Average loss: 5.3114\n","Iteration: 444; Percent complete: 11.1%; Average loss: 5.4023\n","Iteration: 445; Percent complete: 11.1%; Average loss: 5.2670\n","Iteration: 446; Percent complete: 11.2%; Average loss: 5.1916\n","Iteration: 447; Percent complete: 11.2%; Average loss: 5.2946\n","Iteration: 448; Percent complete: 11.2%; Average loss: 5.2064\n","Iteration: 449; Percent complete: 11.2%; Average loss: 5.3210\n","Iteration: 450; Percent complete: 11.2%; Average loss: 5.0583\n","Iteration: 451; Percent complete: 11.3%; Average loss: 5.1988\n","Iteration: 452; Percent complete: 11.3%; Average loss: 5.2282\n","Iteration: 453; Percent complete: 11.3%; Average loss: 5.0630\n","Iteration: 454; Percent complete: 11.3%; Average loss: 5.4129\n","Iteration: 455; Percent complete: 11.4%; Average loss: 5.3339\n","Iteration: 456; Percent complete: 11.4%; Average loss: 5.2536\n","Iteration: 457; Percent complete: 11.4%; Average loss: 5.2573\n","Iteration: 458; Percent complete: 11.5%; Average loss: 5.2826\n","Iteration: 459; Percent complete: 11.5%; Average loss: 5.1679\n","Iteration: 460; Percent complete: 11.5%; Average loss: 5.2504\n","Iteration: 461; Percent complete: 11.5%; Average loss: 5.1739\n","Iteration: 462; Percent complete: 11.6%; Average loss: 5.0550\n","Iteration: 463; Percent complete: 11.6%; Average loss: 5.4544\n","Iteration: 464; Percent complete: 11.6%; Average loss: 5.3794\n","Iteration: 465; Percent complete: 11.6%; Average loss: 5.2527\n","Iteration: 466; Percent complete: 11.7%; Average loss: 5.2144\n","Iteration: 467; Percent complete: 11.7%; Average loss: 5.2908\n","Iteration: 468; Percent complete: 11.7%; Average loss: 5.1359\n","Iteration: 469; Percent complete: 11.7%; Average loss: 5.1057\n","Iteration: 470; Percent complete: 11.8%; Average loss: 5.1615\n","Iteration: 471; Percent complete: 11.8%; Average loss: 5.1118\n","Iteration: 472; Percent complete: 11.8%; Average loss: 5.2611\n","Iteration: 473; Percent complete: 11.8%; Average loss: 5.1922\n","Iteration: 474; Percent complete: 11.8%; Average loss: 5.2076\n","Iteration: 475; Percent complete: 11.9%; Average loss: 5.3957\n","Iteration: 476; Percent complete: 11.9%; Average loss: 5.2840\n","Iteration: 477; Percent complete: 11.9%; Average loss: 5.3661\n","Iteration: 478; Percent complete: 11.9%; Average loss: 5.2191\n","Iteration: 479; Percent complete: 12.0%; Average loss: 5.3001\n","Iteration: 480; Percent complete: 12.0%; Average loss: 5.0461\n","Iteration: 481; Percent complete: 12.0%; Average loss: 5.3756\n","Iteration: 482; Percent complete: 12.0%; Average loss: 5.4280\n","Iteration: 483; Percent complete: 12.1%; Average loss: 5.2071\n","Iteration: 484; Percent complete: 12.1%; Average loss: 5.1602\n","Iteration: 485; Percent complete: 12.1%; Average loss: 5.4062\n","Iteration: 486; Percent complete: 12.2%; Average loss: 5.1658\n","Iteration: 487; Percent complete: 12.2%; Average loss: 4.9943\n","Iteration: 488; Percent complete: 12.2%; Average loss: 5.0732\n","Iteration: 489; Percent complete: 12.2%; Average loss: 5.1680\n","Iteration: 490; Percent complete: 12.2%; Average loss: 5.0157\n","Iteration: 491; Percent complete: 12.3%; Average loss: 5.0620\n","Iteration: 492; Percent complete: 12.3%; Average loss: 5.2836\n","Iteration: 493; Percent complete: 12.3%; Average loss: 5.2292\n","Iteration: 494; Percent complete: 12.3%; Average loss: 5.0865\n","Iteration: 495; Percent complete: 12.4%; Average loss: 5.2259\n","Iteration: 496; Percent complete: 12.4%; Average loss: 5.1122\n","Iteration: 497; Percent complete: 12.4%; Average loss: 4.9515\n","Iteration: 498; Percent complete: 12.4%; Average loss: 5.4291\n","Iteration: 499; Percent complete: 12.5%; Average loss: 5.2564\n","Iteration: 500; Percent complete: 12.5%; Average loss: 5.0870\n","Iteration: 501; Percent complete: 12.5%; Average loss: 5.0958\n","Iteration: 502; Percent complete: 12.6%; Average loss: 5.2333\n","Iteration: 503; Percent complete: 12.6%; Average loss: 5.2996\n","Iteration: 504; Percent complete: 12.6%; Average loss: 5.0153\n","Iteration: 505; Percent complete: 12.6%; Average loss: 5.1951\n","Iteration: 506; Percent complete: 12.7%; Average loss: 5.1609\n","Iteration: 507; Percent complete: 12.7%; Average loss: 5.4707\n","Iteration: 508; Percent complete: 12.7%; Average loss: 5.0505\n","Iteration: 509; Percent complete: 12.7%; Average loss: 5.0296\n","Iteration: 510; Percent complete: 12.8%; Average loss: 4.8678\n","Iteration: 511; Percent complete: 12.8%; Average loss: 5.3119\n","Iteration: 512; Percent complete: 12.8%; Average loss: 5.1371\n","Iteration: 513; Percent complete: 12.8%; Average loss: 5.2625\n","Iteration: 514; Percent complete: 12.8%; Average loss: 5.1721\n","Iteration: 515; Percent complete: 12.9%; Average loss: 4.8952\n","Iteration: 516; Percent complete: 12.9%; Average loss: 4.8530\n","Iteration: 517; Percent complete: 12.9%; Average loss: 4.9925\n","Iteration: 518; Percent complete: 13.0%; Average loss: 5.2084\n","Iteration: 519; Percent complete: 13.0%; Average loss: 5.0339\n","Iteration: 520; Percent complete: 13.0%; Average loss: 5.1976\n","Iteration: 521; Percent complete: 13.0%; Average loss: 5.3671\n","Iteration: 522; Percent complete: 13.1%; Average loss: 5.1326\n","Iteration: 523; Percent complete: 13.1%; Average loss: 5.2042\n","Iteration: 524; Percent complete: 13.1%; Average loss: 5.1122\n","Iteration: 525; Percent complete: 13.1%; Average loss: 5.0610\n","Iteration: 526; Percent complete: 13.2%; Average loss: 5.1207\n","Iteration: 527; Percent complete: 13.2%; Average loss: 5.1295\n","Iteration: 528; Percent complete: 13.2%; Average loss: 5.3865\n","Iteration: 529; Percent complete: 13.2%; Average loss: 5.1259\n","Iteration: 530; Percent complete: 13.2%; Average loss: 4.9371\n","Iteration: 531; Percent complete: 13.3%; Average loss: 4.9574\n","Iteration: 532; Percent complete: 13.3%; Average loss: 5.0960\n","Iteration: 533; Percent complete: 13.3%; Average loss: 5.1617\n","Iteration: 534; Percent complete: 13.4%; Average loss: 5.1500\n","Iteration: 535; Percent complete: 13.4%; Average loss: 5.0594\n","Iteration: 536; Percent complete: 13.4%; Average loss: 5.2814\n","Iteration: 537; Percent complete: 13.4%; Average loss: 5.3195\n","Iteration: 538; Percent complete: 13.5%; Average loss: 5.3245\n","Iteration: 539; Percent complete: 13.5%; Average loss: 5.0063\n","Iteration: 540; Percent complete: 13.5%; Average loss: 5.1606\n","Iteration: 541; Percent complete: 13.5%; Average loss: 5.0945\n","Iteration: 542; Percent complete: 13.6%; Average loss: 5.0330\n","Iteration: 543; Percent complete: 13.6%; Average loss: 5.1171\n","Iteration: 544; Percent complete: 13.6%; Average loss: 5.1422\n","Iteration: 545; Percent complete: 13.6%; Average loss: 5.2144\n","Iteration: 546; Percent complete: 13.7%; Average loss: 5.0827\n","Iteration: 547; Percent complete: 13.7%; Average loss: 5.1701\n","Iteration: 548; Percent complete: 13.7%; Average loss: 5.0605\n","Iteration: 549; Percent complete: 13.7%; Average loss: 5.0774\n","Iteration: 550; Percent complete: 13.8%; Average loss: 5.0508\n","Iteration: 551; Percent complete: 13.8%; Average loss: 5.0117\n","Iteration: 552; Percent complete: 13.8%; Average loss: 5.1178\n","Iteration: 553; Percent complete: 13.8%; Average loss: 4.9671\n","Iteration: 554; Percent complete: 13.9%; Average loss: 5.0085\n","Iteration: 555; Percent complete: 13.9%; Average loss: 5.1190\n","Iteration: 556; Percent complete: 13.9%; Average loss: 5.1709\n","Iteration: 557; Percent complete: 13.9%; Average loss: 5.2202\n","Iteration: 558; Percent complete: 14.0%; Average loss: 5.0078\n","Iteration: 559; Percent complete: 14.0%; Average loss: 5.0031\n","Iteration: 560; Percent complete: 14.0%; Average loss: 5.2176\n","Iteration: 561; Percent complete: 14.0%; Average loss: 4.9716\n","Iteration: 562; Percent complete: 14.1%; Average loss: 5.2021\n","Iteration: 563; Percent complete: 14.1%; Average loss: 5.0441\n","Iteration: 564; Percent complete: 14.1%; Average loss: 4.9512\n","Iteration: 565; Percent complete: 14.1%; Average loss: 5.1927\n","Iteration: 566; Percent complete: 14.1%; Average loss: 4.8661\n","Iteration: 567; Percent complete: 14.2%; Average loss: 4.9946\n","Iteration: 568; Percent complete: 14.2%; Average loss: 4.8325\n","Iteration: 569; Percent complete: 14.2%; Average loss: 5.1698\n","Iteration: 570; Percent complete: 14.2%; Average loss: 4.8815\n","Iteration: 571; Percent complete: 14.3%; Average loss: 5.0721\n","Iteration: 572; Percent complete: 14.3%; Average loss: 5.0250\n","Iteration: 573; Percent complete: 14.3%; Average loss: 5.2389\n","Iteration: 574; Percent complete: 14.3%; Average loss: 4.9561\n","Iteration: 575; Percent complete: 14.4%; Average loss: 5.0040\n","Iteration: 576; Percent complete: 14.4%; Average loss: 5.0305\n","Iteration: 577; Percent complete: 14.4%; Average loss: 4.9667\n","Iteration: 578; Percent complete: 14.4%; Average loss: 5.0338\n","Iteration: 579; Percent complete: 14.5%; Average loss: 4.8981\n","Iteration: 580; Percent complete: 14.5%; Average loss: 4.9256\n","Iteration: 581; Percent complete: 14.5%; Average loss: 4.9753\n","Iteration: 582; Percent complete: 14.5%; Average loss: 4.9226\n","Iteration: 583; Percent complete: 14.6%; Average loss: 5.0167\n","Iteration: 584; Percent complete: 14.6%; Average loss: 5.2640\n","Iteration: 585; Percent complete: 14.6%; Average loss: 4.9873\n","Iteration: 586; Percent complete: 14.6%; Average loss: 5.1086\n","Iteration: 587; Percent complete: 14.7%; Average loss: 4.9847\n","Iteration: 588; Percent complete: 14.7%; Average loss: 5.3175\n","Iteration: 589; Percent complete: 14.7%; Average loss: 5.0298\n","Iteration: 590; Percent complete: 14.8%; Average loss: 4.9299\n","Iteration: 591; Percent complete: 14.8%; Average loss: 4.9415\n","Iteration: 592; Percent complete: 14.8%; Average loss: 5.0400\n","Iteration: 593; Percent complete: 14.8%; Average loss: 5.0894\n","Iteration: 594; Percent complete: 14.8%; Average loss: 4.8430\n","Iteration: 595; Percent complete: 14.9%; Average loss: 5.1455\n","Iteration: 596; Percent complete: 14.9%; Average loss: 5.0822\n","Iteration: 597; Percent complete: 14.9%; Average loss: 5.1069\n","Iteration: 598; Percent complete: 14.9%; Average loss: 5.0736\n","Iteration: 599; Percent complete: 15.0%; Average loss: 4.8962\n","Iteration: 600; Percent complete: 15.0%; Average loss: 5.2128\n","Iteration: 601; Percent complete: 15.0%; Average loss: 5.1301\n","Iteration: 602; Percent complete: 15.0%; Average loss: 5.0108\n","Iteration: 603; Percent complete: 15.1%; Average loss: 4.8009\n","Iteration: 604; Percent complete: 15.1%; Average loss: 5.0172\n","Iteration: 605; Percent complete: 15.1%; Average loss: 5.1451\n","Iteration: 606; Percent complete: 15.2%; Average loss: 4.8526\n","Iteration: 607; Percent complete: 15.2%; Average loss: 4.9982\n","Iteration: 608; Percent complete: 15.2%; Average loss: 5.1004\n","Iteration: 609; Percent complete: 15.2%; Average loss: 4.7985\n","Iteration: 610; Percent complete: 15.2%; Average loss: 5.1919\n","Iteration: 611; Percent complete: 15.3%; Average loss: 4.8785\n","Iteration: 612; Percent complete: 15.3%; Average loss: 5.1334\n","Iteration: 613; Percent complete: 15.3%; Average loss: 4.9625\n","Iteration: 614; Percent complete: 15.3%; Average loss: 5.0349\n","Iteration: 615; Percent complete: 15.4%; Average loss: 4.9971\n","Iteration: 616; Percent complete: 15.4%; Average loss: 5.0311\n","Iteration: 617; Percent complete: 15.4%; Average loss: 5.1703\n","Iteration: 618; Percent complete: 15.4%; Average loss: 5.1469\n","Iteration: 619; Percent complete: 15.5%; Average loss: 4.9506\n","Iteration: 620; Percent complete: 15.5%; Average loss: 5.0880\n","Iteration: 621; Percent complete: 15.5%; Average loss: 5.1471\n","Iteration: 622; Percent complete: 15.6%; Average loss: 4.9806\n","Iteration: 623; Percent complete: 15.6%; Average loss: 5.2212\n","Iteration: 624; Percent complete: 15.6%; Average loss: 4.7775\n","Iteration: 625; Percent complete: 15.6%; Average loss: 4.8644\n","Iteration: 626; Percent complete: 15.7%; Average loss: 4.8838\n","Iteration: 627; Percent complete: 15.7%; Average loss: 4.9306\n","Iteration: 628; Percent complete: 15.7%; Average loss: 4.8865\n","Iteration: 629; Percent complete: 15.7%; Average loss: 5.0763\n","Iteration: 630; Percent complete: 15.8%; Average loss: 4.8385\n","Iteration: 631; Percent complete: 15.8%; Average loss: 4.9619\n","Iteration: 632; Percent complete: 15.8%; Average loss: 4.8619\n","Iteration: 633; Percent complete: 15.8%; Average loss: 4.9757\n","Iteration: 634; Percent complete: 15.8%; Average loss: 5.1112\n","Iteration: 635; Percent complete: 15.9%; Average loss: 4.7687\n","Iteration: 636; Percent complete: 15.9%; Average loss: 5.0272\n","Iteration: 637; Percent complete: 15.9%; Average loss: 4.9210\n","Iteration: 638; Percent complete: 16.0%; Average loss: 5.1587\n","Iteration: 639; Percent complete: 16.0%; Average loss: 5.0307\n","Iteration: 640; Percent complete: 16.0%; Average loss: 4.9337\n","Iteration: 641; Percent complete: 16.0%; Average loss: 5.2121\n","Iteration: 642; Percent complete: 16.1%; Average loss: 4.8615\n","Iteration: 643; Percent complete: 16.1%; Average loss: 5.0317\n","Iteration: 644; Percent complete: 16.1%; Average loss: 4.9718\n","Iteration: 645; Percent complete: 16.1%; Average loss: 5.0372\n","Iteration: 646; Percent complete: 16.2%; Average loss: 5.0236\n","Iteration: 647; Percent complete: 16.2%; Average loss: 4.8678\n","Iteration: 648; Percent complete: 16.2%; Average loss: 5.1939\n","Iteration: 649; Percent complete: 16.2%; Average loss: 4.9841\n","Iteration: 650; Percent complete: 16.2%; Average loss: 5.0686\n","Iteration: 651; Percent complete: 16.3%; Average loss: 4.9765\n","Iteration: 652; Percent complete: 16.3%; Average loss: 4.8971\n","Iteration: 653; Percent complete: 16.3%; Average loss: 4.8968\n","Iteration: 654; Percent complete: 16.4%; Average loss: 4.9503\n","Iteration: 655; Percent complete: 16.4%; Average loss: 5.1420\n","Iteration: 656; Percent complete: 16.4%; Average loss: 4.8641\n","Iteration: 657; Percent complete: 16.4%; Average loss: 4.9740\n","Iteration: 658; Percent complete: 16.4%; Average loss: 4.7807\n","Iteration: 659; Percent complete: 16.5%; Average loss: 4.8925\n","Iteration: 660; Percent complete: 16.5%; Average loss: 4.9003\n","Iteration: 661; Percent complete: 16.5%; Average loss: 4.9032\n","Iteration: 662; Percent complete: 16.6%; Average loss: 4.8635\n","Iteration: 663; Percent complete: 16.6%; Average loss: 4.7849\n","Iteration: 664; Percent complete: 16.6%; Average loss: 4.8949\n","Iteration: 665; Percent complete: 16.6%; Average loss: 4.9845\n","Iteration: 666; Percent complete: 16.7%; Average loss: 4.9104\n","Iteration: 667; Percent complete: 16.7%; Average loss: 5.1047\n","Iteration: 668; Percent complete: 16.7%; Average loss: 4.8018\n","Iteration: 669; Percent complete: 16.7%; Average loss: 4.9009\n","Iteration: 670; Percent complete: 16.8%; Average loss: 4.8684\n","Iteration: 671; Percent complete: 16.8%; Average loss: 5.0379\n","Iteration: 672; Percent complete: 16.8%; Average loss: 5.0332\n","Iteration: 673; Percent complete: 16.8%; Average loss: 4.9369\n","Iteration: 674; Percent complete: 16.9%; Average loss: 4.6725\n","Iteration: 675; Percent complete: 16.9%; Average loss: 4.8313\n","Iteration: 676; Percent complete: 16.9%; Average loss: 5.0436\n","Iteration: 677; Percent complete: 16.9%; Average loss: 4.8515\n","Iteration: 678; Percent complete: 17.0%; Average loss: 5.2388\n","Iteration: 679; Percent complete: 17.0%; Average loss: 4.9007\n","Iteration: 680; Percent complete: 17.0%; Average loss: 4.7513\n","Iteration: 681; Percent complete: 17.0%; Average loss: 4.8702\n","Iteration: 682; Percent complete: 17.1%; Average loss: 4.8385\n","Iteration: 683; Percent complete: 17.1%; Average loss: 4.7786\n","Iteration: 684; Percent complete: 17.1%; Average loss: 5.1222\n","Iteration: 685; Percent complete: 17.1%; Average loss: 4.9679\n","Iteration: 686; Percent complete: 17.2%; Average loss: 5.0264\n","Iteration: 687; Percent complete: 17.2%; Average loss: 4.8681\n","Iteration: 688; Percent complete: 17.2%; Average loss: 4.7914\n","Iteration: 689; Percent complete: 17.2%; Average loss: 4.7981\n","Iteration: 690; Percent complete: 17.2%; Average loss: 4.7041\n","Iteration: 691; Percent complete: 17.3%; Average loss: 4.9041\n","Iteration: 692; Percent complete: 17.3%; Average loss: 4.9896\n","Iteration: 693; Percent complete: 17.3%; Average loss: 4.8290\n","Iteration: 694; Percent complete: 17.3%; Average loss: 4.8417\n","Iteration: 695; Percent complete: 17.4%; Average loss: 4.7986\n","Iteration: 696; Percent complete: 17.4%; Average loss: 4.9720\n","Iteration: 697; Percent complete: 17.4%; Average loss: 5.0605\n","Iteration: 698; Percent complete: 17.4%; Average loss: 5.0109\n","Iteration: 699; Percent complete: 17.5%; Average loss: 4.8060\n","Iteration: 700; Percent complete: 17.5%; Average loss: 4.8608\n","Iteration: 701; Percent complete: 17.5%; Average loss: 4.7882\n","Iteration: 702; Percent complete: 17.5%; Average loss: 4.8538\n","Iteration: 703; Percent complete: 17.6%; Average loss: 4.9368\n","Iteration: 704; Percent complete: 17.6%; Average loss: 4.9524\n","Iteration: 705; Percent complete: 17.6%; Average loss: 4.8855\n","Iteration: 706; Percent complete: 17.6%; Average loss: 4.9720\n","Iteration: 707; Percent complete: 17.7%; Average loss: 4.8722\n","Iteration: 708; Percent complete: 17.7%; Average loss: 4.8810\n","Iteration: 709; Percent complete: 17.7%; Average loss: 4.9166\n","Iteration: 710; Percent complete: 17.8%; Average loss: 4.9872\n","Iteration: 711; Percent complete: 17.8%; Average loss: 4.9493\n","Iteration: 712; Percent complete: 17.8%; Average loss: 5.0535\n","Iteration: 713; Percent complete: 17.8%; Average loss: 4.9985\n","Iteration: 714; Percent complete: 17.8%; Average loss: 5.0010\n","Iteration: 715; Percent complete: 17.9%; Average loss: 4.9172\n","Iteration: 716; Percent complete: 17.9%; Average loss: 4.9818\n","Iteration: 717; Percent complete: 17.9%; Average loss: 5.0910\n","Iteration: 718; Percent complete: 17.9%; Average loss: 4.9608\n","Iteration: 719; Percent complete: 18.0%; Average loss: 5.1108\n","Iteration: 720; Percent complete: 18.0%; Average loss: 4.7370\n","Iteration: 721; Percent complete: 18.0%; Average loss: 4.8613\n","Iteration: 722; Percent complete: 18.1%; Average loss: 4.7534\n","Iteration: 723; Percent complete: 18.1%; Average loss: 4.7764\n","Iteration: 724; Percent complete: 18.1%; Average loss: 4.8121\n","Iteration: 725; Percent complete: 18.1%; Average loss: 4.7031\n","Iteration: 726; Percent complete: 18.1%; Average loss: 4.9497\n","Iteration: 727; Percent complete: 18.2%; Average loss: 5.2418\n","Iteration: 728; Percent complete: 18.2%; Average loss: 4.7966\n","Iteration: 729; Percent complete: 18.2%; Average loss: 5.0063\n","Iteration: 730; Percent complete: 18.2%; Average loss: 5.1019\n","Iteration: 731; Percent complete: 18.3%; Average loss: 4.8802\n","Iteration: 732; Percent complete: 18.3%; Average loss: 5.0159\n","Iteration: 733; Percent complete: 18.3%; Average loss: 4.8804\n","Iteration: 734; Percent complete: 18.4%; Average loss: 5.0261\n","Iteration: 735; Percent complete: 18.4%; Average loss: 5.0091\n","Iteration: 736; Percent complete: 18.4%; Average loss: 4.8779\n","Iteration: 737; Percent complete: 18.4%; Average loss: 4.9239\n","Iteration: 738; Percent complete: 18.4%; Average loss: 4.6821\n","Iteration: 739; Percent complete: 18.5%; Average loss: 4.6772\n","Iteration: 740; Percent complete: 18.5%; Average loss: 4.9052\n","Iteration: 741; Percent complete: 18.5%; Average loss: 4.9615\n","Iteration: 742; Percent complete: 18.6%; Average loss: 4.7715\n","Iteration: 743; Percent complete: 18.6%; Average loss: 4.9525\n","Iteration: 744; Percent complete: 18.6%; Average loss: 4.9391\n","Iteration: 745; Percent complete: 18.6%; Average loss: 4.8371\n","Iteration: 746; Percent complete: 18.6%; Average loss: 4.9186\n","Iteration: 747; Percent complete: 18.7%; Average loss: 4.8477\n","Iteration: 748; Percent complete: 18.7%; Average loss: 4.7485\n","Iteration: 749; Percent complete: 18.7%; Average loss: 4.8020\n","Iteration: 750; Percent complete: 18.8%; Average loss: 4.8578\n","Iteration: 751; Percent complete: 18.8%; Average loss: 4.9202\n","Iteration: 752; Percent complete: 18.8%; Average loss: 4.5221\n","Iteration: 753; Percent complete: 18.8%; Average loss: 4.8605\n","Iteration: 754; Percent complete: 18.9%; Average loss: 4.9838\n","Iteration: 755; Percent complete: 18.9%; Average loss: 4.8047\n","Iteration: 756; Percent complete: 18.9%; Average loss: 4.7430\n","Iteration: 757; Percent complete: 18.9%; Average loss: 4.7475\n","Iteration: 758; Percent complete: 18.9%; Average loss: 4.6852\n","Iteration: 759; Percent complete: 19.0%; Average loss: 5.1493\n","Iteration: 760; Percent complete: 19.0%; Average loss: 4.7765\n","Iteration: 761; Percent complete: 19.0%; Average loss: 4.8761\n","Iteration: 762; Percent complete: 19.1%; Average loss: 4.8927\n","Iteration: 763; Percent complete: 19.1%; Average loss: 4.8029\n","Iteration: 764; Percent complete: 19.1%; Average loss: 4.9228\n","Iteration: 765; Percent complete: 19.1%; Average loss: 4.7835\n","Iteration: 766; Percent complete: 19.1%; Average loss: 4.9155\n","Iteration: 767; Percent complete: 19.2%; Average loss: 4.8299\n","Iteration: 768; Percent complete: 19.2%; Average loss: 4.6856\n","Iteration: 769; Percent complete: 19.2%; Average loss: 5.0197\n","Iteration: 770; Percent complete: 19.2%; Average loss: 4.9204\n","Iteration: 771; Percent complete: 19.3%; Average loss: 4.9450\n","Iteration: 772; Percent complete: 19.3%; Average loss: 4.8554\n","Iteration: 773; Percent complete: 19.3%; Average loss: 4.8126\n","Iteration: 774; Percent complete: 19.4%; Average loss: 4.8622\n","Iteration: 775; Percent complete: 19.4%; Average loss: 4.9538\n","Iteration: 776; Percent complete: 19.4%; Average loss: 4.7722\n","Iteration: 777; Percent complete: 19.4%; Average loss: 4.7572\n","Iteration: 778; Percent complete: 19.4%; Average loss: 4.8482\n","Iteration: 779; Percent complete: 19.5%; Average loss: 4.8764\n","Iteration: 780; Percent complete: 19.5%; Average loss: 4.9606\n","Iteration: 781; Percent complete: 19.5%; Average loss: 4.7664\n","Iteration: 782; Percent complete: 19.6%; Average loss: 4.8807\n","Iteration: 783; Percent complete: 19.6%; Average loss: 4.7037\n","Iteration: 784; Percent complete: 19.6%; Average loss: 4.9117\n","Iteration: 785; Percent complete: 19.6%; Average loss: 4.8599\n","Iteration: 786; Percent complete: 19.7%; Average loss: 4.7918\n","Iteration: 787; Percent complete: 19.7%; Average loss: 4.6807\n","Iteration: 788; Percent complete: 19.7%; Average loss: 4.8593\n","Iteration: 789; Percent complete: 19.7%; Average loss: 4.8762\n","Iteration: 790; Percent complete: 19.8%; Average loss: 4.6695\n","Iteration: 791; Percent complete: 19.8%; Average loss: 4.9242\n","Iteration: 792; Percent complete: 19.8%; Average loss: 4.8139\n","Iteration: 793; Percent complete: 19.8%; Average loss: 4.8102\n","Iteration: 794; Percent complete: 19.9%; Average loss: 4.8146\n","Iteration: 795; Percent complete: 19.9%; Average loss: 5.0247\n","Iteration: 796; Percent complete: 19.9%; Average loss: 4.7862\n","Iteration: 797; Percent complete: 19.9%; Average loss: 4.8494\n","Iteration: 798; Percent complete: 20.0%; Average loss: 5.0385\n","Iteration: 799; Percent complete: 20.0%; Average loss: 4.8180\n","Iteration: 800; Percent complete: 20.0%; Average loss: 4.7575\n","Iteration: 801; Percent complete: 20.0%; Average loss: 4.8194\n","Iteration: 802; Percent complete: 20.1%; Average loss: 4.8672\n","Iteration: 803; Percent complete: 20.1%; Average loss: 4.5317\n","Iteration: 804; Percent complete: 20.1%; Average loss: 4.7500\n","Iteration: 805; Percent complete: 20.1%; Average loss: 4.8827\n","Iteration: 806; Percent complete: 20.2%; Average loss: 4.8633\n","Iteration: 807; Percent complete: 20.2%; Average loss: 4.9497\n","Iteration: 808; Percent complete: 20.2%; Average loss: 4.8373\n","Iteration: 809; Percent complete: 20.2%; Average loss: 4.8113\n","Iteration: 810; Percent complete: 20.2%; Average loss: 4.7502\n","Iteration: 811; Percent complete: 20.3%; Average loss: 5.0082\n","Iteration: 812; Percent complete: 20.3%; Average loss: 4.7879\n","Iteration: 813; Percent complete: 20.3%; Average loss: 5.0080\n","Iteration: 814; Percent complete: 20.3%; Average loss: 4.9178\n","Iteration: 815; Percent complete: 20.4%; Average loss: 4.8723\n","Iteration: 816; Percent complete: 20.4%; Average loss: 4.9632\n","Iteration: 817; Percent complete: 20.4%; Average loss: 4.6472\n","Iteration: 818; Percent complete: 20.4%; Average loss: 4.8866\n","Iteration: 819; Percent complete: 20.5%; Average loss: 4.8563\n","Iteration: 820; Percent complete: 20.5%; Average loss: 5.0565\n","Iteration: 821; Percent complete: 20.5%; Average loss: 4.8951\n","Iteration: 822; Percent complete: 20.5%; Average loss: 4.8467\n","Iteration: 823; Percent complete: 20.6%; Average loss: 4.6832\n","Iteration: 824; Percent complete: 20.6%; Average loss: 4.9044\n","Iteration: 825; Percent complete: 20.6%; Average loss: 4.7567\n","Iteration: 826; Percent complete: 20.6%; Average loss: 4.8980\n","Iteration: 827; Percent complete: 20.7%; Average loss: 5.0399\n","Iteration: 828; Percent complete: 20.7%; Average loss: 5.0494\n","Iteration: 829; Percent complete: 20.7%; Average loss: 4.8508\n","Iteration: 830; Percent complete: 20.8%; Average loss: 5.0412\n","Iteration: 831; Percent complete: 20.8%; Average loss: 4.7010\n","Iteration: 832; Percent complete: 20.8%; Average loss: 4.6570\n","Iteration: 833; Percent complete: 20.8%; Average loss: 4.8158\n","Iteration: 834; Percent complete: 20.8%; Average loss: 4.8542\n","Iteration: 835; Percent complete: 20.9%; Average loss: 4.8564\n","Iteration: 836; Percent complete: 20.9%; Average loss: 4.7941\n","Iteration: 837; Percent complete: 20.9%; Average loss: 4.7707\n","Iteration: 838; Percent complete: 20.9%; Average loss: 4.7428\n","Iteration: 839; Percent complete: 21.0%; Average loss: 5.0709\n","Iteration: 840; Percent complete: 21.0%; Average loss: 4.6061\n","Iteration: 841; Percent complete: 21.0%; Average loss: 4.7578\n","Iteration: 842; Percent complete: 21.1%; Average loss: 4.8191\n","Iteration: 843; Percent complete: 21.1%; Average loss: 4.9648\n","Iteration: 844; Percent complete: 21.1%; Average loss: 4.7719\n","Iteration: 845; Percent complete: 21.1%; Average loss: 4.8324\n","Iteration: 846; Percent complete: 21.1%; Average loss: 4.8722\n","Iteration: 847; Percent complete: 21.2%; Average loss: 4.6888\n","Iteration: 848; Percent complete: 21.2%; Average loss: 4.6855\n","Iteration: 849; Percent complete: 21.2%; Average loss: 4.8826\n","Iteration: 850; Percent complete: 21.2%; Average loss: 4.6630\n","Iteration: 851; Percent complete: 21.3%; Average loss: 4.8230\n","Iteration: 852; Percent complete: 21.3%; Average loss: 4.7810\n","Iteration: 853; Percent complete: 21.3%; Average loss: 4.9076\n","Iteration: 854; Percent complete: 21.3%; Average loss: 4.6721\n","Iteration: 855; Percent complete: 21.4%; Average loss: 4.6414\n","Iteration: 856; Percent complete: 21.4%; Average loss: 4.7688\n","Iteration: 857; Percent complete: 21.4%; Average loss: 4.7967\n","Iteration: 858; Percent complete: 21.4%; Average loss: 4.8282\n","Iteration: 859; Percent complete: 21.5%; Average loss: 4.7458\n","Iteration: 860; Percent complete: 21.5%; Average loss: 4.9242\n","Iteration: 861; Percent complete: 21.5%; Average loss: 4.7927\n","Iteration: 862; Percent complete: 21.6%; Average loss: 4.7673\n","Iteration: 863; Percent complete: 21.6%; Average loss: 4.7342\n","Iteration: 864; Percent complete: 21.6%; Average loss: 4.8121\n","Iteration: 865; Percent complete: 21.6%; Average loss: 4.6600\n","Iteration: 866; Percent complete: 21.6%; Average loss: 4.8228\n","Iteration: 867; Percent complete: 21.7%; Average loss: 4.7900\n","Iteration: 868; Percent complete: 21.7%; Average loss: 4.9035\n","Iteration: 869; Percent complete: 21.7%; Average loss: 4.8878\n","Iteration: 870; Percent complete: 21.8%; Average loss: 4.7966\n","Iteration: 871; Percent complete: 21.8%; Average loss: 4.7360\n","Iteration: 872; Percent complete: 21.8%; Average loss: 4.6988\n","Iteration: 873; Percent complete: 21.8%; Average loss: 4.8616\n","Iteration: 874; Percent complete: 21.9%; Average loss: 4.7506\n","Iteration: 875; Percent complete: 21.9%; Average loss: 4.7052\n","Iteration: 876; Percent complete: 21.9%; Average loss: 4.8005\n","Iteration: 877; Percent complete: 21.9%; Average loss: 4.9375\n","Iteration: 878; Percent complete: 21.9%; Average loss: 4.6380\n","Iteration: 879; Percent complete: 22.0%; Average loss: 4.8855\n","Iteration: 880; Percent complete: 22.0%; Average loss: 4.6976\n","Iteration: 881; Percent complete: 22.0%; Average loss: 4.7834\n","Iteration: 882; Percent complete: 22.1%; Average loss: 4.8380\n","Iteration: 883; Percent complete: 22.1%; Average loss: 5.0676\n","Iteration: 884; Percent complete: 22.1%; Average loss: 5.0547\n","Iteration: 885; Percent complete: 22.1%; Average loss: 4.8184\n","Iteration: 886; Percent complete: 22.1%; Average loss: 4.6970\n","Iteration: 887; Percent complete: 22.2%; Average loss: 4.7406\n","Iteration: 888; Percent complete: 22.2%; Average loss: 4.6644\n","Iteration: 889; Percent complete: 22.2%; Average loss: 4.8429\n","Iteration: 890; Percent complete: 22.2%; Average loss: 4.8264\n","Iteration: 891; Percent complete: 22.3%; Average loss: 4.9116\n","Iteration: 892; Percent complete: 22.3%; Average loss: 4.8133\n","Iteration: 893; Percent complete: 22.3%; Average loss: 4.8291\n","Iteration: 894; Percent complete: 22.4%; Average loss: 4.6904\n","Iteration: 895; Percent complete: 22.4%; Average loss: 4.8494\n","Iteration: 896; Percent complete: 22.4%; Average loss: 4.8513\n","Iteration: 897; Percent complete: 22.4%; Average loss: 4.6932\n","Iteration: 898; Percent complete: 22.4%; Average loss: 4.8099\n","Iteration: 899; Percent complete: 22.5%; Average loss: 4.9614\n","Iteration: 900; Percent complete: 22.5%; Average loss: 4.7657\n","Iteration: 901; Percent complete: 22.5%; Average loss: 4.7778\n","Iteration: 902; Percent complete: 22.6%; Average loss: 4.8401\n","Iteration: 903; Percent complete: 22.6%; Average loss: 4.7831\n","Iteration: 904; Percent complete: 22.6%; Average loss: 4.7052\n","Iteration: 905; Percent complete: 22.6%; Average loss: 4.9079\n","Iteration: 906; Percent complete: 22.7%; Average loss: 4.9318\n","Iteration: 907; Percent complete: 22.7%; Average loss: 4.9110\n","Iteration: 908; Percent complete: 22.7%; Average loss: 4.8109\n","Iteration: 909; Percent complete: 22.7%; Average loss: 4.7487\n","Iteration: 910; Percent complete: 22.8%; Average loss: 4.7622\n","Iteration: 911; Percent complete: 22.8%; Average loss: 4.9718\n","Iteration: 912; Percent complete: 22.8%; Average loss: 4.7309\n","Iteration: 913; Percent complete: 22.8%; Average loss: 4.6165\n","Iteration: 914; Percent complete: 22.9%; Average loss: 4.5423\n","Iteration: 915; Percent complete: 22.9%; Average loss: 4.8682\n","Iteration: 916; Percent complete: 22.9%; Average loss: 4.8026\n","Iteration: 917; Percent complete: 22.9%; Average loss: 4.7950\n","Iteration: 918; Percent complete: 22.9%; Average loss: 4.8210\n","Iteration: 919; Percent complete: 23.0%; Average loss: 4.6235\n","Iteration: 920; Percent complete: 23.0%; Average loss: 5.0554\n","Iteration: 921; Percent complete: 23.0%; Average loss: 4.9504\n","Iteration: 922; Percent complete: 23.1%; Average loss: 4.6611\n","Iteration: 923; Percent complete: 23.1%; Average loss: 4.5964\n","Iteration: 924; Percent complete: 23.1%; Average loss: 4.9229\n","Iteration: 925; Percent complete: 23.1%; Average loss: 4.7505\n","Iteration: 926; Percent complete: 23.2%; Average loss: 4.8570\n","Iteration: 927; Percent complete: 23.2%; Average loss: 4.7609\n","Iteration: 928; Percent complete: 23.2%; Average loss: 4.8908\n","Iteration: 929; Percent complete: 23.2%; Average loss: 4.7516\n","Iteration: 930; Percent complete: 23.2%; Average loss: 4.6144\n","Iteration: 931; Percent complete: 23.3%; Average loss: 4.8629\n","Iteration: 932; Percent complete: 23.3%; Average loss: 4.8011\n","Iteration: 933; Percent complete: 23.3%; Average loss: 4.7224\n","Iteration: 934; Percent complete: 23.4%; Average loss: 4.6805\n","Iteration: 935; Percent complete: 23.4%; Average loss: 4.8017\n","Iteration: 936; Percent complete: 23.4%; Average loss: 4.8269\n","Iteration: 937; Percent complete: 23.4%; Average loss: 4.9412\n","Iteration: 938; Percent complete: 23.4%; Average loss: 4.8375\n","Iteration: 939; Percent complete: 23.5%; Average loss: 4.8310\n","Iteration: 940; Percent complete: 23.5%; Average loss: 4.8058\n","Iteration: 941; Percent complete: 23.5%; Average loss: 4.7937\n","Iteration: 942; Percent complete: 23.5%; Average loss: 4.7341\n","Iteration: 943; Percent complete: 23.6%; Average loss: 4.7189\n","Iteration: 944; Percent complete: 23.6%; Average loss: 4.9113\n","Iteration: 945; Percent complete: 23.6%; Average loss: 4.6700\n","Iteration: 946; Percent complete: 23.6%; Average loss: 4.8673\n","Iteration: 947; Percent complete: 23.7%; Average loss: 4.7191\n","Iteration: 948; Percent complete: 23.7%; Average loss: 4.7815\n","Iteration: 949; Percent complete: 23.7%; Average loss: 4.6869\n","Iteration: 950; Percent complete: 23.8%; Average loss: 4.7387\n","Iteration: 951; Percent complete: 23.8%; Average loss: 4.7119\n","Iteration: 952; Percent complete: 23.8%; Average loss: 5.0190\n","Iteration: 953; Percent complete: 23.8%; Average loss: 4.8400\n","Iteration: 954; Percent complete: 23.8%; Average loss: 4.8041\n","Iteration: 955; Percent complete: 23.9%; Average loss: 4.5137\n","Iteration: 956; Percent complete: 23.9%; Average loss: 4.7600\n","Iteration: 957; Percent complete: 23.9%; Average loss: 4.7068\n","Iteration: 958; Percent complete: 23.9%; Average loss: 4.8690\n","Iteration: 959; Percent complete: 24.0%; Average loss: 4.6004\n","Iteration: 960; Percent complete: 24.0%; Average loss: 4.8909\n","Iteration: 961; Percent complete: 24.0%; Average loss: 4.8595\n","Iteration: 962; Percent complete: 24.1%; Average loss: 4.6275\n","Iteration: 963; Percent complete: 24.1%; Average loss: 4.8683\n","Iteration: 964; Percent complete: 24.1%; Average loss: 4.7876\n","Iteration: 965; Percent complete: 24.1%; Average loss: 4.9119\n","Iteration: 966; Percent complete: 24.1%; Average loss: 4.8656\n","Iteration: 967; Percent complete: 24.2%; Average loss: 4.8421\n","Iteration: 968; Percent complete: 24.2%; Average loss: 4.5255\n","Iteration: 969; Percent complete: 24.2%; Average loss: 4.8115\n","Iteration: 970; Percent complete: 24.2%; Average loss: 4.7003\n","Iteration: 971; Percent complete: 24.3%; Average loss: 4.7365\n","Iteration: 972; Percent complete: 24.3%; Average loss: 4.7012\n","Iteration: 973; Percent complete: 24.3%; Average loss: 5.0351\n","Iteration: 974; Percent complete: 24.3%; Average loss: 4.7904\n","Iteration: 975; Percent complete: 24.4%; Average loss: 4.8352\n","Iteration: 976; Percent complete: 24.4%; Average loss: 4.6603\n","Iteration: 977; Percent complete: 24.4%; Average loss: 4.7041\n","Iteration: 978; Percent complete: 24.4%; Average loss: 5.0039\n","Iteration: 979; Percent complete: 24.5%; Average loss: 4.6653\n","Iteration: 980; Percent complete: 24.5%; Average loss: 4.8747\n","Iteration: 981; Percent complete: 24.5%; Average loss: 4.7704\n","Iteration: 982; Percent complete: 24.6%; Average loss: 4.8007\n","Iteration: 983; Percent complete: 24.6%; Average loss: 4.8046\n","Iteration: 984; Percent complete: 24.6%; Average loss: 4.7587\n","Iteration: 985; Percent complete: 24.6%; Average loss: 4.8564\n","Iteration: 986; Percent complete: 24.6%; Average loss: 4.8105\n","Iteration: 987; Percent complete: 24.7%; Average loss: 4.8305\n","Iteration: 988; Percent complete: 24.7%; Average loss: 4.5500\n","Iteration: 989; Percent complete: 24.7%; Average loss: 4.7378\n","Iteration: 990; Percent complete: 24.8%; Average loss: 4.8356\n","Iteration: 991; Percent complete: 24.8%; Average loss: 4.9318\n","Iteration: 992; Percent complete: 24.8%; Average loss: 4.7056\n","Iteration: 993; Percent complete: 24.8%; Average loss: 4.5451\n","Iteration: 994; Percent complete: 24.9%; Average loss: 4.6123\n","Iteration: 995; Percent complete: 24.9%; Average loss: 4.8869\n","Iteration: 996; Percent complete: 24.9%; Average loss: 4.6899\n","Iteration: 997; Percent complete: 24.9%; Average loss: 4.6915\n","Iteration: 998; Percent complete: 24.9%; Average loss: 4.6411\n","Iteration: 999; Percent complete: 25.0%; Average loss: 4.7201\n","Iteration: 1000; Percent complete: 25.0%; Average loss: 4.9046\n","Iteration: 1001; Percent complete: 25.0%; Average loss: 4.6369\n","Iteration: 1002; Percent complete: 25.1%; Average loss: 4.7811\n","Iteration: 1003; Percent complete: 25.1%; Average loss: 4.8558\n","Iteration: 1004; Percent complete: 25.1%; Average loss: 4.8245\n","Iteration: 1005; Percent complete: 25.1%; Average loss: 4.6916\n","Iteration: 1006; Percent complete: 25.1%; Average loss: 4.9309\n","Iteration: 1007; Percent complete: 25.2%; Average loss: 4.7653\n","Iteration: 1008; Percent complete: 25.2%; Average loss: 4.6515\n","Iteration: 1009; Percent complete: 25.2%; Average loss: 4.9123\n","Iteration: 1010; Percent complete: 25.2%; Average loss: 4.9901\n","Iteration: 1011; Percent complete: 25.3%; Average loss: 4.8004\n","Iteration: 1012; Percent complete: 25.3%; Average loss: 4.3995\n","Iteration: 1013; Percent complete: 25.3%; Average loss: 4.5490\n","Iteration: 1014; Percent complete: 25.4%; Average loss: 4.8853\n","Iteration: 1015; Percent complete: 25.4%; Average loss: 4.6918\n","Iteration: 1016; Percent complete: 25.4%; Average loss: 4.7261\n","Iteration: 1017; Percent complete: 25.4%; Average loss: 4.8125\n","Iteration: 1018; Percent complete: 25.4%; Average loss: 4.6541\n","Iteration: 1019; Percent complete: 25.5%; Average loss: 5.0436\n","Iteration: 1020; Percent complete: 25.5%; Average loss: 4.8280\n","Iteration: 1021; Percent complete: 25.5%; Average loss: 4.8705\n","Iteration: 1022; Percent complete: 25.6%; Average loss: 4.8076\n","Iteration: 1023; Percent complete: 25.6%; Average loss: 4.5038\n","Iteration: 1024; Percent complete: 25.6%; Average loss: 4.8309\n","Iteration: 1025; Percent complete: 25.6%; Average loss: 4.7914\n","Iteration: 1026; Percent complete: 25.7%; Average loss: 4.4618\n","Iteration: 1027; Percent complete: 25.7%; Average loss: 4.7009\n","Iteration: 1028; Percent complete: 25.7%; Average loss: 4.7260\n","Iteration: 1029; Percent complete: 25.7%; Average loss: 4.7031\n","Iteration: 1030; Percent complete: 25.8%; Average loss: 4.6052\n","Iteration: 1031; Percent complete: 25.8%; Average loss: 4.6314\n","Iteration: 1032; Percent complete: 25.8%; Average loss: 4.6561\n","Iteration: 1033; Percent complete: 25.8%; Average loss: 4.7724\n","Iteration: 1034; Percent complete: 25.9%; Average loss: 4.6511\n","Iteration: 1035; Percent complete: 25.9%; Average loss: 4.8504\n","Iteration: 1036; Percent complete: 25.9%; Average loss: 5.0132\n","Iteration: 1037; Percent complete: 25.9%; Average loss: 4.8966\n","Iteration: 1038; Percent complete: 25.9%; Average loss: 4.8743\n","Iteration: 1039; Percent complete: 26.0%; Average loss: 4.8754\n","Iteration: 1040; Percent complete: 26.0%; Average loss: 4.7222\n","Iteration: 1041; Percent complete: 26.0%; Average loss: 4.6760\n","Iteration: 1042; Percent complete: 26.1%; Average loss: 4.8012\n","Iteration: 1043; Percent complete: 26.1%; Average loss: 4.8923\n","Iteration: 1044; Percent complete: 26.1%; Average loss: 4.7131\n","Iteration: 1045; Percent complete: 26.1%; Average loss: 4.7473\n","Iteration: 1046; Percent complete: 26.2%; Average loss: 4.7596\n","Iteration: 1047; Percent complete: 26.2%; Average loss: 4.7921\n","Iteration: 1048; Percent complete: 26.2%; Average loss: 4.5263\n","Iteration: 1049; Percent complete: 26.2%; Average loss: 4.8574\n","Iteration: 1050; Percent complete: 26.2%; Average loss: 4.5028\n","Iteration: 1051; Percent complete: 26.3%; Average loss: 5.0109\n","Iteration: 1052; Percent complete: 26.3%; Average loss: 4.7282\n","Iteration: 1053; Percent complete: 26.3%; Average loss: 4.7207\n","Iteration: 1054; Percent complete: 26.4%; Average loss: 4.4953\n","Iteration: 1055; Percent complete: 26.4%; Average loss: 4.6754\n","Iteration: 1056; Percent complete: 26.4%; Average loss: 4.7536\n","Iteration: 1057; Percent complete: 26.4%; Average loss: 4.7726\n","Iteration: 1058; Percent complete: 26.5%; Average loss: 4.8901\n","Iteration: 1059; Percent complete: 26.5%; Average loss: 4.6943\n","Iteration: 1060; Percent complete: 26.5%; Average loss: 4.6618\n","Iteration: 1061; Percent complete: 26.5%; Average loss: 4.7741\n","Iteration: 1062; Percent complete: 26.6%; Average loss: 4.8083\n","Iteration: 1063; Percent complete: 26.6%; Average loss: 4.6771\n","Iteration: 1064; Percent complete: 26.6%; Average loss: 4.8672\n","Iteration: 1065; Percent complete: 26.6%; Average loss: 4.8738\n","Iteration: 1066; Percent complete: 26.7%; Average loss: 4.6816\n","Iteration: 1067; Percent complete: 26.7%; Average loss: 4.4884\n","Iteration: 1068; Percent complete: 26.7%; Average loss: 4.7282\n","Iteration: 1069; Percent complete: 26.7%; Average loss: 4.8178\n","Iteration: 1070; Percent complete: 26.8%; Average loss: 4.6779\n","Iteration: 1071; Percent complete: 26.8%; Average loss: 4.7024\n","Iteration: 1072; Percent complete: 26.8%; Average loss: 4.6366\n","Iteration: 1073; Percent complete: 26.8%; Average loss: 4.6589\n","Iteration: 1074; Percent complete: 26.9%; Average loss: 4.8155\n","Iteration: 1075; Percent complete: 26.9%; Average loss: 4.4535\n","Iteration: 1076; Percent complete: 26.9%; Average loss: 4.6518\n","Iteration: 1077; Percent complete: 26.9%; Average loss: 4.9059\n","Iteration: 1078; Percent complete: 27.0%; Average loss: 4.6787\n","Iteration: 1079; Percent complete: 27.0%; Average loss: 4.4892\n","Iteration: 1080; Percent complete: 27.0%; Average loss: 4.6210\n","Iteration: 1081; Percent complete: 27.0%; Average loss: 4.7572\n","Iteration: 1082; Percent complete: 27.1%; Average loss: 4.6171\n","Iteration: 1083; Percent complete: 27.1%; Average loss: 4.7624\n","Iteration: 1084; Percent complete: 27.1%; Average loss: 4.4255\n","Iteration: 1085; Percent complete: 27.1%; Average loss: 4.7852\n","Iteration: 1086; Percent complete: 27.2%; Average loss: 4.7651\n","Iteration: 1087; Percent complete: 27.2%; Average loss: 4.6478\n","Iteration: 1088; Percent complete: 27.2%; Average loss: 4.7055\n","Iteration: 1089; Percent complete: 27.2%; Average loss: 4.5524\n","Iteration: 1090; Percent complete: 27.3%; Average loss: 4.5049\n","Iteration: 1091; Percent complete: 27.3%; Average loss: 4.7012\n","Iteration: 1092; Percent complete: 27.3%; Average loss: 4.6264\n","Iteration: 1093; Percent complete: 27.3%; Average loss: 4.7236\n","Iteration: 1094; Percent complete: 27.4%; Average loss: 4.7932\n","Iteration: 1095; Percent complete: 27.4%; Average loss: 4.6710\n","Iteration: 1096; Percent complete: 27.4%; Average loss: 4.6237\n","Iteration: 1097; Percent complete: 27.4%; Average loss: 4.7266\n","Iteration: 1098; Percent complete: 27.5%; Average loss: 4.4917\n","Iteration: 1099; Percent complete: 27.5%; Average loss: 4.8768\n","Iteration: 1100; Percent complete: 27.5%; Average loss: 4.6411\n","Iteration: 1101; Percent complete: 27.5%; Average loss: 4.7990\n","Iteration: 1102; Percent complete: 27.6%; Average loss: 4.6657\n","Iteration: 1103; Percent complete: 27.6%; Average loss: 4.6023\n","Iteration: 1104; Percent complete: 27.6%; Average loss: 4.8180\n","Iteration: 1105; Percent complete: 27.6%; Average loss: 4.7246\n","Iteration: 1106; Percent complete: 27.7%; Average loss: 4.8492\n","Iteration: 1107; Percent complete: 27.7%; Average loss: 4.6504\n","Iteration: 1108; Percent complete: 27.7%; Average loss: 4.6102\n","Iteration: 1109; Percent complete: 27.7%; Average loss: 4.6739\n","Iteration: 1110; Percent complete: 27.8%; Average loss: 4.7119\n","Iteration: 1111; Percent complete: 27.8%; Average loss: 4.6764\n","Iteration: 1112; Percent complete: 27.8%; Average loss: 4.7648\n","Iteration: 1113; Percent complete: 27.8%; Average loss: 4.7302\n","Iteration: 1114; Percent complete: 27.9%; Average loss: 4.6559\n","Iteration: 1115; Percent complete: 27.9%; Average loss: 4.6962\n","Iteration: 1116; Percent complete: 27.9%; Average loss: 4.8143\n","Iteration: 1117; Percent complete: 27.9%; Average loss: 4.4751\n","Iteration: 1118; Percent complete: 28.0%; Average loss: 4.4702\n","Iteration: 1119; Percent complete: 28.0%; Average loss: 4.6829\n","Iteration: 1120; Percent complete: 28.0%; Average loss: 4.6881\n","Iteration: 1121; Percent complete: 28.0%; Average loss: 4.7689\n","Iteration: 1122; Percent complete: 28.1%; Average loss: 4.7978\n","Iteration: 1123; Percent complete: 28.1%; Average loss: 4.6701\n","Iteration: 1124; Percent complete: 28.1%; Average loss: 4.6069\n","Iteration: 1125; Percent complete: 28.1%; Average loss: 4.6974\n","Iteration: 1126; Percent complete: 28.1%; Average loss: 4.6427\n","Iteration: 1127; Percent complete: 28.2%; Average loss: 4.6964\n","Iteration: 1128; Percent complete: 28.2%; Average loss: 4.6380\n","Iteration: 1129; Percent complete: 28.2%; Average loss: 4.8219\n","Iteration: 1130; Percent complete: 28.2%; Average loss: 4.6478\n","Iteration: 1131; Percent complete: 28.3%; Average loss: 4.7079\n","Iteration: 1132; Percent complete: 28.3%; Average loss: 4.5583\n","Iteration: 1133; Percent complete: 28.3%; Average loss: 4.6364\n","Iteration: 1134; Percent complete: 28.3%; Average loss: 4.7593\n","Iteration: 1135; Percent complete: 28.4%; Average loss: 4.5223\n","Iteration: 1136; Percent complete: 28.4%; Average loss: 4.5443\n","Iteration: 1137; Percent complete: 28.4%; Average loss: 4.5873\n","Iteration: 1138; Percent complete: 28.4%; Average loss: 4.8322\n","Iteration: 1139; Percent complete: 28.5%; Average loss: 4.5918\n","Iteration: 1140; Percent complete: 28.5%; Average loss: 4.6876\n","Iteration: 1141; Percent complete: 28.5%; Average loss: 4.8777\n","Iteration: 1142; Percent complete: 28.5%; Average loss: 4.9002\n","Iteration: 1143; Percent complete: 28.6%; Average loss: 4.3379\n","Iteration: 1144; Percent complete: 28.6%; Average loss: 4.7866\n","Iteration: 1145; Percent complete: 28.6%; Average loss: 4.6980\n","Iteration: 1146; Percent complete: 28.6%; Average loss: 4.5036\n","Iteration: 1147; Percent complete: 28.7%; Average loss: 4.9171\n","Iteration: 1148; Percent complete: 28.7%; Average loss: 4.7149\n","Iteration: 1149; Percent complete: 28.7%; Average loss: 4.8397\n","Iteration: 1150; Percent complete: 28.7%; Average loss: 4.5901\n","Iteration: 1151; Percent complete: 28.8%; Average loss: 4.7503\n","Iteration: 1152; Percent complete: 28.8%; Average loss: 4.6650\n","Iteration: 1153; Percent complete: 28.8%; Average loss: 4.4689\n","Iteration: 1154; Percent complete: 28.8%; Average loss: 4.5869\n","Iteration: 1155; Percent complete: 28.9%; Average loss: 4.4618\n","Iteration: 1156; Percent complete: 28.9%; Average loss: 4.5003\n","Iteration: 1157; Percent complete: 28.9%; Average loss: 4.6219\n","Iteration: 1158; Percent complete: 28.9%; Average loss: 4.7015\n","Iteration: 1159; Percent complete: 29.0%; Average loss: 4.7052\n","Iteration: 1160; Percent complete: 29.0%; Average loss: 4.7061\n","Iteration: 1161; Percent complete: 29.0%; Average loss: 4.7572\n","Iteration: 1162; Percent complete: 29.0%; Average loss: 4.6325\n","Iteration: 1163; Percent complete: 29.1%; Average loss: 4.5829\n","Iteration: 1164; Percent complete: 29.1%; Average loss: 4.7101\n","Iteration: 1165; Percent complete: 29.1%; Average loss: 4.7776\n","Iteration: 1166; Percent complete: 29.1%; Average loss: 4.6255\n","Iteration: 1167; Percent complete: 29.2%; Average loss: 4.7127\n","Iteration: 1168; Percent complete: 29.2%; Average loss: 4.7178\n","Iteration: 1169; Percent complete: 29.2%; Average loss: 4.7781\n","Iteration: 1170; Percent complete: 29.2%; Average loss: 4.7325\n","Iteration: 1171; Percent complete: 29.3%; Average loss: 4.7286\n","Iteration: 1172; Percent complete: 29.3%; Average loss: 4.7588\n","Iteration: 1173; Percent complete: 29.3%; Average loss: 4.6365\n","Iteration: 1174; Percent complete: 29.3%; Average loss: 4.6734\n","Iteration: 1175; Percent complete: 29.4%; Average loss: 4.7913\n","Iteration: 1176; Percent complete: 29.4%; Average loss: 4.5242\n","Iteration: 1177; Percent complete: 29.4%; Average loss: 4.5324\n","Iteration: 1178; Percent complete: 29.4%; Average loss: 4.5637\n","Iteration: 1179; Percent complete: 29.5%; Average loss: 4.7666\n","Iteration: 1180; Percent complete: 29.5%; Average loss: 4.8408\n","Iteration: 1181; Percent complete: 29.5%; Average loss: 4.4569\n","Iteration: 1182; Percent complete: 29.5%; Average loss: 4.7593\n","Iteration: 1183; Percent complete: 29.6%; Average loss: 4.5764\n","Iteration: 1184; Percent complete: 29.6%; Average loss: 4.7169\n","Iteration: 1185; Percent complete: 29.6%; Average loss: 4.7030\n","Iteration: 1186; Percent complete: 29.6%; Average loss: 4.6450\n","Iteration: 1187; Percent complete: 29.7%; Average loss: 4.6795\n","Iteration: 1188; Percent complete: 29.7%; Average loss: 4.5970\n","Iteration: 1189; Percent complete: 29.7%; Average loss: 4.6674\n","Iteration: 1190; Percent complete: 29.8%; Average loss: 4.5231\n","Iteration: 1191; Percent complete: 29.8%; Average loss: 4.7523\n","Iteration: 1192; Percent complete: 29.8%; Average loss: 4.7688\n","Iteration: 1193; Percent complete: 29.8%; Average loss: 4.5666\n","Iteration: 1194; Percent complete: 29.8%; Average loss: 4.5492\n","Iteration: 1195; Percent complete: 29.9%; Average loss: 4.4568\n","Iteration: 1196; Percent complete: 29.9%; Average loss: 4.3623\n","Iteration: 1197; Percent complete: 29.9%; Average loss: 4.7277\n","Iteration: 1198; Percent complete: 29.9%; Average loss: 4.5111\n","Iteration: 1199; Percent complete: 30.0%; Average loss: 4.8394\n","Iteration: 1200; Percent complete: 30.0%; Average loss: 4.8009\n","Iteration: 1201; Percent complete: 30.0%; Average loss: 4.5529\n","Iteration: 1202; Percent complete: 30.0%; Average loss: 4.7054\n","Iteration: 1203; Percent complete: 30.1%; Average loss: 4.7180\n","Iteration: 1204; Percent complete: 30.1%; Average loss: 4.5007\n","Iteration: 1205; Percent complete: 30.1%; Average loss: 4.7425\n","Iteration: 1206; Percent complete: 30.1%; Average loss: 4.2688\n","Iteration: 1207; Percent complete: 30.2%; Average loss: 4.4949\n","Iteration: 1208; Percent complete: 30.2%; Average loss: 4.7267\n","Iteration: 1209; Percent complete: 30.2%; Average loss: 4.9040\n","Iteration: 1210; Percent complete: 30.2%; Average loss: 4.7268\n","Iteration: 1211; Percent complete: 30.3%; Average loss: 4.6370\n","Iteration: 1212; Percent complete: 30.3%; Average loss: 4.6794\n","Iteration: 1213; Percent complete: 30.3%; Average loss: 4.6690\n","Iteration: 1214; Percent complete: 30.3%; Average loss: 4.5907\n","Iteration: 1215; Percent complete: 30.4%; Average loss: 4.6797\n","Iteration: 1216; Percent complete: 30.4%; Average loss: 4.5558\n","Iteration: 1217; Percent complete: 30.4%; Average loss: 4.5038\n","Iteration: 1218; Percent complete: 30.4%; Average loss: 4.6738\n","Iteration: 1219; Percent complete: 30.5%; Average loss: 4.6105\n","Iteration: 1220; Percent complete: 30.5%; Average loss: 4.8806\n","Iteration: 1221; Percent complete: 30.5%; Average loss: 4.5407\n","Iteration: 1222; Percent complete: 30.6%; Average loss: 4.4580\n","Iteration: 1223; Percent complete: 30.6%; Average loss: 4.4125\n","Iteration: 1224; Percent complete: 30.6%; Average loss: 4.5142\n","Iteration: 1225; Percent complete: 30.6%; Average loss: 4.5083\n","Iteration: 1226; Percent complete: 30.6%; Average loss: 4.6367\n","Iteration: 1227; Percent complete: 30.7%; Average loss: 4.8755\n","Iteration: 1228; Percent complete: 30.7%; Average loss: 4.5014\n","Iteration: 1229; Percent complete: 30.7%; Average loss: 4.7322\n","Iteration: 1230; Percent complete: 30.8%; Average loss: 4.8728\n","Iteration: 1231; Percent complete: 30.8%; Average loss: 4.8455\n","Iteration: 1232; Percent complete: 30.8%; Average loss: 4.6419\n","Iteration: 1233; Percent complete: 30.8%; Average loss: 4.5042\n","Iteration: 1234; Percent complete: 30.9%; Average loss: 4.8040\n","Iteration: 1235; Percent complete: 30.9%; Average loss: 4.6833\n","Iteration: 1236; Percent complete: 30.9%; Average loss: 4.6575\n","Iteration: 1237; Percent complete: 30.9%; Average loss: 4.6693\n","Iteration: 1238; Percent complete: 30.9%; Average loss: 4.6575\n","Iteration: 1239; Percent complete: 31.0%; Average loss: 4.6154\n","Iteration: 1240; Percent complete: 31.0%; Average loss: 4.7231\n","Iteration: 1241; Percent complete: 31.0%; Average loss: 4.6565\n","Iteration: 1242; Percent complete: 31.1%; Average loss: 4.5261\n","Iteration: 1243; Percent complete: 31.1%; Average loss: 4.7366\n","Iteration: 1244; Percent complete: 31.1%; Average loss: 4.6715\n","Iteration: 1245; Percent complete: 31.1%; Average loss: 4.5651\n","Iteration: 1246; Percent complete: 31.1%; Average loss: 4.5589\n","Iteration: 1247; Percent complete: 31.2%; Average loss: 4.6415\n","Iteration: 1248; Percent complete: 31.2%; Average loss: 4.4655\n","Iteration: 1249; Percent complete: 31.2%; Average loss: 4.5222\n","Iteration: 1250; Percent complete: 31.2%; Average loss: 4.6772\n","Iteration: 1251; Percent complete: 31.3%; Average loss: 4.6021\n","Iteration: 1252; Percent complete: 31.3%; Average loss: 4.6561\n","Iteration: 1253; Percent complete: 31.3%; Average loss: 4.7182\n","Iteration: 1254; Percent complete: 31.4%; Average loss: 4.6215\n","Iteration: 1255; Percent complete: 31.4%; Average loss: 4.6122\n","Iteration: 1256; Percent complete: 31.4%; Average loss: 4.5419\n","Iteration: 1257; Percent complete: 31.4%; Average loss: 4.6643\n","Iteration: 1258; Percent complete: 31.4%; Average loss: 4.4252\n","Iteration: 1259; Percent complete: 31.5%; Average loss: 4.5052\n","Iteration: 1260; Percent complete: 31.5%; Average loss: 4.5095\n","Iteration: 1261; Percent complete: 31.5%; Average loss: 4.8776\n","Iteration: 1262; Percent complete: 31.6%; Average loss: 4.6341\n","Iteration: 1263; Percent complete: 31.6%; Average loss: 4.6559\n","Iteration: 1264; Percent complete: 31.6%; Average loss: 4.6905\n","Iteration: 1265; Percent complete: 31.6%; Average loss: 4.7759\n","Iteration: 1266; Percent complete: 31.6%; Average loss: 4.6586\n","Iteration: 1267; Percent complete: 31.7%; Average loss: 4.8744\n","Iteration: 1268; Percent complete: 31.7%; Average loss: 4.4935\n","Iteration: 1269; Percent complete: 31.7%; Average loss: 4.5082\n","Iteration: 1270; Percent complete: 31.8%; Average loss: 4.6297\n","Iteration: 1271; Percent complete: 31.8%; Average loss: 4.7045\n","Iteration: 1272; Percent complete: 31.8%; Average loss: 4.5062\n","Iteration: 1273; Percent complete: 31.8%; Average loss: 4.5542\n","Iteration: 1274; Percent complete: 31.9%; Average loss: 4.3263\n","Iteration: 1275; Percent complete: 31.9%; Average loss: 4.6350\n","Iteration: 1276; Percent complete: 31.9%; Average loss: 4.5166\n","Iteration: 1277; Percent complete: 31.9%; Average loss: 4.4105\n","Iteration: 1278; Percent complete: 31.9%; Average loss: 4.7244\n","Iteration: 1279; Percent complete: 32.0%; Average loss: 4.6415\n","Iteration: 1280; Percent complete: 32.0%; Average loss: 4.6572\n","Iteration: 1281; Percent complete: 32.0%; Average loss: 4.6324\n","Iteration: 1282; Percent complete: 32.0%; Average loss: 4.8565\n","Iteration: 1283; Percent complete: 32.1%; Average loss: 4.6701\n","Iteration: 1284; Percent complete: 32.1%; Average loss: 4.6731\n","Iteration: 1285; Percent complete: 32.1%; Average loss: 4.4203\n","Iteration: 1286; Percent complete: 32.1%; Average loss: 4.5912\n","Iteration: 1287; Percent complete: 32.2%; Average loss: 4.4029\n","Iteration: 1288; Percent complete: 32.2%; Average loss: 4.7658\n","Iteration: 1289; Percent complete: 32.2%; Average loss: 4.6583\n","Iteration: 1290; Percent complete: 32.2%; Average loss: 4.5738\n","Iteration: 1291; Percent complete: 32.3%; Average loss: 4.3544\n","Iteration: 1292; Percent complete: 32.3%; Average loss: 4.6588\n","Iteration: 1293; Percent complete: 32.3%; Average loss: 4.4522\n","Iteration: 1294; Percent complete: 32.4%; Average loss: 4.6765\n","Iteration: 1295; Percent complete: 32.4%; Average loss: 4.5180\n","Iteration: 1296; Percent complete: 32.4%; Average loss: 4.5275\n","Iteration: 1297; Percent complete: 32.4%; Average loss: 4.6305\n","Iteration: 1298; Percent complete: 32.5%; Average loss: 4.7780\n","Iteration: 1299; Percent complete: 32.5%; Average loss: 4.5465\n","Iteration: 1300; Percent complete: 32.5%; Average loss: 4.6916\n","Iteration: 1301; Percent complete: 32.5%; Average loss: 4.7210\n","Iteration: 1302; Percent complete: 32.6%; Average loss: 4.7505\n","Iteration: 1303; Percent complete: 32.6%; Average loss: 4.6657\n","Iteration: 1304; Percent complete: 32.6%; Average loss: 4.6708\n","Iteration: 1305; Percent complete: 32.6%; Average loss: 4.4265\n","Iteration: 1306; Percent complete: 32.6%; Average loss: 4.6145\n","Iteration: 1307; Percent complete: 32.7%; Average loss: 4.4200\n","Iteration: 1308; Percent complete: 32.7%; Average loss: 4.8918\n","Iteration: 1309; Percent complete: 32.7%; Average loss: 4.7627\n","Iteration: 1310; Percent complete: 32.8%; Average loss: 4.7872\n","Iteration: 1311; Percent complete: 32.8%; Average loss: 4.3872\n","Iteration: 1312; Percent complete: 32.8%; Average loss: 4.3944\n","Iteration: 1313; Percent complete: 32.8%; Average loss: 4.3575\n","Iteration: 1314; Percent complete: 32.9%; Average loss: 4.5999\n","Iteration: 1315; Percent complete: 32.9%; Average loss: 4.4849\n","Iteration: 1316; Percent complete: 32.9%; Average loss: 4.5017\n","Iteration: 1317; Percent complete: 32.9%; Average loss: 4.4133\n","Iteration: 1318; Percent complete: 33.0%; Average loss: 4.4567\n","Iteration: 1319; Percent complete: 33.0%; Average loss: 4.6172\n","Iteration: 1320; Percent complete: 33.0%; Average loss: 4.2791\n","Iteration: 1321; Percent complete: 33.0%; Average loss: 4.5398\n","Iteration: 1322; Percent complete: 33.1%; Average loss: 4.7076\n","Iteration: 1323; Percent complete: 33.1%; Average loss: 4.3936\n","Iteration: 1324; Percent complete: 33.1%; Average loss: 4.6894\n","Iteration: 1325; Percent complete: 33.1%; Average loss: 4.6371\n","Iteration: 1326; Percent complete: 33.1%; Average loss: 4.6642\n","Iteration: 1327; Percent complete: 33.2%; Average loss: 4.4471\n","Iteration: 1328; Percent complete: 33.2%; Average loss: 4.5042\n","Iteration: 1329; Percent complete: 33.2%; Average loss: 4.5356\n","Iteration: 1330; Percent complete: 33.2%; Average loss: 4.4417\n","Iteration: 1331; Percent complete: 33.3%; Average loss: 4.7122\n","Iteration: 1332; Percent complete: 33.3%; Average loss: 4.6378\n","Iteration: 1333; Percent complete: 33.3%; Average loss: 4.6370\n","Iteration: 1334; Percent complete: 33.4%; Average loss: 4.4633\n","Iteration: 1335; Percent complete: 33.4%; Average loss: 4.5610\n","Iteration: 1336; Percent complete: 33.4%; Average loss: 4.4526\n","Iteration: 1337; Percent complete: 33.4%; Average loss: 4.5016\n","Iteration: 1338; Percent complete: 33.5%; Average loss: 4.6081\n","Iteration: 1339; Percent complete: 33.5%; Average loss: 4.6073\n","Iteration: 1340; Percent complete: 33.5%; Average loss: 4.7407\n","Iteration: 1341; Percent complete: 33.5%; Average loss: 4.5881\n","Iteration: 1342; Percent complete: 33.6%; Average loss: 4.4963\n","Iteration: 1343; Percent complete: 33.6%; Average loss: 4.5823\n","Iteration: 1344; Percent complete: 33.6%; Average loss: 4.6300\n","Iteration: 1345; Percent complete: 33.6%; Average loss: 4.4848\n","Iteration: 1346; Percent complete: 33.7%; Average loss: 4.5314\n","Iteration: 1347; Percent complete: 33.7%; Average loss: 4.4733\n","Iteration: 1348; Percent complete: 33.7%; Average loss: 4.6135\n","Iteration: 1349; Percent complete: 33.7%; Average loss: 4.9139\n","Iteration: 1350; Percent complete: 33.8%; Average loss: 4.5541\n","Iteration: 1351; Percent complete: 33.8%; Average loss: 4.4861\n","Iteration: 1352; Percent complete: 33.8%; Average loss: 4.3430\n","Iteration: 1353; Percent complete: 33.8%; Average loss: 4.3366\n","Iteration: 1354; Percent complete: 33.9%; Average loss: 4.7087\n","Iteration: 1355; Percent complete: 33.9%; Average loss: 4.6053\n","Iteration: 1356; Percent complete: 33.9%; Average loss: 4.5617\n","Iteration: 1357; Percent complete: 33.9%; Average loss: 4.3773\n","Iteration: 1358; Percent complete: 34.0%; Average loss: 4.4216\n","Iteration: 1359; Percent complete: 34.0%; Average loss: 4.7068\n","Iteration: 1360; Percent complete: 34.0%; Average loss: 4.2842\n","Iteration: 1361; Percent complete: 34.0%; Average loss: 4.7414\n","Iteration: 1362; Percent complete: 34.1%; Average loss: 4.4124\n","Iteration: 1363; Percent complete: 34.1%; Average loss: 4.5548\n","Iteration: 1364; Percent complete: 34.1%; Average loss: 4.2635\n","Iteration: 1365; Percent complete: 34.1%; Average loss: 4.6478\n","Iteration: 1366; Percent complete: 34.2%; Average loss: 4.5011\n","Iteration: 1367; Percent complete: 34.2%; Average loss: 4.7646\n","Iteration: 1368; Percent complete: 34.2%; Average loss: 4.6691\n","Iteration: 1369; Percent complete: 34.2%; Average loss: 4.6048\n","Iteration: 1370; Percent complete: 34.2%; Average loss: 4.5918\n","Iteration: 1371; Percent complete: 34.3%; Average loss: 4.5647\n","Iteration: 1372; Percent complete: 34.3%; Average loss: 4.8267\n","Iteration: 1373; Percent complete: 34.3%; Average loss: 4.4997\n","Iteration: 1374; Percent complete: 34.4%; Average loss: 4.7178\n","Iteration: 1375; Percent complete: 34.4%; Average loss: 4.2958\n","Iteration: 1376; Percent complete: 34.4%; Average loss: 4.4911\n","Iteration: 1377; Percent complete: 34.4%; Average loss: 4.5853\n","Iteration: 1378; Percent complete: 34.4%; Average loss: 4.5396\n","Iteration: 1379; Percent complete: 34.5%; Average loss: 4.7114\n","Iteration: 1380; Percent complete: 34.5%; Average loss: 4.6490\n","Iteration: 1381; Percent complete: 34.5%; Average loss: 4.6882\n","Iteration: 1382; Percent complete: 34.5%; Average loss: 4.7309\n","Iteration: 1383; Percent complete: 34.6%; Average loss: 4.4302\n","Iteration: 1384; Percent complete: 34.6%; Average loss: 4.7024\n","Iteration: 1385; Percent complete: 34.6%; Average loss: 4.5310\n","Iteration: 1386; Percent complete: 34.6%; Average loss: 4.6789\n","Iteration: 1387; Percent complete: 34.7%; Average loss: 4.8849\n","Iteration: 1388; Percent complete: 34.7%; Average loss: 4.8150\n","Iteration: 1389; Percent complete: 34.7%; Average loss: 4.6122\n","Iteration: 1390; Percent complete: 34.8%; Average loss: 4.5795\n","Iteration: 1391; Percent complete: 34.8%; Average loss: 4.4448\n","Iteration: 1392; Percent complete: 34.8%; Average loss: 4.7667\n","Iteration: 1393; Percent complete: 34.8%; Average loss: 4.5328\n","Iteration: 1394; Percent complete: 34.8%; Average loss: 4.6164\n","Iteration: 1395; Percent complete: 34.9%; Average loss: 4.5860\n","Iteration: 1396; Percent complete: 34.9%; Average loss: 4.4636\n","Iteration: 1397; Percent complete: 34.9%; Average loss: 4.8163\n","Iteration: 1398; Percent complete: 34.9%; Average loss: 4.8615\n","Iteration: 1399; Percent complete: 35.0%; Average loss: 4.3865\n","Iteration: 1400; Percent complete: 35.0%; Average loss: 4.8603\n","Iteration: 1401; Percent complete: 35.0%; Average loss: 4.4241\n","Iteration: 1402; Percent complete: 35.0%; Average loss: 4.5748\n","Iteration: 1403; Percent complete: 35.1%; Average loss: 4.6419\n","Iteration: 1404; Percent complete: 35.1%; Average loss: 4.4578\n","Iteration: 1405; Percent complete: 35.1%; Average loss: 4.4908\n","Iteration: 1406; Percent complete: 35.1%; Average loss: 4.6207\n","Iteration: 1407; Percent complete: 35.2%; Average loss: 4.6212\n","Iteration: 1408; Percent complete: 35.2%; Average loss: 4.7112\n","Iteration: 1409; Percent complete: 35.2%; Average loss: 4.4322\n","Iteration: 1410; Percent complete: 35.2%; Average loss: 4.4845\n","Iteration: 1411; Percent complete: 35.3%; Average loss: 4.5089\n","Iteration: 1412; Percent complete: 35.3%; Average loss: 4.5456\n","Iteration: 1413; Percent complete: 35.3%; Average loss: 4.4705\n","Iteration: 1414; Percent complete: 35.4%; Average loss: 4.4320\n","Iteration: 1415; Percent complete: 35.4%; Average loss: 4.4419\n","Iteration: 1416; Percent complete: 35.4%; Average loss: 4.6292\n","Iteration: 1417; Percent complete: 35.4%; Average loss: 4.4455\n","Iteration: 1418; Percent complete: 35.4%; Average loss: 4.5005\n","Iteration: 1419; Percent complete: 35.5%; Average loss: 4.4756\n","Iteration: 1420; Percent complete: 35.5%; Average loss: 4.5491\n","Iteration: 1421; Percent complete: 35.5%; Average loss: 4.5876\n","Iteration: 1422; Percent complete: 35.5%; Average loss: 4.6773\n","Iteration: 1423; Percent complete: 35.6%; Average loss: 4.6699\n","Iteration: 1424; Percent complete: 35.6%; Average loss: 4.4758\n","Iteration: 1425; Percent complete: 35.6%; Average loss: 4.6720\n","Iteration: 1426; Percent complete: 35.6%; Average loss: 4.5033\n","Iteration: 1427; Percent complete: 35.7%; Average loss: 4.5131\n","Iteration: 1428; Percent complete: 35.7%; Average loss: 4.4927\n","Iteration: 1429; Percent complete: 35.7%; Average loss: 4.5322\n","Iteration: 1430; Percent complete: 35.8%; Average loss: 4.4445\n","Iteration: 1431; Percent complete: 35.8%; Average loss: 4.6746\n","Iteration: 1432; Percent complete: 35.8%; Average loss: 4.3939\n","Iteration: 1433; Percent complete: 35.8%; Average loss: 4.3315\n","Iteration: 1434; Percent complete: 35.9%; Average loss: 4.5498\n","Iteration: 1435; Percent complete: 35.9%; Average loss: 4.5593\n","Iteration: 1436; Percent complete: 35.9%; Average loss: 4.5133\n","Iteration: 1437; Percent complete: 35.9%; Average loss: 4.2759\n","Iteration: 1438; Percent complete: 35.9%; Average loss: 4.5649\n","Iteration: 1439; Percent complete: 36.0%; Average loss: 4.5976\n","Iteration: 1440; Percent complete: 36.0%; Average loss: 4.5036\n","Iteration: 1441; Percent complete: 36.0%; Average loss: 4.8029\n","Iteration: 1442; Percent complete: 36.0%; Average loss: 4.4704\n","Iteration: 1443; Percent complete: 36.1%; Average loss: 4.5098\n","Iteration: 1444; Percent complete: 36.1%; Average loss: 4.4100\n","Iteration: 1445; Percent complete: 36.1%; Average loss: 4.6569\n","Iteration: 1446; Percent complete: 36.1%; Average loss: 4.5070\n","Iteration: 1447; Percent complete: 36.2%; Average loss: 4.5225\n","Iteration: 1448; Percent complete: 36.2%; Average loss: 4.5952\n","Iteration: 1449; Percent complete: 36.2%; Average loss: 4.4264\n","Iteration: 1450; Percent complete: 36.2%; Average loss: 4.5883\n","Iteration: 1451; Percent complete: 36.3%; Average loss: 4.7113\n","Iteration: 1452; Percent complete: 36.3%; Average loss: 4.5527\n","Iteration: 1453; Percent complete: 36.3%; Average loss: 4.1979\n","Iteration: 1454; Percent complete: 36.4%; Average loss: 4.6339\n","Iteration: 1455; Percent complete: 36.4%; Average loss: 4.6217\n","Iteration: 1456; Percent complete: 36.4%; Average loss: 4.3894\n","Iteration: 1457; Percent complete: 36.4%; Average loss: 4.4264\n","Iteration: 1458; Percent complete: 36.4%; Average loss: 4.4383\n","Iteration: 1459; Percent complete: 36.5%; Average loss: 4.4366\n","Iteration: 1460; Percent complete: 36.5%; Average loss: 4.3013\n","Iteration: 1461; Percent complete: 36.5%; Average loss: 4.4020\n","Iteration: 1462; Percent complete: 36.5%; Average loss: 4.5788\n","Iteration: 1463; Percent complete: 36.6%; Average loss: 4.4916\n","Iteration: 1464; Percent complete: 36.6%; Average loss: 4.2835\n","Iteration: 1465; Percent complete: 36.6%; Average loss: 4.3210\n","Iteration: 1466; Percent complete: 36.6%; Average loss: 4.3137\n","Iteration: 1467; Percent complete: 36.7%; Average loss: 4.6011\n","Iteration: 1468; Percent complete: 36.7%; Average loss: 4.4992\n","Iteration: 1469; Percent complete: 36.7%; Average loss: 4.4991\n","Iteration: 1470; Percent complete: 36.8%; Average loss: 4.5617\n","Iteration: 1471; Percent complete: 36.8%; Average loss: 4.5197\n","Iteration: 1472; Percent complete: 36.8%; Average loss: 4.6474\n","Iteration: 1473; Percent complete: 36.8%; Average loss: 4.4998\n","Iteration: 1474; Percent complete: 36.9%; Average loss: 4.5788\n","Iteration: 1475; Percent complete: 36.9%; Average loss: 4.5051\n","Iteration: 1476; Percent complete: 36.9%; Average loss: 4.5401\n","Iteration: 1477; Percent complete: 36.9%; Average loss: 4.7339\n","Iteration: 1478; Percent complete: 37.0%; Average loss: 4.4624\n","Iteration: 1479; Percent complete: 37.0%; Average loss: 4.5139\n","Iteration: 1480; Percent complete: 37.0%; Average loss: 4.4790\n","Iteration: 1481; Percent complete: 37.0%; Average loss: 4.5964\n","Iteration: 1482; Percent complete: 37.0%; Average loss: 4.5146\n","Iteration: 1483; Percent complete: 37.1%; Average loss: 4.3976\n","Iteration: 1484; Percent complete: 37.1%; Average loss: 4.6764\n","Iteration: 1485; Percent complete: 37.1%; Average loss: 4.4568\n","Iteration: 1486; Percent complete: 37.1%; Average loss: 4.3881\n","Iteration: 1487; Percent complete: 37.2%; Average loss: 4.3496\n","Iteration: 1488; Percent complete: 37.2%; Average loss: 4.4579\n","Iteration: 1489; Percent complete: 37.2%; Average loss: 4.4998\n","Iteration: 1490; Percent complete: 37.2%; Average loss: 4.4058\n","Iteration: 1491; Percent complete: 37.3%; Average loss: 4.5742\n","Iteration: 1492; Percent complete: 37.3%; Average loss: 4.6278\n","Iteration: 1493; Percent complete: 37.3%; Average loss: 4.5148\n","Iteration: 1494; Percent complete: 37.4%; Average loss: 4.5021\n","Iteration: 1495; Percent complete: 37.4%; Average loss: 4.6462\n","Iteration: 1496; Percent complete: 37.4%; Average loss: 4.4443\n","Iteration: 1497; Percent complete: 37.4%; Average loss: 4.5322\n","Iteration: 1498; Percent complete: 37.5%; Average loss: 4.3520\n","Iteration: 1499; Percent complete: 37.5%; Average loss: 4.6080\n","Iteration: 1500; Percent complete: 37.5%; Average loss: 4.5146\n","Iteration: 1501; Percent complete: 37.5%; Average loss: 4.2974\n","Iteration: 1502; Percent complete: 37.5%; Average loss: 4.5203\n","Iteration: 1503; Percent complete: 37.6%; Average loss: 4.7491\n","Iteration: 1504; Percent complete: 37.6%; Average loss: 4.5248\n","Iteration: 1505; Percent complete: 37.6%; Average loss: 4.7260\n","Iteration: 1506; Percent complete: 37.6%; Average loss: 4.4077\n","Iteration: 1507; Percent complete: 37.7%; Average loss: 4.4365\n","Iteration: 1508; Percent complete: 37.7%; Average loss: 4.7541\n","Iteration: 1509; Percent complete: 37.7%; Average loss: 4.3778\n","Iteration: 1510; Percent complete: 37.8%; Average loss: 4.3430\n","Iteration: 1511; Percent complete: 37.8%; Average loss: 4.5528\n","Iteration: 1512; Percent complete: 37.8%; Average loss: 4.4297\n","Iteration: 1513; Percent complete: 37.8%; Average loss: 4.3252\n","Iteration: 1514; Percent complete: 37.9%; Average loss: 4.5721\n","Iteration: 1515; Percent complete: 37.9%; Average loss: 4.5758\n","Iteration: 1516; Percent complete: 37.9%; Average loss: 4.6227\n","Iteration: 1517; Percent complete: 37.9%; Average loss: 4.4101\n","Iteration: 1518; Percent complete: 38.0%; Average loss: 4.3915\n","Iteration: 1519; Percent complete: 38.0%; Average loss: 4.3865\n","Iteration: 1520; Percent complete: 38.0%; Average loss: 4.6117\n","Iteration: 1521; Percent complete: 38.0%; Average loss: 4.7561\n","Iteration: 1522; Percent complete: 38.0%; Average loss: 4.4023\n","Iteration: 1523; Percent complete: 38.1%; Average loss: 4.5309\n","Iteration: 1524; Percent complete: 38.1%; Average loss: 4.3744\n","Iteration: 1525; Percent complete: 38.1%; Average loss: 4.4488\n","Iteration: 1526; Percent complete: 38.1%; Average loss: 4.3377\n","Iteration: 1527; Percent complete: 38.2%; Average loss: 4.4711\n","Iteration: 1528; Percent complete: 38.2%; Average loss: 4.3869\n","Iteration: 1529; Percent complete: 38.2%; Average loss: 4.3899\n","Iteration: 1530; Percent complete: 38.2%; Average loss: 4.4421\n","Iteration: 1531; Percent complete: 38.3%; Average loss: 4.5247\n","Iteration: 1532; Percent complete: 38.3%; Average loss: 4.3316\n","Iteration: 1533; Percent complete: 38.3%; Average loss: 4.4359\n","Iteration: 1534; Percent complete: 38.4%; Average loss: 4.3560\n","Iteration: 1535; Percent complete: 38.4%; Average loss: 4.6585\n","Iteration: 1536; Percent complete: 38.4%; Average loss: 4.5024\n","Iteration: 1537; Percent complete: 38.4%; Average loss: 4.3512\n","Iteration: 1538; Percent complete: 38.5%; Average loss: 4.6231\n","Iteration: 1539; Percent complete: 38.5%; Average loss: 4.3218\n","Iteration: 1540; Percent complete: 38.5%; Average loss: 4.2867\n","Iteration: 1541; Percent complete: 38.5%; Average loss: 4.6706\n","Iteration: 1542; Percent complete: 38.6%; Average loss: 4.6635\n","Iteration: 1543; Percent complete: 38.6%; Average loss: 4.6094\n","Iteration: 1544; Percent complete: 38.6%; Average loss: 4.5309\n","Iteration: 1545; Percent complete: 38.6%; Average loss: 4.3187\n","Iteration: 1546; Percent complete: 38.6%; Average loss: 4.3197\n","Iteration: 1547; Percent complete: 38.7%; Average loss: 4.5564\n","Iteration: 1548; Percent complete: 38.7%; Average loss: 4.6871\n","Iteration: 1549; Percent complete: 38.7%; Average loss: 4.4855\n","Iteration: 1550; Percent complete: 38.8%; Average loss: 4.6926\n","Iteration: 1551; Percent complete: 38.8%; Average loss: 4.3763\n","Iteration: 1552; Percent complete: 38.8%; Average loss: 4.6110\n","Iteration: 1553; Percent complete: 38.8%; Average loss: 4.7269\n","Iteration: 1554; Percent complete: 38.9%; Average loss: 4.6139\n","Iteration: 1555; Percent complete: 38.9%; Average loss: 4.5668\n","Iteration: 1556; Percent complete: 38.9%; Average loss: 4.6613\n","Iteration: 1557; Percent complete: 38.9%; Average loss: 4.3735\n","Iteration: 1558; Percent complete: 39.0%; Average loss: 4.5896\n","Iteration: 1559; Percent complete: 39.0%; Average loss: 4.3274\n","Iteration: 1560; Percent complete: 39.0%; Average loss: 4.7065\n","Iteration: 1561; Percent complete: 39.0%; Average loss: 4.4952\n","Iteration: 1562; Percent complete: 39.1%; Average loss: 4.3984\n","Iteration: 1563; Percent complete: 39.1%; Average loss: 4.5324\n","Iteration: 1564; Percent complete: 39.1%; Average loss: 4.3308\n","Iteration: 1565; Percent complete: 39.1%; Average loss: 4.4930\n","Iteration: 1566; Percent complete: 39.1%; Average loss: 4.5149\n","Iteration: 1567; Percent complete: 39.2%; Average loss: 4.5409\n","Iteration: 1568; Percent complete: 39.2%; Average loss: 4.4107\n","Iteration: 1569; Percent complete: 39.2%; Average loss: 4.5583\n","Iteration: 1570; Percent complete: 39.2%; Average loss: 4.6637\n","Iteration: 1571; Percent complete: 39.3%; Average loss: 4.6830\n","Iteration: 1572; Percent complete: 39.3%; Average loss: 4.3829\n","Iteration: 1573; Percent complete: 39.3%; Average loss: 4.4676\n","Iteration: 1574; Percent complete: 39.4%; Average loss: 4.5849\n","Iteration: 1575; Percent complete: 39.4%; Average loss: 4.2167\n","Iteration: 1576; Percent complete: 39.4%; Average loss: 4.3165\n","Iteration: 1577; Percent complete: 39.4%; Average loss: 4.2697\n","Iteration: 1578; Percent complete: 39.5%; Average loss: 4.3906\n","Iteration: 1579; Percent complete: 39.5%; Average loss: 4.3423\n","Iteration: 1580; Percent complete: 39.5%; Average loss: 4.4056\n","Iteration: 1581; Percent complete: 39.5%; Average loss: 4.4580\n","Iteration: 1582; Percent complete: 39.6%; Average loss: 4.5382\n","Iteration: 1583; Percent complete: 39.6%; Average loss: 4.4729\n","Iteration: 1584; Percent complete: 39.6%; Average loss: 4.3779\n","Iteration: 1585; Percent complete: 39.6%; Average loss: 4.7326\n","Iteration: 1586; Percent complete: 39.6%; Average loss: 4.3166\n","Iteration: 1587; Percent complete: 39.7%; Average loss: 4.4186\n","Iteration: 1588; Percent complete: 39.7%; Average loss: 4.4457\n","Iteration: 1589; Percent complete: 39.7%; Average loss: 4.5264\n","Iteration: 1590; Percent complete: 39.8%; Average loss: 4.5447\n","Iteration: 1591; Percent complete: 39.8%; Average loss: 4.5537\n","Iteration: 1592; Percent complete: 39.8%; Average loss: 4.5253\n","Iteration: 1593; Percent complete: 39.8%; Average loss: 4.7681\n","Iteration: 1594; Percent complete: 39.9%; Average loss: 4.3757\n","Iteration: 1595; Percent complete: 39.9%; Average loss: 4.4496\n","Iteration: 1596; Percent complete: 39.9%; Average loss: 4.4044\n","Iteration: 1597; Percent complete: 39.9%; Average loss: 4.5177\n","Iteration: 1598; Percent complete: 40.0%; Average loss: 4.2448\n","Iteration: 1599; Percent complete: 40.0%; Average loss: 4.4866\n","Iteration: 1600; Percent complete: 40.0%; Average loss: 4.4014\n","Iteration: 1601; Percent complete: 40.0%; Average loss: 4.2921\n","Iteration: 1602; Percent complete: 40.1%; Average loss: 4.5121\n","Iteration: 1603; Percent complete: 40.1%; Average loss: 4.4363\n","Iteration: 1604; Percent complete: 40.1%; Average loss: 4.3667\n","Iteration: 1605; Percent complete: 40.1%; Average loss: 4.3940\n","Iteration: 1606; Percent complete: 40.2%; Average loss: 4.3386\n","Iteration: 1607; Percent complete: 40.2%; Average loss: 4.2908\n","Iteration: 1608; Percent complete: 40.2%; Average loss: 4.3651\n","Iteration: 1609; Percent complete: 40.2%; Average loss: 4.2713\n","Iteration: 1610; Percent complete: 40.2%; Average loss: 4.2607\n","Iteration: 1611; Percent complete: 40.3%; Average loss: 4.3311\n","Iteration: 1612; Percent complete: 40.3%; Average loss: 4.4079\n","Iteration: 1613; Percent complete: 40.3%; Average loss: 4.2089\n","Iteration: 1614; Percent complete: 40.4%; Average loss: 4.3641\n","Iteration: 1615; Percent complete: 40.4%; Average loss: 4.3387\n","Iteration: 1616; Percent complete: 40.4%; Average loss: 4.6394\n","Iteration: 1617; Percent complete: 40.4%; Average loss: 4.7076\n","Iteration: 1618; Percent complete: 40.5%; Average loss: 4.4860\n","Iteration: 1619; Percent complete: 40.5%; Average loss: 4.5924\n","Iteration: 1620; Percent complete: 40.5%; Average loss: 4.4459\n","Iteration: 1621; Percent complete: 40.5%; Average loss: 4.3916\n","Iteration: 1622; Percent complete: 40.6%; Average loss: 4.3287\n","Iteration: 1623; Percent complete: 40.6%; Average loss: 4.5405\n","Iteration: 1624; Percent complete: 40.6%; Average loss: 4.4564\n","Iteration: 1625; Percent complete: 40.6%; Average loss: 4.4492\n","Iteration: 1626; Percent complete: 40.6%; Average loss: 4.7070\n","Iteration: 1627; Percent complete: 40.7%; Average loss: 4.5834\n","Iteration: 1628; Percent complete: 40.7%; Average loss: 4.4761\n","Iteration: 1629; Percent complete: 40.7%; Average loss: 4.5227\n","Iteration: 1630; Percent complete: 40.8%; Average loss: 4.3684\n","Iteration: 1631; Percent complete: 40.8%; Average loss: 4.3870\n","Iteration: 1632; Percent complete: 40.8%; Average loss: 4.4978\n","Iteration: 1633; Percent complete: 40.8%; Average loss: 4.5985\n","Iteration: 1634; Percent complete: 40.8%; Average loss: 4.3997\n","Iteration: 1635; Percent complete: 40.9%; Average loss: 4.4356\n","Iteration: 1636; Percent complete: 40.9%; Average loss: 4.4636\n","Iteration: 1637; Percent complete: 40.9%; Average loss: 4.4840\n","Iteration: 1638; Percent complete: 40.9%; Average loss: 4.4150\n","Iteration: 1639; Percent complete: 41.0%; Average loss: 4.7025\n","Iteration: 1640; Percent complete: 41.0%; Average loss: 4.3671\n","Iteration: 1641; Percent complete: 41.0%; Average loss: 4.5870\n","Iteration: 1642; Percent complete: 41.0%; Average loss: 4.5393\n","Iteration: 1643; Percent complete: 41.1%; Average loss: 4.3680\n","Iteration: 1644; Percent complete: 41.1%; Average loss: 4.4377\n","Iteration: 1645; Percent complete: 41.1%; Average loss: 4.4488\n","Iteration: 1646; Percent complete: 41.1%; Average loss: 4.5426\n","Iteration: 1647; Percent complete: 41.2%; Average loss: 4.4914\n","Iteration: 1648; Percent complete: 41.2%; Average loss: 4.4006\n","Iteration: 1649; Percent complete: 41.2%; Average loss: 4.2237\n","Iteration: 1650; Percent complete: 41.2%; Average loss: 4.4318\n","Iteration: 1651; Percent complete: 41.3%; Average loss: 4.4071\n","Iteration: 1652; Percent complete: 41.3%; Average loss: 4.2476\n","Iteration: 1653; Percent complete: 41.3%; Average loss: 4.5471\n","Iteration: 1654; Percent complete: 41.3%; Average loss: 4.3926\n","Iteration: 1655; Percent complete: 41.4%; Average loss: 4.3920\n","Iteration: 1656; Percent complete: 41.4%; Average loss: 4.2358\n","Iteration: 1657; Percent complete: 41.4%; Average loss: 4.3373\n","Iteration: 1658; Percent complete: 41.4%; Average loss: 4.6858\n","Iteration: 1659; Percent complete: 41.5%; Average loss: 4.6973\n","Iteration: 1660; Percent complete: 41.5%; Average loss: 4.4378\n","Iteration: 1661; Percent complete: 41.5%; Average loss: 4.4851\n","Iteration: 1662; Percent complete: 41.5%; Average loss: 4.3848\n","Iteration: 1663; Percent complete: 41.6%; Average loss: 4.4106\n","Iteration: 1664; Percent complete: 41.6%; Average loss: 4.5234\n","Iteration: 1665; Percent complete: 41.6%; Average loss: 4.4691\n","Iteration: 1666; Percent complete: 41.6%; Average loss: 4.4982\n","Iteration: 1667; Percent complete: 41.7%; Average loss: 4.4447\n","Iteration: 1668; Percent complete: 41.7%; Average loss: 4.4714\n","Iteration: 1669; Percent complete: 41.7%; Average loss: 4.4414\n","Iteration: 1670; Percent complete: 41.8%; Average loss: 4.6572\n","Iteration: 1671; Percent complete: 41.8%; Average loss: 4.6347\n","Iteration: 1672; Percent complete: 41.8%; Average loss: 4.4528\n","Iteration: 1673; Percent complete: 41.8%; Average loss: 4.3797\n","Iteration: 1674; Percent complete: 41.9%; Average loss: 4.5640\n","Iteration: 1675; Percent complete: 41.9%; Average loss: 4.4486\n","Iteration: 1676; Percent complete: 41.9%; Average loss: 4.3563\n","Iteration: 1677; Percent complete: 41.9%; Average loss: 4.4067\n","Iteration: 1678; Percent complete: 41.9%; Average loss: 4.4870\n","Iteration: 1679; Percent complete: 42.0%; Average loss: 4.4900\n","Iteration: 1680; Percent complete: 42.0%; Average loss: 4.6144\n","Iteration: 1681; Percent complete: 42.0%; Average loss: 4.5873\n","Iteration: 1682; Percent complete: 42.0%; Average loss: 4.7448\n","Iteration: 1683; Percent complete: 42.1%; Average loss: 4.4372\n","Iteration: 1684; Percent complete: 42.1%; Average loss: 4.4104\n","Iteration: 1685; Percent complete: 42.1%; Average loss: 4.4581\n","Iteration: 1686; Percent complete: 42.1%; Average loss: 4.3912\n","Iteration: 1687; Percent complete: 42.2%; Average loss: 4.6995\n","Iteration: 1688; Percent complete: 42.2%; Average loss: 4.1089\n","Iteration: 1689; Percent complete: 42.2%; Average loss: 4.6285\n","Iteration: 1690; Percent complete: 42.2%; Average loss: 4.3952\n","Iteration: 1691; Percent complete: 42.3%; Average loss: 4.4054\n","Iteration: 1692; Percent complete: 42.3%; Average loss: 4.5890\n","Iteration: 1693; Percent complete: 42.3%; Average loss: 4.4110\n","Iteration: 1694; Percent complete: 42.4%; Average loss: 4.5981\n","Iteration: 1695; Percent complete: 42.4%; Average loss: 4.3520\n","Iteration: 1696; Percent complete: 42.4%; Average loss: 4.3079\n","Iteration: 1697; Percent complete: 42.4%; Average loss: 4.3766\n","Iteration: 1698; Percent complete: 42.4%; Average loss: 4.7076\n","Iteration: 1699; Percent complete: 42.5%; Average loss: 4.5098\n","Iteration: 1700; Percent complete: 42.5%; Average loss: 4.6317\n","Iteration: 1701; Percent complete: 42.5%; Average loss: 4.4897\n","Iteration: 1702; Percent complete: 42.5%; Average loss: 4.4487\n","Iteration: 1703; Percent complete: 42.6%; Average loss: 4.4492\n","Iteration: 1704; Percent complete: 42.6%; Average loss: 4.4700\n","Iteration: 1705; Percent complete: 42.6%; Average loss: 4.2426\n","Iteration: 1706; Percent complete: 42.6%; Average loss: 4.4903\n","Iteration: 1707; Percent complete: 42.7%; Average loss: 4.1824\n","Iteration: 1708; Percent complete: 42.7%; Average loss: 4.5817\n","Iteration: 1709; Percent complete: 42.7%; Average loss: 4.5176\n","Iteration: 1710; Percent complete: 42.8%; Average loss: 4.5739\n","Iteration: 1711; Percent complete: 42.8%; Average loss: 4.4894\n","Iteration: 1712; Percent complete: 42.8%; Average loss: 4.5539\n","Iteration: 1713; Percent complete: 42.8%; Average loss: 4.5596\n","Iteration: 1714; Percent complete: 42.9%; Average loss: 4.5126\n","Iteration: 1715; Percent complete: 42.9%; Average loss: 4.2206\n","Iteration: 1716; Percent complete: 42.9%; Average loss: 4.4243\n","Iteration: 1717; Percent complete: 42.9%; Average loss: 4.3738\n","Iteration: 1718; Percent complete: 43.0%; Average loss: 4.4612\n","Iteration: 1719; Percent complete: 43.0%; Average loss: 4.4468\n","Iteration: 1720; Percent complete: 43.0%; Average loss: 4.8333\n","Iteration: 1721; Percent complete: 43.0%; Average loss: 4.5570\n","Iteration: 1722; Percent complete: 43.0%; Average loss: 4.4211\n","Iteration: 1723; Percent complete: 43.1%; Average loss: 4.4805\n","Iteration: 1724; Percent complete: 43.1%; Average loss: 4.3239\n","Iteration: 1725; Percent complete: 43.1%; Average loss: 4.5215\n","Iteration: 1726; Percent complete: 43.1%; Average loss: 4.3135\n","Iteration: 1727; Percent complete: 43.2%; Average loss: 4.6308\n","Iteration: 1728; Percent complete: 43.2%; Average loss: 4.4335\n","Iteration: 1729; Percent complete: 43.2%; Average loss: 4.3668\n","Iteration: 1730; Percent complete: 43.2%; Average loss: 4.2988\n","Iteration: 1731; Percent complete: 43.3%; Average loss: 4.6737\n","Iteration: 1732; Percent complete: 43.3%; Average loss: 4.3466\n","Iteration: 1733; Percent complete: 43.3%; Average loss: 4.3840\n","Iteration: 1734; Percent complete: 43.4%; Average loss: 4.4341\n","Iteration: 1735; Percent complete: 43.4%; Average loss: 4.5485\n","Iteration: 1736; Percent complete: 43.4%; Average loss: 4.2126\n","Iteration: 1737; Percent complete: 43.4%; Average loss: 4.2163\n","Iteration: 1738; Percent complete: 43.5%; Average loss: 4.5394\n","Iteration: 1739; Percent complete: 43.5%; Average loss: 4.5326\n","Iteration: 1740; Percent complete: 43.5%; Average loss: 4.3976\n","Iteration: 1741; Percent complete: 43.5%; Average loss: 4.2560\n","Iteration: 1742; Percent complete: 43.5%; Average loss: 4.3703\n","Iteration: 1743; Percent complete: 43.6%; Average loss: 4.2653\n","Iteration: 1744; Percent complete: 43.6%; Average loss: 4.3530\n","Iteration: 1745; Percent complete: 43.6%; Average loss: 4.6384\n","Iteration: 1746; Percent complete: 43.6%; Average loss: 4.5898\n","Iteration: 1747; Percent complete: 43.7%; Average loss: 4.2823\n","Iteration: 1748; Percent complete: 43.7%; Average loss: 4.3477\n","Iteration: 1749; Percent complete: 43.7%; Average loss: 4.3285\n","Iteration: 1750; Percent complete: 43.8%; Average loss: 4.4687\n","Iteration: 1751; Percent complete: 43.8%; Average loss: 4.5540\n","Iteration: 1752; Percent complete: 43.8%; Average loss: 4.6462\n","Iteration: 1753; Percent complete: 43.8%; Average loss: 4.5246\n","Iteration: 1754; Percent complete: 43.9%; Average loss: 4.5187\n","Iteration: 1755; Percent complete: 43.9%; Average loss: 4.5926\n","Iteration: 1756; Percent complete: 43.9%; Average loss: 4.3612\n","Iteration: 1757; Percent complete: 43.9%; Average loss: 4.5206\n","Iteration: 1758; Percent complete: 44.0%; Average loss: 4.3553\n","Iteration: 1759; Percent complete: 44.0%; Average loss: 4.4699\n","Iteration: 1760; Percent complete: 44.0%; Average loss: 4.2016\n","Iteration: 1761; Percent complete: 44.0%; Average loss: 4.4090\n","Iteration: 1762; Percent complete: 44.0%; Average loss: 4.3054\n","Iteration: 1763; Percent complete: 44.1%; Average loss: 4.3571\n","Iteration: 1764; Percent complete: 44.1%; Average loss: 4.4391\n","Iteration: 1765; Percent complete: 44.1%; Average loss: 4.5485\n","Iteration: 1766; Percent complete: 44.1%; Average loss: 4.4096\n","Iteration: 1767; Percent complete: 44.2%; Average loss: 4.2997\n","Iteration: 1768; Percent complete: 44.2%; Average loss: 4.2055\n","Iteration: 1769; Percent complete: 44.2%; Average loss: 4.2819\n","Iteration: 1770; Percent complete: 44.2%; Average loss: 4.4623\n","Iteration: 1771; Percent complete: 44.3%; Average loss: 4.4035\n","Iteration: 1772; Percent complete: 44.3%; Average loss: 4.4757\n","Iteration: 1773; Percent complete: 44.3%; Average loss: 4.4131\n","Iteration: 1774; Percent complete: 44.4%; Average loss: 4.6472\n","Iteration: 1775; Percent complete: 44.4%; Average loss: 4.2666\n","Iteration: 1776; Percent complete: 44.4%; Average loss: 4.4013\n","Iteration: 1777; Percent complete: 44.4%; Average loss: 4.3485\n","Iteration: 1778; Percent complete: 44.5%; Average loss: 4.2980\n","Iteration: 1779; Percent complete: 44.5%; Average loss: 4.3567\n","Iteration: 1780; Percent complete: 44.5%; Average loss: 4.7626\n","Iteration: 1781; Percent complete: 44.5%; Average loss: 4.2440\n","Iteration: 1782; Percent complete: 44.5%; Average loss: 4.4116\n","Iteration: 1783; Percent complete: 44.6%; Average loss: 4.4286\n","Iteration: 1784; Percent complete: 44.6%; Average loss: 4.4392\n","Iteration: 1785; Percent complete: 44.6%; Average loss: 4.4227\n","Iteration: 1786; Percent complete: 44.6%; Average loss: 4.5527\n","Iteration: 1787; Percent complete: 44.7%; Average loss: 4.5042\n","Iteration: 1788; Percent complete: 44.7%; Average loss: 4.3762\n","Iteration: 1789; Percent complete: 44.7%; Average loss: 4.2894\n","Iteration: 1790; Percent complete: 44.8%; Average loss: 4.4019\n","Iteration: 1791; Percent complete: 44.8%; Average loss: 4.4973\n","Iteration: 1792; Percent complete: 44.8%; Average loss: 4.7998\n","Iteration: 1793; Percent complete: 44.8%; Average loss: 4.5106\n","Iteration: 1794; Percent complete: 44.9%; Average loss: 4.3992\n","Iteration: 1795; Percent complete: 44.9%; Average loss: 4.4591\n","Iteration: 1796; Percent complete: 44.9%; Average loss: 4.1177\n","Iteration: 1797; Percent complete: 44.9%; Average loss: 4.5188\n","Iteration: 1798; Percent complete: 45.0%; Average loss: 4.4648\n","Iteration: 1799; Percent complete: 45.0%; Average loss: 4.5558\n","Iteration: 1800; Percent complete: 45.0%; Average loss: 4.3151\n","Iteration: 1801; Percent complete: 45.0%; Average loss: 4.5394\n","Iteration: 1802; Percent complete: 45.1%; Average loss: 4.5418\n","Iteration: 1803; Percent complete: 45.1%; Average loss: 4.2906\n","Iteration: 1804; Percent complete: 45.1%; Average loss: 4.0521\n","Iteration: 1805; Percent complete: 45.1%; Average loss: 4.5950\n","Iteration: 1806; Percent complete: 45.1%; Average loss: 4.3925\n","Iteration: 1807; Percent complete: 45.2%; Average loss: 4.4245\n","Iteration: 1808; Percent complete: 45.2%; Average loss: 4.5783\n","Iteration: 1809; Percent complete: 45.2%; Average loss: 4.2840\n","Iteration: 1810; Percent complete: 45.2%; Average loss: 4.4594\n","Iteration: 1811; Percent complete: 45.3%; Average loss: 4.5544\n","Iteration: 1812; Percent complete: 45.3%; Average loss: 4.4011\n","Iteration: 1813; Percent complete: 45.3%; Average loss: 4.5324\n","Iteration: 1814; Percent complete: 45.4%; Average loss: 4.2416\n","Iteration: 1815; Percent complete: 45.4%; Average loss: 4.5197\n","Iteration: 1816; Percent complete: 45.4%; Average loss: 4.4135\n","Iteration: 1817; Percent complete: 45.4%; Average loss: 4.4416\n","Iteration: 1818; Percent complete: 45.5%; Average loss: 4.4025\n","Iteration: 1819; Percent complete: 45.5%; Average loss: 4.5799\n","Iteration: 1820; Percent complete: 45.5%; Average loss: 4.4804\n","Iteration: 1821; Percent complete: 45.5%; Average loss: 4.4566\n","Iteration: 1822; Percent complete: 45.6%; Average loss: 4.5307\n","Iteration: 1823; Percent complete: 45.6%; Average loss: 4.3247\n","Iteration: 1824; Percent complete: 45.6%; Average loss: 4.2304\n","Iteration: 1825; Percent complete: 45.6%; Average loss: 4.5345\n","Iteration: 1826; Percent complete: 45.6%; Average loss: 4.0727\n","Iteration: 1827; Percent complete: 45.7%; Average loss: 4.6191\n","Iteration: 1828; Percent complete: 45.7%; Average loss: 4.3865\n","Iteration: 1829; Percent complete: 45.7%; Average loss: 4.4333\n","Iteration: 1830; Percent complete: 45.8%; Average loss: 4.7420\n","Iteration: 1831; Percent complete: 45.8%; Average loss: 4.2926\n","Iteration: 1832; Percent complete: 45.8%; Average loss: 4.2572\n","Iteration: 1833; Percent complete: 45.8%; Average loss: 4.4672\n","Iteration: 1834; Percent complete: 45.9%; Average loss: 4.4428\n","Iteration: 1835; Percent complete: 45.9%; Average loss: 4.1097\n","Iteration: 1836; Percent complete: 45.9%; Average loss: 4.3869\n","Iteration: 1837; Percent complete: 45.9%; Average loss: 4.3666\n","Iteration: 1838; Percent complete: 46.0%; Average loss: 4.4928\n","Iteration: 1839; Percent complete: 46.0%; Average loss: 4.4302\n","Iteration: 1840; Percent complete: 46.0%; Average loss: 4.3293\n","Iteration: 1841; Percent complete: 46.0%; Average loss: 4.4466\n","Iteration: 1842; Percent complete: 46.1%; Average loss: 4.2094\n","Iteration: 1843; Percent complete: 46.1%; Average loss: 4.2514\n","Iteration: 1844; Percent complete: 46.1%; Average loss: 4.3950\n","Iteration: 1845; Percent complete: 46.1%; Average loss: 4.4748\n","Iteration: 1846; Percent complete: 46.2%; Average loss: 4.4609\n","Iteration: 1847; Percent complete: 46.2%; Average loss: 4.4236\n","Iteration: 1848; Percent complete: 46.2%; Average loss: 4.6867\n","Iteration: 1849; Percent complete: 46.2%; Average loss: 4.5308\n","Iteration: 1850; Percent complete: 46.2%; Average loss: 4.4294\n","Iteration: 1851; Percent complete: 46.3%; Average loss: 4.2615\n","Iteration: 1852; Percent complete: 46.3%; Average loss: 4.3862\n","Iteration: 1853; Percent complete: 46.3%; Average loss: 4.3964\n","Iteration: 1854; Percent complete: 46.4%; Average loss: 4.3342\n","Iteration: 1855; Percent complete: 46.4%; Average loss: 4.5962\n","Iteration: 1856; Percent complete: 46.4%; Average loss: 4.4520\n","Iteration: 1857; Percent complete: 46.4%; Average loss: 4.3282\n","Iteration: 1858; Percent complete: 46.5%; Average loss: 4.3368\n","Iteration: 1859; Percent complete: 46.5%; Average loss: 4.6194\n","Iteration: 1860; Percent complete: 46.5%; Average loss: 4.3333\n","Iteration: 1861; Percent complete: 46.5%; Average loss: 4.3512\n","Iteration: 1862; Percent complete: 46.6%; Average loss: 4.4774\n","Iteration: 1863; Percent complete: 46.6%; Average loss: 4.4234\n","Iteration: 1864; Percent complete: 46.6%; Average loss: 4.6494\n","Iteration: 1865; Percent complete: 46.6%; Average loss: 4.2334\n","Iteration: 1866; Percent complete: 46.7%; Average loss: 4.5362\n","Iteration: 1867; Percent complete: 46.7%; Average loss: 4.3477\n","Iteration: 1868; Percent complete: 46.7%; Average loss: 4.3437\n","Iteration: 1869; Percent complete: 46.7%; Average loss: 4.6217\n","Iteration: 1870; Percent complete: 46.8%; Average loss: 4.5925\n","Iteration: 1871; Percent complete: 46.8%; Average loss: 4.2785\n","Iteration: 1872; Percent complete: 46.8%; Average loss: 4.3111\n","Iteration: 1873; Percent complete: 46.8%; Average loss: 4.1989\n","Iteration: 1874; Percent complete: 46.9%; Average loss: 4.5508\n","Iteration: 1875; Percent complete: 46.9%; Average loss: 4.3985\n","Iteration: 1876; Percent complete: 46.9%; Average loss: 4.4901\n","Iteration: 1877; Percent complete: 46.9%; Average loss: 4.0899\n","Iteration: 1878; Percent complete: 46.9%; Average loss: 4.3543\n","Iteration: 1879; Percent complete: 47.0%; Average loss: 4.4066\n","Iteration: 1880; Percent complete: 47.0%; Average loss: 4.2551\n","Iteration: 1881; Percent complete: 47.0%; Average loss: 4.3225\n","Iteration: 1882; Percent complete: 47.0%; Average loss: 4.2769\n","Iteration: 1883; Percent complete: 47.1%; Average loss: 4.3780\n","Iteration: 1884; Percent complete: 47.1%; Average loss: 4.4409\n","Iteration: 1885; Percent complete: 47.1%; Average loss: 4.2342\n","Iteration: 1886; Percent complete: 47.1%; Average loss: 4.2206\n","Iteration: 1887; Percent complete: 47.2%; Average loss: 4.5114\n","Iteration: 1888; Percent complete: 47.2%; Average loss: 4.5039\n","Iteration: 1889; Percent complete: 47.2%; Average loss: 4.5131\n","Iteration: 1890; Percent complete: 47.2%; Average loss: 4.3556\n","Iteration: 1891; Percent complete: 47.3%; Average loss: 4.3744\n","Iteration: 1892; Percent complete: 47.3%; Average loss: 4.4571\n","Iteration: 1893; Percent complete: 47.3%; Average loss: 4.3538\n","Iteration: 1894; Percent complete: 47.3%; Average loss: 4.4351\n","Iteration: 1895; Percent complete: 47.4%; Average loss: 4.5036\n","Iteration: 1896; Percent complete: 47.4%; Average loss: 4.4680\n","Iteration: 1897; Percent complete: 47.4%; Average loss: 4.2984\n","Iteration: 1898; Percent complete: 47.4%; Average loss: 4.6706\n","Iteration: 1899; Percent complete: 47.5%; Average loss: 4.5745\n","Iteration: 1900; Percent complete: 47.5%; Average loss: 4.5066\n","Iteration: 1901; Percent complete: 47.5%; Average loss: 4.2802\n","Iteration: 1902; Percent complete: 47.5%; Average loss: 4.4819\n","Iteration: 1903; Percent complete: 47.6%; Average loss: 4.3177\n","Iteration: 1904; Percent complete: 47.6%; Average loss: 4.5964\n","Iteration: 1905; Percent complete: 47.6%; Average loss: 4.3256\n","Iteration: 1906; Percent complete: 47.6%; Average loss: 4.3400\n","Iteration: 1907; Percent complete: 47.7%; Average loss: 4.5058\n","Iteration: 1908; Percent complete: 47.7%; Average loss: 4.4017\n","Iteration: 1909; Percent complete: 47.7%; Average loss: 4.3730\n","Iteration: 1910; Percent complete: 47.8%; Average loss: 4.5852\n","Iteration: 1911; Percent complete: 47.8%; Average loss: 4.3214\n","Iteration: 1912; Percent complete: 47.8%; Average loss: 4.6933\n","Iteration: 1913; Percent complete: 47.8%; Average loss: 4.2081\n","Iteration: 1914; Percent complete: 47.9%; Average loss: 4.4015\n","Iteration: 1915; Percent complete: 47.9%; Average loss: 4.4350\n","Iteration: 1916; Percent complete: 47.9%; Average loss: 4.2153\n","Iteration: 1917; Percent complete: 47.9%; Average loss: 4.1262\n","Iteration: 1918; Percent complete: 47.9%; Average loss: 4.2769\n","Iteration: 1919; Percent complete: 48.0%; Average loss: 4.2932\n","Iteration: 1920; Percent complete: 48.0%; Average loss: 4.5497\n","Iteration: 1921; Percent complete: 48.0%; Average loss: 4.2209\n","Iteration: 1922; Percent complete: 48.0%; Average loss: 4.5132\n","Iteration: 1923; Percent complete: 48.1%; Average loss: 4.4656\n","Iteration: 1924; Percent complete: 48.1%; Average loss: 4.3164\n","Iteration: 1925; Percent complete: 48.1%; Average loss: 4.4412\n","Iteration: 1926; Percent complete: 48.1%; Average loss: 4.4657\n","Iteration: 1927; Percent complete: 48.2%; Average loss: 4.4130\n","Iteration: 1928; Percent complete: 48.2%; Average loss: 4.3732\n","Iteration: 1929; Percent complete: 48.2%; Average loss: 4.3500\n","Iteration: 1930; Percent complete: 48.2%; Average loss: 4.2828\n","Iteration: 1931; Percent complete: 48.3%; Average loss: 4.2449\n","Iteration: 1932; Percent complete: 48.3%; Average loss: 4.1127\n","Iteration: 1933; Percent complete: 48.3%; Average loss: 4.2488\n","Iteration: 1934; Percent complete: 48.4%; Average loss: 4.5583\n","Iteration: 1935; Percent complete: 48.4%; Average loss: 4.5363\n","Iteration: 1936; Percent complete: 48.4%; Average loss: 4.2264\n","Iteration: 1937; Percent complete: 48.4%; Average loss: 4.4049\n","Iteration: 1938; Percent complete: 48.4%; Average loss: 4.6700\n","Iteration: 1939; Percent complete: 48.5%; Average loss: 4.4251\n","Iteration: 1940; Percent complete: 48.5%; Average loss: 4.0996\n","Iteration: 1941; Percent complete: 48.5%; Average loss: 4.2755\n","Iteration: 1942; Percent complete: 48.5%; Average loss: 4.3694\n","Iteration: 1943; Percent complete: 48.6%; Average loss: 4.4524\n","Iteration: 1944; Percent complete: 48.6%; Average loss: 4.2529\n","Iteration: 1945; Percent complete: 48.6%; Average loss: 4.3733\n","Iteration: 1946; Percent complete: 48.6%; Average loss: 4.3977\n","Iteration: 1947; Percent complete: 48.7%; Average loss: 4.4060\n","Iteration: 1948; Percent complete: 48.7%; Average loss: 4.4330\n","Iteration: 1949; Percent complete: 48.7%; Average loss: 4.2852\n","Iteration: 1950; Percent complete: 48.8%; Average loss: 4.3990\n","Iteration: 1951; Percent complete: 48.8%; Average loss: 4.2071\n","Iteration: 1952; Percent complete: 48.8%; Average loss: 4.3516\n","Iteration: 1953; Percent complete: 48.8%; Average loss: 4.3726\n","Iteration: 1954; Percent complete: 48.9%; Average loss: 4.3865\n","Iteration: 1955; Percent complete: 48.9%; Average loss: 4.4602\n","Iteration: 1956; Percent complete: 48.9%; Average loss: 4.4872\n","Iteration: 1957; Percent complete: 48.9%; Average loss: 4.4285\n","Iteration: 1958; Percent complete: 48.9%; Average loss: 4.5516\n","Iteration: 1959; Percent complete: 49.0%; Average loss: 4.4964\n","Iteration: 1960; Percent complete: 49.0%; Average loss: 4.3233\n","Iteration: 1961; Percent complete: 49.0%; Average loss: 4.5047\n","Iteration: 1962; Percent complete: 49.0%; Average loss: 4.5576\n","Iteration: 1963; Percent complete: 49.1%; Average loss: 4.4340\n","Iteration: 1964; Percent complete: 49.1%; Average loss: 4.3933\n","Iteration: 1965; Percent complete: 49.1%; Average loss: 4.5358\n","Iteration: 1966; Percent complete: 49.1%; Average loss: 4.4640\n","Iteration: 1967; Percent complete: 49.2%; Average loss: 4.3030\n","Iteration: 1968; Percent complete: 49.2%; Average loss: 4.3556\n","Iteration: 1969; Percent complete: 49.2%; Average loss: 4.6136\n","Iteration: 1970; Percent complete: 49.2%; Average loss: 4.3958\n","Iteration: 1971; Percent complete: 49.3%; Average loss: 4.2857\n","Iteration: 1972; Percent complete: 49.3%; Average loss: 4.4743\n","Iteration: 1973; Percent complete: 49.3%; Average loss: 4.2930\n","Iteration: 1974; Percent complete: 49.4%; Average loss: 4.4863\n","Iteration: 1975; Percent complete: 49.4%; Average loss: 4.3454\n","Iteration: 1976; Percent complete: 49.4%; Average loss: 4.2045\n","Iteration: 1977; Percent complete: 49.4%; Average loss: 4.4200\n","Iteration: 1978; Percent complete: 49.5%; Average loss: 4.2781\n","Iteration: 1979; Percent complete: 49.5%; Average loss: 4.3471\n","Iteration: 1980; Percent complete: 49.5%; Average loss: 4.3097\n","Iteration: 1981; Percent complete: 49.5%; Average loss: 4.4440\n","Iteration: 1982; Percent complete: 49.5%; Average loss: 4.4782\n","Iteration: 1983; Percent complete: 49.6%; Average loss: 4.2924\n","Iteration: 1984; Percent complete: 49.6%; Average loss: 4.2092\n","Iteration: 1985; Percent complete: 49.6%; Average loss: 4.4231\n","Iteration: 1986; Percent complete: 49.6%; Average loss: 4.5381\n","Iteration: 1987; Percent complete: 49.7%; Average loss: 4.4085\n","Iteration: 1988; Percent complete: 49.7%; Average loss: 4.2990\n","Iteration: 1989; Percent complete: 49.7%; Average loss: 4.3094\n","Iteration: 1990; Percent complete: 49.8%; Average loss: 4.3237\n","Iteration: 1991; Percent complete: 49.8%; Average loss: 4.4443\n","Iteration: 1992; Percent complete: 49.8%; Average loss: 4.1298\n","Iteration: 1993; Percent complete: 49.8%; Average loss: 4.3886\n","Iteration: 1994; Percent complete: 49.9%; Average loss: 4.2351\n","Iteration: 1995; Percent complete: 49.9%; Average loss: 4.3611\n","Iteration: 1996; Percent complete: 49.9%; Average loss: 4.5955\n","Iteration: 1997; Percent complete: 49.9%; Average loss: 4.5769\n","Iteration: 1998; Percent complete: 50.0%; Average loss: 4.4939\n","Iteration: 1999; Percent complete: 50.0%; Average loss: 4.2110\n","Iteration: 2000; Percent complete: 50.0%; Average loss: 4.5260\n","Iteration: 2001; Percent complete: 50.0%; Average loss: 4.3553\n","Iteration: 2002; Percent complete: 50.0%; Average loss: 4.2635\n","Iteration: 2003; Percent complete: 50.1%; Average loss: 4.6074\n","Iteration: 2004; Percent complete: 50.1%; Average loss: 4.6386\n","Iteration: 2005; Percent complete: 50.1%; Average loss: 4.5200\n","Iteration: 2006; Percent complete: 50.1%; Average loss: 4.0906\n","Iteration: 2007; Percent complete: 50.2%; Average loss: 4.4950\n","Iteration: 2008; Percent complete: 50.2%; Average loss: 4.4090\n","Iteration: 2009; Percent complete: 50.2%; Average loss: 4.2238\n","Iteration: 2010; Percent complete: 50.2%; Average loss: 4.4020\n","Iteration: 2011; Percent complete: 50.3%; Average loss: 4.3829\n","Iteration: 2012; Percent complete: 50.3%; Average loss: 4.2468\n","Iteration: 2013; Percent complete: 50.3%; Average loss: 4.2189\n","Iteration: 2014; Percent complete: 50.3%; Average loss: 4.3680\n","Iteration: 2015; Percent complete: 50.4%; Average loss: 4.4667\n","Iteration: 2016; Percent complete: 50.4%; Average loss: 4.2812\n","Iteration: 2017; Percent complete: 50.4%; Average loss: 4.2336\n","Iteration: 2018; Percent complete: 50.4%; Average loss: 4.3893\n","Iteration: 2019; Percent complete: 50.5%; Average loss: 4.3961\n","Iteration: 2020; Percent complete: 50.5%; Average loss: 4.3297\n","Iteration: 2021; Percent complete: 50.5%; Average loss: 4.4421\n","Iteration: 2022; Percent complete: 50.5%; Average loss: 4.2831\n","Iteration: 2023; Percent complete: 50.6%; Average loss: 4.2331\n","Iteration: 2024; Percent complete: 50.6%; Average loss: 4.4045\n","Iteration: 2025; Percent complete: 50.6%; Average loss: 4.5740\n","Iteration: 2026; Percent complete: 50.6%; Average loss: 4.5812\n","Iteration: 2027; Percent complete: 50.7%; Average loss: 4.4710\n","Iteration: 2028; Percent complete: 50.7%; Average loss: 4.3229\n","Iteration: 2029; Percent complete: 50.7%; Average loss: 4.4026\n","Iteration: 2030; Percent complete: 50.7%; Average loss: 4.4041\n","Iteration: 2031; Percent complete: 50.8%; Average loss: 4.3905\n","Iteration: 2032; Percent complete: 50.8%; Average loss: 4.3751\n","Iteration: 2033; Percent complete: 50.8%; Average loss: 4.3773\n","Iteration: 2034; Percent complete: 50.8%; Average loss: 4.3413\n","Iteration: 2035; Percent complete: 50.9%; Average loss: 4.4169\n","Iteration: 2036; Percent complete: 50.9%; Average loss: 4.2690\n","Iteration: 2037; Percent complete: 50.9%; Average loss: 4.5895\n","Iteration: 2038; Percent complete: 50.9%; Average loss: 4.5377\n","Iteration: 2039; Percent complete: 51.0%; Average loss: 4.2723\n","Iteration: 2040; Percent complete: 51.0%; Average loss: 4.3145\n","Iteration: 2041; Percent complete: 51.0%; Average loss: 4.3036\n","Iteration: 2042; Percent complete: 51.0%; Average loss: 4.3705\n","Iteration: 2043; Percent complete: 51.1%; Average loss: 4.6106\n","Iteration: 2044; Percent complete: 51.1%; Average loss: 4.2164\n","Iteration: 2045; Percent complete: 51.1%; Average loss: 4.6332\n","Iteration: 2046; Percent complete: 51.1%; Average loss: 4.2050\n","Iteration: 2047; Percent complete: 51.2%; Average loss: 4.4165\n","Iteration: 2048; Percent complete: 51.2%; Average loss: 4.4708\n","Iteration: 2049; Percent complete: 51.2%; Average loss: 3.9266\n","Iteration: 2050; Percent complete: 51.2%; Average loss: 4.3635\n","Iteration: 2051; Percent complete: 51.3%; Average loss: 4.3745\n","Iteration: 2052; Percent complete: 51.3%; Average loss: 4.3953\n","Iteration: 2053; Percent complete: 51.3%; Average loss: 4.0650\n","Iteration: 2054; Percent complete: 51.3%; Average loss: 4.5867\n","Iteration: 2055; Percent complete: 51.4%; Average loss: 4.4194\n","Iteration: 2056; Percent complete: 51.4%; Average loss: 4.3496\n","Iteration: 2057; Percent complete: 51.4%; Average loss: 4.1871\n","Iteration: 2058; Percent complete: 51.4%; Average loss: 4.2707\n","Iteration: 2059; Percent complete: 51.5%; Average loss: 4.3774\n","Iteration: 2060; Percent complete: 51.5%; Average loss: 4.2658\n","Iteration: 2061; Percent complete: 51.5%; Average loss: 4.3620\n","Iteration: 2062; Percent complete: 51.5%; Average loss: 4.3871\n","Iteration: 2063; Percent complete: 51.6%; Average loss: 4.2512\n","Iteration: 2064; Percent complete: 51.6%; Average loss: 4.5106\n","Iteration: 2065; Percent complete: 51.6%; Average loss: 4.4279\n","Iteration: 2066; Percent complete: 51.6%; Average loss: 4.3152\n","Iteration: 2067; Percent complete: 51.7%; Average loss: 4.3838\n","Iteration: 2068; Percent complete: 51.7%; Average loss: 4.1677\n","Iteration: 2069; Percent complete: 51.7%; Average loss: 4.3021\n","Iteration: 2070; Percent complete: 51.7%; Average loss: 4.0692\n","Iteration: 2071; Percent complete: 51.8%; Average loss: 4.2523\n","Iteration: 2072; Percent complete: 51.8%; Average loss: 4.5834\n","Iteration: 2073; Percent complete: 51.8%; Average loss: 4.4099\n","Iteration: 2074; Percent complete: 51.8%; Average loss: 4.3538\n","Iteration: 2075; Percent complete: 51.9%; Average loss: 4.5106\n","Iteration: 2076; Percent complete: 51.9%; Average loss: 4.1383\n","Iteration: 2077; Percent complete: 51.9%; Average loss: 4.3749\n","Iteration: 2078; Percent complete: 51.9%; Average loss: 4.2649\n","Iteration: 2079; Percent complete: 52.0%; Average loss: 4.4458\n","Iteration: 2080; Percent complete: 52.0%; Average loss: 4.3008\n","Iteration: 2081; Percent complete: 52.0%; Average loss: 4.3110\n","Iteration: 2082; Percent complete: 52.0%; Average loss: 4.3148\n","Iteration: 2083; Percent complete: 52.1%; Average loss: 4.0638\n","Iteration: 2084; Percent complete: 52.1%; Average loss: 4.3029\n","Iteration: 2085; Percent complete: 52.1%; Average loss: 4.2641\n","Iteration: 2086; Percent complete: 52.1%; Average loss: 4.2827\n","Iteration: 2087; Percent complete: 52.2%; Average loss: 4.2356\n","Iteration: 2088; Percent complete: 52.2%; Average loss: 4.4174\n","Iteration: 2089; Percent complete: 52.2%; Average loss: 4.1928\n","Iteration: 2090; Percent complete: 52.2%; Average loss: 4.5065\n","Iteration: 2091; Percent complete: 52.3%; Average loss: 4.3492\n","Iteration: 2092; Percent complete: 52.3%; Average loss: 4.3564\n","Iteration: 2093; Percent complete: 52.3%; Average loss: 4.3431\n","Iteration: 2094; Percent complete: 52.3%; Average loss: 4.4237\n","Iteration: 2095; Percent complete: 52.4%; Average loss: 4.2489\n","Iteration: 2096; Percent complete: 52.4%; Average loss: 4.2745\n","Iteration: 2097; Percent complete: 52.4%; Average loss: 4.4710\n","Iteration: 2098; Percent complete: 52.4%; Average loss: 4.2867\n","Iteration: 2099; Percent complete: 52.5%; Average loss: 3.9926\n","Iteration: 2100; Percent complete: 52.5%; Average loss: 4.5699\n","Iteration: 2101; Percent complete: 52.5%; Average loss: 4.5548\n","Iteration: 2102; Percent complete: 52.5%; Average loss: 4.3515\n","Iteration: 2103; Percent complete: 52.6%; Average loss: 4.2955\n","Iteration: 2104; Percent complete: 52.6%; Average loss: 4.4376\n","Iteration: 2105; Percent complete: 52.6%; Average loss: 4.4175\n","Iteration: 2106; Percent complete: 52.6%; Average loss: 4.1243\n","Iteration: 2107; Percent complete: 52.7%; Average loss: 4.5920\n","Iteration: 2108; Percent complete: 52.7%; Average loss: 4.3638\n","Iteration: 2109; Percent complete: 52.7%; Average loss: 4.0438\n","Iteration: 2110; Percent complete: 52.8%; Average loss: 4.4953\n","Iteration: 2111; Percent complete: 52.8%; Average loss: 4.3241\n","Iteration: 2112; Percent complete: 52.8%; Average loss: 4.4244\n","Iteration: 2113; Percent complete: 52.8%; Average loss: 4.3164\n","Iteration: 2114; Percent complete: 52.8%; Average loss: 4.3331\n","Iteration: 2115; Percent complete: 52.9%; Average loss: 4.3111\n","Iteration: 2116; Percent complete: 52.9%; Average loss: 4.4317\n","Iteration: 2117; Percent complete: 52.9%; Average loss: 4.1751\n","Iteration: 2118; Percent complete: 52.9%; Average loss: 4.2889\n","Iteration: 2119; Percent complete: 53.0%; Average loss: 4.0947\n","Iteration: 2120; Percent complete: 53.0%; Average loss: 4.2926\n","Iteration: 2121; Percent complete: 53.0%; Average loss: 4.2671\n","Iteration: 2122; Percent complete: 53.0%; Average loss: 4.4922\n","Iteration: 2123; Percent complete: 53.1%; Average loss: 4.1001\n","Iteration: 2124; Percent complete: 53.1%; Average loss: 4.2782\n","Iteration: 2125; Percent complete: 53.1%; Average loss: 4.3309\n","Iteration: 2126; Percent complete: 53.1%; Average loss: 4.3279\n","Iteration: 2127; Percent complete: 53.2%; Average loss: 4.2353\n","Iteration: 2128; Percent complete: 53.2%; Average loss: 4.5801\n","Iteration: 2129; Percent complete: 53.2%; Average loss: 4.2115\n","Iteration: 2130; Percent complete: 53.2%; Average loss: 4.1500\n","Iteration: 2131; Percent complete: 53.3%; Average loss: 4.3747\n","Iteration: 2132; Percent complete: 53.3%; Average loss: 4.1075\n","Iteration: 2133; Percent complete: 53.3%; Average loss: 4.4201\n","Iteration: 2134; Percent complete: 53.3%; Average loss: 4.4051\n","Iteration: 2135; Percent complete: 53.4%; Average loss: 4.4208\n","Iteration: 2136; Percent complete: 53.4%; Average loss: 4.1993\n","Iteration: 2137; Percent complete: 53.4%; Average loss: 4.4239\n","Iteration: 2138; Percent complete: 53.4%; Average loss: 4.2511\n","Iteration: 2139; Percent complete: 53.5%; Average loss: 4.2570\n","Iteration: 2140; Percent complete: 53.5%; Average loss: 4.4105\n","Iteration: 2141; Percent complete: 53.5%; Average loss: 4.4048\n","Iteration: 2142; Percent complete: 53.5%; Average loss: 4.3937\n","Iteration: 2143; Percent complete: 53.6%; Average loss: 4.4307\n","Iteration: 2144; Percent complete: 53.6%; Average loss: 4.2652\n","Iteration: 2145; Percent complete: 53.6%; Average loss: 4.3501\n","Iteration: 2146; Percent complete: 53.6%; Average loss: 4.3590\n","Iteration: 2147; Percent complete: 53.7%; Average loss: 4.3023\n","Iteration: 2148; Percent complete: 53.7%; Average loss: 4.3742\n","Iteration: 2149; Percent complete: 53.7%; Average loss: 4.1424\n","Iteration: 2150; Percent complete: 53.8%; Average loss: 4.2927\n","Iteration: 2151; Percent complete: 53.8%; Average loss: 4.1826\n","Iteration: 2152; Percent complete: 53.8%; Average loss: 4.1930\n","Iteration: 2153; Percent complete: 53.8%; Average loss: 4.3318\n","Iteration: 2154; Percent complete: 53.8%; Average loss: 4.1732\n","Iteration: 2155; Percent complete: 53.9%; Average loss: 4.1969\n","Iteration: 2156; Percent complete: 53.9%; Average loss: 4.3647\n","Iteration: 2157; Percent complete: 53.9%; Average loss: 4.3408\n","Iteration: 2158; Percent complete: 53.9%; Average loss: 4.4309\n","Iteration: 2159; Percent complete: 54.0%; Average loss: 4.4938\n","Iteration: 2160; Percent complete: 54.0%; Average loss: 4.2256\n","Iteration: 2161; Percent complete: 54.0%; Average loss: 4.3416\n","Iteration: 2162; Percent complete: 54.0%; Average loss: 4.2172\n","Iteration: 2163; Percent complete: 54.1%; Average loss: 4.4631\n","Iteration: 2164; Percent complete: 54.1%; Average loss: 4.2482\n","Iteration: 2165; Percent complete: 54.1%; Average loss: 4.2906\n","Iteration: 2166; Percent complete: 54.1%; Average loss: 4.1512\n","Iteration: 2167; Percent complete: 54.2%; Average loss: 4.3904\n","Iteration: 2168; Percent complete: 54.2%; Average loss: 4.0626\n","Iteration: 2169; Percent complete: 54.2%; Average loss: 4.2466\n","Iteration: 2170; Percent complete: 54.2%; Average loss: 4.5411\n","Iteration: 2171; Percent complete: 54.3%; Average loss: 4.3710\n","Iteration: 2172; Percent complete: 54.3%; Average loss: 4.0952\n","Iteration: 2173; Percent complete: 54.3%; Average loss: 4.2704\n","Iteration: 2174; Percent complete: 54.4%; Average loss: 4.4017\n","Iteration: 2175; Percent complete: 54.4%; Average loss: 4.2656\n","Iteration: 2176; Percent complete: 54.4%; Average loss: 4.3383\n","Iteration: 2177; Percent complete: 54.4%; Average loss: 4.2255\n","Iteration: 2178; Percent complete: 54.4%; Average loss: 4.2430\n","Iteration: 2179; Percent complete: 54.5%; Average loss: 4.3635\n","Iteration: 2180; Percent complete: 54.5%; Average loss: 4.2959\n","Iteration: 2181; Percent complete: 54.5%; Average loss: 4.4826\n","Iteration: 2182; Percent complete: 54.5%; Average loss: 4.3808\n","Iteration: 2183; Percent complete: 54.6%; Average loss: 4.3364\n","Iteration: 2184; Percent complete: 54.6%; Average loss: 4.0936\n","Iteration: 2185; Percent complete: 54.6%; Average loss: 4.3580\n","Iteration: 2186; Percent complete: 54.6%; Average loss: 4.2236\n","Iteration: 2187; Percent complete: 54.7%; Average loss: 4.1493\n","Iteration: 2188; Percent complete: 54.7%; Average loss: 4.4130\n","Iteration: 2189; Percent complete: 54.7%; Average loss: 4.4514\n","Iteration: 2190; Percent complete: 54.8%; Average loss: 4.3918\n","Iteration: 2191; Percent complete: 54.8%; Average loss: 4.2196\n","Iteration: 2192; Percent complete: 54.8%; Average loss: 4.5758\n","Iteration: 2193; Percent complete: 54.8%; Average loss: 4.3822\n","Iteration: 2194; Percent complete: 54.9%; Average loss: 4.2465\n","Iteration: 2195; Percent complete: 54.9%; Average loss: 4.4801\n","Iteration: 2196; Percent complete: 54.9%; Average loss: 4.4444\n","Iteration: 2197; Percent complete: 54.9%; Average loss: 4.3044\n","Iteration: 2198; Percent complete: 54.9%; Average loss: 4.3907\n","Iteration: 2199; Percent complete: 55.0%; Average loss: 4.2573\n","Iteration: 2200; Percent complete: 55.0%; Average loss: 4.3580\n","Iteration: 2201; Percent complete: 55.0%; Average loss: 4.2339\n","Iteration: 2202; Percent complete: 55.0%; Average loss: 4.4879\n","Iteration: 2203; Percent complete: 55.1%; Average loss: 4.3152\n","Iteration: 2204; Percent complete: 55.1%; Average loss: 4.5580\n","Iteration: 2205; Percent complete: 55.1%; Average loss: 4.1400\n","Iteration: 2206; Percent complete: 55.1%; Average loss: 4.2895\n","Iteration: 2207; Percent complete: 55.2%; Average loss: 4.2185\n","Iteration: 2208; Percent complete: 55.2%; Average loss: 4.3451\n","Iteration: 2209; Percent complete: 55.2%; Average loss: 4.4216\n","Iteration: 2210; Percent complete: 55.2%; Average loss: 4.3001\n","Iteration: 2211; Percent complete: 55.3%; Average loss: 4.4198\n","Iteration: 2212; Percent complete: 55.3%; Average loss: 3.9935\n","Iteration: 2213; Percent complete: 55.3%; Average loss: 4.3194\n","Iteration: 2214; Percent complete: 55.4%; Average loss: 4.1662\n","Iteration: 2215; Percent complete: 55.4%; Average loss: 4.2505\n","Iteration: 2216; Percent complete: 55.4%; Average loss: 4.1891\n","Iteration: 2217; Percent complete: 55.4%; Average loss: 4.3918\n","Iteration: 2218; Percent complete: 55.5%; Average loss: 4.4788\n","Iteration: 2219; Percent complete: 55.5%; Average loss: 4.2771\n","Iteration: 2220; Percent complete: 55.5%; Average loss: 4.2786\n","Iteration: 2221; Percent complete: 55.5%; Average loss: 4.2967\n","Iteration: 2222; Percent complete: 55.5%; Average loss: 4.1383\n","Iteration: 2223; Percent complete: 55.6%; Average loss: 4.1335\n","Iteration: 2224; Percent complete: 55.6%; Average loss: 4.4902\n","Iteration: 2225; Percent complete: 55.6%; Average loss: 4.0180\n","Iteration: 2226; Percent complete: 55.6%; Average loss: 4.2339\n","Iteration: 2227; Percent complete: 55.7%; Average loss: 4.5232\n","Iteration: 2228; Percent complete: 55.7%; Average loss: 4.4133\n","Iteration: 2229; Percent complete: 55.7%; Average loss: 4.3163\n","Iteration: 2230; Percent complete: 55.8%; Average loss: 4.3499\n","Iteration: 2231; Percent complete: 55.8%; Average loss: 4.3448\n","Iteration: 2232; Percent complete: 55.8%; Average loss: 4.0929\n","Iteration: 2233; Percent complete: 55.8%; Average loss: 4.4058\n","Iteration: 2234; Percent complete: 55.9%; Average loss: 4.1670\n","Iteration: 2235; Percent complete: 55.9%; Average loss: 4.1312\n","Iteration: 2236; Percent complete: 55.9%; Average loss: 4.4894\n","Iteration: 2237; Percent complete: 55.9%; Average loss: 4.2406\n","Iteration: 2238; Percent complete: 56.0%; Average loss: 4.4276\n","Iteration: 2239; Percent complete: 56.0%; Average loss: 4.3897\n","Iteration: 2240; Percent complete: 56.0%; Average loss: 4.3209\n","Iteration: 2241; Percent complete: 56.0%; Average loss: 4.4067\n","Iteration: 2242; Percent complete: 56.0%; Average loss: 4.3886\n","Iteration: 2243; Percent complete: 56.1%; Average loss: 4.2584\n","Iteration: 2244; Percent complete: 56.1%; Average loss: 3.9758\n","Iteration: 2245; Percent complete: 56.1%; Average loss: 4.3027\n","Iteration: 2246; Percent complete: 56.1%; Average loss: 4.2304\n","Iteration: 2247; Percent complete: 56.2%; Average loss: 4.3311\n","Iteration: 2248; Percent complete: 56.2%; Average loss: 4.4307\n","Iteration: 2249; Percent complete: 56.2%; Average loss: 4.3589\n","Iteration: 2250; Percent complete: 56.2%; Average loss: 4.2627\n","Iteration: 2251; Percent complete: 56.3%; Average loss: 4.4367\n","Iteration: 2252; Percent complete: 56.3%; Average loss: 4.1597\n","Iteration: 2253; Percent complete: 56.3%; Average loss: 4.2729\n","Iteration: 2254; Percent complete: 56.4%; Average loss: 4.0941\n","Iteration: 2255; Percent complete: 56.4%; Average loss: 4.2503\n","Iteration: 2256; Percent complete: 56.4%; Average loss: 4.0956\n","Iteration: 2257; Percent complete: 56.4%; Average loss: 4.3448\n","Iteration: 2258; Percent complete: 56.5%; Average loss: 4.3587\n","Iteration: 2259; Percent complete: 56.5%; Average loss: 4.4583\n","Iteration: 2260; Percent complete: 56.5%; Average loss: 3.9172\n","Iteration: 2261; Percent complete: 56.5%; Average loss: 3.9959\n","Iteration: 2262; Percent complete: 56.5%; Average loss: 4.3210\n","Iteration: 2263; Percent complete: 56.6%; Average loss: 4.3881\n","Iteration: 2264; Percent complete: 56.6%; Average loss: 4.2534\n","Iteration: 2265; Percent complete: 56.6%; Average loss: 4.6010\n","Iteration: 2266; Percent complete: 56.6%; Average loss: 4.2489\n","Iteration: 2267; Percent complete: 56.7%; Average loss: 4.4977\n","Iteration: 2268; Percent complete: 56.7%; Average loss: 4.6558\n","Iteration: 2269; Percent complete: 56.7%; Average loss: 4.2613\n","Iteration: 2270; Percent complete: 56.8%; Average loss: 4.2456\n","Iteration: 2271; Percent complete: 56.8%; Average loss: 4.1567\n","Iteration: 2272; Percent complete: 56.8%; Average loss: 4.2005\n","Iteration: 2273; Percent complete: 56.8%; Average loss: 4.3467\n","Iteration: 2274; Percent complete: 56.9%; Average loss: 4.3418\n","Iteration: 2275; Percent complete: 56.9%; Average loss: 4.3588\n","Iteration: 2276; Percent complete: 56.9%; Average loss: 4.2076\n","Iteration: 2277; Percent complete: 56.9%; Average loss: 4.4396\n","Iteration: 2278; Percent complete: 57.0%; Average loss: 4.2991\n","Iteration: 2279; Percent complete: 57.0%; Average loss: 4.1811\n","Iteration: 2280; Percent complete: 57.0%; Average loss: 4.3882\n","Iteration: 2281; Percent complete: 57.0%; Average loss: 4.1864\n","Iteration: 2282; Percent complete: 57.0%; Average loss: 4.2735\n","Iteration: 2283; Percent complete: 57.1%; Average loss: 4.3154\n","Iteration: 2284; Percent complete: 57.1%; Average loss: 4.0937\n","Iteration: 2285; Percent complete: 57.1%; Average loss: 4.3811\n","Iteration: 2286; Percent complete: 57.1%; Average loss: 4.5048\n","Iteration: 2287; Percent complete: 57.2%; Average loss: 4.3416\n","Iteration: 2288; Percent complete: 57.2%; Average loss: 4.1290\n","Iteration: 2289; Percent complete: 57.2%; Average loss: 4.1190\n","Iteration: 2290; Percent complete: 57.2%; Average loss: 4.3996\n","Iteration: 2291; Percent complete: 57.3%; Average loss: 4.1550\n","Iteration: 2292; Percent complete: 57.3%; Average loss: 4.3159\n","Iteration: 2293; Percent complete: 57.3%; Average loss: 4.3390\n","Iteration: 2294; Percent complete: 57.4%; Average loss: 4.4963\n","Iteration: 2295; Percent complete: 57.4%; Average loss: 4.2265\n","Iteration: 2296; Percent complete: 57.4%; Average loss: 4.2951\n","Iteration: 2297; Percent complete: 57.4%; Average loss: 4.3300\n","Iteration: 2298; Percent complete: 57.5%; Average loss: 4.2133\n","Iteration: 2299; Percent complete: 57.5%; Average loss: 4.4805\n","Iteration: 2300; Percent complete: 57.5%; Average loss: 4.3037\n","Iteration: 2301; Percent complete: 57.5%; Average loss: 4.2518\n","Iteration: 2302; Percent complete: 57.6%; Average loss: 4.3388\n","Iteration: 2303; Percent complete: 57.6%; Average loss: 4.2691\n","Iteration: 2304; Percent complete: 57.6%; Average loss: 4.2648\n","Iteration: 2305; Percent complete: 57.6%; Average loss: 4.3398\n","Iteration: 2306; Percent complete: 57.6%; Average loss: 4.1888\n","Iteration: 2307; Percent complete: 57.7%; Average loss: 4.1481\n","Iteration: 2308; Percent complete: 57.7%; Average loss: 4.1168\n","Iteration: 2309; Percent complete: 57.7%; Average loss: 4.4338\n","Iteration: 2310; Percent complete: 57.8%; Average loss: 4.0771\n","Iteration: 2311; Percent complete: 57.8%; Average loss: 4.2632\n","Iteration: 2312; Percent complete: 57.8%; Average loss: 4.3009\n","Iteration: 2313; Percent complete: 57.8%; Average loss: 4.2355\n","Iteration: 2314; Percent complete: 57.9%; Average loss: 4.1994\n","Iteration: 2315; Percent complete: 57.9%; Average loss: 4.3499\n","Iteration: 2316; Percent complete: 57.9%; Average loss: 4.5052\n","Iteration: 2317; Percent complete: 57.9%; Average loss: 4.2773\n","Iteration: 2318; Percent complete: 58.0%; Average loss: 4.1982\n","Iteration: 2319; Percent complete: 58.0%; Average loss: 4.2059\n","Iteration: 2320; Percent complete: 58.0%; Average loss: 4.3390\n","Iteration: 2321; Percent complete: 58.0%; Average loss: 4.3232\n","Iteration: 2322; Percent complete: 58.1%; Average loss: 4.2154\n","Iteration: 2323; Percent complete: 58.1%; Average loss: 4.2213\n","Iteration: 2324; Percent complete: 58.1%; Average loss: 4.1484\n","Iteration: 2325; Percent complete: 58.1%; Average loss: 4.3037\n","Iteration: 2326; Percent complete: 58.1%; Average loss: 4.3027\n","Iteration: 2327; Percent complete: 58.2%; Average loss: 4.2971\n","Iteration: 2328; Percent complete: 58.2%; Average loss: 4.2823\n","Iteration: 2329; Percent complete: 58.2%; Average loss: 4.4456\n","Iteration: 2330; Percent complete: 58.2%; Average loss: 4.3450\n","Iteration: 2331; Percent complete: 58.3%; Average loss: 4.2842\n","Iteration: 2332; Percent complete: 58.3%; Average loss: 4.5223\n","Iteration: 2333; Percent complete: 58.3%; Average loss: 4.3976\n","Iteration: 2334; Percent complete: 58.4%; Average loss: 4.2868\n","Iteration: 2335; Percent complete: 58.4%; Average loss: 4.2919\n","Iteration: 2336; Percent complete: 58.4%; Average loss: 4.4170\n","Iteration: 2337; Percent complete: 58.4%; Average loss: 4.2772\n","Iteration: 2338; Percent complete: 58.5%; Average loss: 4.2847\n","Iteration: 2339; Percent complete: 58.5%; Average loss: 4.1493\n","Iteration: 2340; Percent complete: 58.5%; Average loss: 4.0436\n","Iteration: 2341; Percent complete: 58.5%; Average loss: 4.2666\n","Iteration: 2342; Percent complete: 58.6%; Average loss: 4.2086\n","Iteration: 2343; Percent complete: 58.6%; Average loss: 4.0397\n","Iteration: 2344; Percent complete: 58.6%; Average loss: 4.4528\n","Iteration: 2345; Percent complete: 58.6%; Average loss: 4.3866\n","Iteration: 2346; Percent complete: 58.7%; Average loss: 4.2948\n","Iteration: 2347; Percent complete: 58.7%; Average loss: 4.3858\n","Iteration: 2348; Percent complete: 58.7%; Average loss: 4.3894\n","Iteration: 2349; Percent complete: 58.7%; Average loss: 4.2419\n","Iteration: 2350; Percent complete: 58.8%; Average loss: 4.1848\n","Iteration: 2351; Percent complete: 58.8%; Average loss: 4.2682\n","Iteration: 2352; Percent complete: 58.8%; Average loss: 4.2486\n","Iteration: 2353; Percent complete: 58.8%; Average loss: 4.3861\n","Iteration: 2354; Percent complete: 58.9%; Average loss: 4.4433\n","Iteration: 2355; Percent complete: 58.9%; Average loss: 4.2188\n","Iteration: 2356; Percent complete: 58.9%; Average loss: 4.3454\n","Iteration: 2357; Percent complete: 58.9%; Average loss: 4.3259\n","Iteration: 2358; Percent complete: 59.0%; Average loss: 4.1573\n","Iteration: 2359; Percent complete: 59.0%; Average loss: 4.1984\n","Iteration: 2360; Percent complete: 59.0%; Average loss: 4.3638\n","Iteration: 2361; Percent complete: 59.0%; Average loss: 4.1880\n","Iteration: 2362; Percent complete: 59.1%; Average loss: 4.1646\n","Iteration: 2363; Percent complete: 59.1%; Average loss: 4.2287\n","Iteration: 2364; Percent complete: 59.1%; Average loss: 4.2763\n","Iteration: 2365; Percent complete: 59.1%; Average loss: 4.2457\n","Iteration: 2366; Percent complete: 59.2%; Average loss: 4.1298\n","Iteration: 2367; Percent complete: 59.2%; Average loss: 4.2301\n","Iteration: 2368; Percent complete: 59.2%; Average loss: 4.6422\n","Iteration: 2369; Percent complete: 59.2%; Average loss: 4.1744\n","Iteration: 2370; Percent complete: 59.2%; Average loss: 4.2187\n","Iteration: 2371; Percent complete: 59.3%; Average loss: 4.1656\n","Iteration: 2372; Percent complete: 59.3%; Average loss: 4.4123\n","Iteration: 2373; Percent complete: 59.3%; Average loss: 4.3459\n","Iteration: 2374; Percent complete: 59.4%; Average loss: 3.9423\n","Iteration: 2375; Percent complete: 59.4%; Average loss: 4.2196\n","Iteration: 2376; Percent complete: 59.4%; Average loss: 4.3566\n","Iteration: 2377; Percent complete: 59.4%; Average loss: 4.2081\n","Iteration: 2378; Percent complete: 59.5%; Average loss: 4.2049\n","Iteration: 2379; Percent complete: 59.5%; Average loss: 4.2070\n","Iteration: 2380; Percent complete: 59.5%; Average loss: 4.3300\n","Iteration: 2381; Percent complete: 59.5%; Average loss: 4.3607\n","Iteration: 2382; Percent complete: 59.6%; Average loss: 4.3473\n","Iteration: 2383; Percent complete: 59.6%; Average loss: 4.3729\n","Iteration: 2384; Percent complete: 59.6%; Average loss: 4.4002\n","Iteration: 2385; Percent complete: 59.6%; Average loss: 4.2844\n","Iteration: 2386; Percent complete: 59.7%; Average loss: 4.2052\n","Iteration: 2387; Percent complete: 59.7%; Average loss: 4.1498\n","Iteration: 2388; Percent complete: 59.7%; Average loss: 4.3500\n","Iteration: 2389; Percent complete: 59.7%; Average loss: 4.3181\n","Iteration: 2390; Percent complete: 59.8%; Average loss: 4.4103\n","Iteration: 2391; Percent complete: 59.8%; Average loss: 4.4226\n","Iteration: 2392; Percent complete: 59.8%; Average loss: 4.3550\n","Iteration: 2393; Percent complete: 59.8%; Average loss: 4.3006\n","Iteration: 2394; Percent complete: 59.9%; Average loss: 4.2657\n","Iteration: 2395; Percent complete: 59.9%; Average loss: 4.3351\n","Iteration: 2396; Percent complete: 59.9%; Average loss: 4.3174\n","Iteration: 2397; Percent complete: 59.9%; Average loss: 4.3772\n","Iteration: 2398; Percent complete: 60.0%; Average loss: 4.5008\n","Iteration: 2399; Percent complete: 60.0%; Average loss: 4.3370\n","Iteration: 2400; Percent complete: 60.0%; Average loss: 4.2734\n","Iteration: 2401; Percent complete: 60.0%; Average loss: 4.5397\n","Iteration: 2402; Percent complete: 60.1%; Average loss: 4.0719\n","Iteration: 2403; Percent complete: 60.1%; Average loss: 4.3781\n","Iteration: 2404; Percent complete: 60.1%; Average loss: 4.4320\n","Iteration: 2405; Percent complete: 60.1%; Average loss: 4.3227\n","Iteration: 2406; Percent complete: 60.2%; Average loss: 4.2072\n","Iteration: 2407; Percent complete: 60.2%; Average loss: 4.3579\n","Iteration: 2408; Percent complete: 60.2%; Average loss: 4.4087\n","Iteration: 2409; Percent complete: 60.2%; Average loss: 4.2602\n","Iteration: 2410; Percent complete: 60.2%; Average loss: 4.1550\n","Iteration: 2411; Percent complete: 60.3%; Average loss: 4.1822\n","Iteration: 2412; Percent complete: 60.3%; Average loss: 4.2336\n","Iteration: 2413; Percent complete: 60.3%; Average loss: 4.2312\n","Iteration: 2414; Percent complete: 60.4%; Average loss: 4.3335\n","Iteration: 2415; Percent complete: 60.4%; Average loss: 4.1307\n","Iteration: 2416; Percent complete: 60.4%; Average loss: 4.4946\n","Iteration: 2417; Percent complete: 60.4%; Average loss: 4.1891\n","Iteration: 2418; Percent complete: 60.5%; Average loss: 4.2063\n","Iteration: 2419; Percent complete: 60.5%; Average loss: 4.2744\n","Iteration: 2420; Percent complete: 60.5%; Average loss: 4.2096\n","Iteration: 2421; Percent complete: 60.5%; Average loss: 4.1591\n","Iteration: 2422; Percent complete: 60.6%; Average loss: 4.2774\n","Iteration: 2423; Percent complete: 60.6%; Average loss: 4.2851\n","Iteration: 2424; Percent complete: 60.6%; Average loss: 4.0761\n","Iteration: 2425; Percent complete: 60.6%; Average loss: 4.2105\n","Iteration: 2426; Percent complete: 60.7%; Average loss: 4.3144\n","Iteration: 2427; Percent complete: 60.7%; Average loss: 4.2015\n","Iteration: 2428; Percent complete: 60.7%; Average loss: 4.1174\n","Iteration: 2429; Percent complete: 60.7%; Average loss: 4.2776\n","Iteration: 2430; Percent complete: 60.8%; Average loss: 4.4862\n","Iteration: 2431; Percent complete: 60.8%; Average loss: 4.2003\n","Iteration: 2432; Percent complete: 60.8%; Average loss: 4.1693\n","Iteration: 2433; Percent complete: 60.8%; Average loss: 4.2871\n","Iteration: 2434; Percent complete: 60.9%; Average loss: 4.3520\n","Iteration: 2435; Percent complete: 60.9%; Average loss: 4.1613\n","Iteration: 2436; Percent complete: 60.9%; Average loss: 4.4013\n","Iteration: 2437; Percent complete: 60.9%; Average loss: 4.0438\n","Iteration: 2438; Percent complete: 61.0%; Average loss: 4.3362\n","Iteration: 2439; Percent complete: 61.0%; Average loss: 4.3083\n","Iteration: 2440; Percent complete: 61.0%; Average loss: 4.4971\n","Iteration: 2441; Percent complete: 61.0%; Average loss: 4.2945\n","Iteration: 2442; Percent complete: 61.1%; Average loss: 4.0579\n","Iteration: 2443; Percent complete: 61.1%; Average loss: 4.2069\n","Iteration: 2444; Percent complete: 61.1%; Average loss: 4.3579\n","Iteration: 2445; Percent complete: 61.1%; Average loss: 4.4552\n","Iteration: 2446; Percent complete: 61.2%; Average loss: 4.2969\n","Iteration: 2447; Percent complete: 61.2%; Average loss: 4.2030\n","Iteration: 2448; Percent complete: 61.2%; Average loss: 4.0292\n","Iteration: 2449; Percent complete: 61.2%; Average loss: 4.2766\n","Iteration: 2450; Percent complete: 61.3%; Average loss: 4.0722\n","Iteration: 2451; Percent complete: 61.3%; Average loss: 4.2691\n","Iteration: 2452; Percent complete: 61.3%; Average loss: 3.9889\n","Iteration: 2453; Percent complete: 61.3%; Average loss: 4.1279\n","Iteration: 2454; Percent complete: 61.4%; Average loss: 4.4638\n","Iteration: 2455; Percent complete: 61.4%; Average loss: 4.2444\n","Iteration: 2456; Percent complete: 61.4%; Average loss: 4.2333\n","Iteration: 2457; Percent complete: 61.4%; Average loss: 4.0837\n","Iteration: 2458; Percent complete: 61.5%; Average loss: 4.2926\n","Iteration: 2459; Percent complete: 61.5%; Average loss: 4.2731\n","Iteration: 2460; Percent complete: 61.5%; Average loss: 4.4575\n","Iteration: 2461; Percent complete: 61.5%; Average loss: 4.0917\n","Iteration: 2462; Percent complete: 61.6%; Average loss: 4.3560\n","Iteration: 2463; Percent complete: 61.6%; Average loss: 4.2089\n","Iteration: 2464; Percent complete: 61.6%; Average loss: 4.2767\n","Iteration: 2465; Percent complete: 61.6%; Average loss: 4.1112\n","Iteration: 2466; Percent complete: 61.7%; Average loss: 4.2764\n","Iteration: 2467; Percent complete: 61.7%; Average loss: 4.2418\n","Iteration: 2468; Percent complete: 61.7%; Average loss: 4.1547\n","Iteration: 2469; Percent complete: 61.7%; Average loss: 4.0767\n","Iteration: 2470; Percent complete: 61.8%; Average loss: 4.1498\n","Iteration: 2471; Percent complete: 61.8%; Average loss: 3.9665\n","Iteration: 2472; Percent complete: 61.8%; Average loss: 4.2489\n","Iteration: 2473; Percent complete: 61.8%; Average loss: 4.0396\n","Iteration: 2474; Percent complete: 61.9%; Average loss: 4.1697\n","Iteration: 2475; Percent complete: 61.9%; Average loss: 4.0994\n","Iteration: 2476; Percent complete: 61.9%; Average loss: 4.1533\n","Iteration: 2477; Percent complete: 61.9%; Average loss: 4.1931\n","Iteration: 2478; Percent complete: 62.0%; Average loss: 4.1298\n","Iteration: 2479; Percent complete: 62.0%; Average loss: 4.4590\n","Iteration: 2480; Percent complete: 62.0%; Average loss: 4.0220\n","Iteration: 2481; Percent complete: 62.0%; Average loss: 4.1503\n","Iteration: 2482; Percent complete: 62.1%; Average loss: 4.1727\n","Iteration: 2483; Percent complete: 62.1%; Average loss: 4.3247\n","Iteration: 2484; Percent complete: 62.1%; Average loss: 4.3337\n","Iteration: 2485; Percent complete: 62.1%; Average loss: 4.4929\n","Iteration: 2486; Percent complete: 62.2%; Average loss: 4.2596\n","Iteration: 2487; Percent complete: 62.2%; Average loss: 4.1672\n","Iteration: 2488; Percent complete: 62.2%; Average loss: 4.2968\n","Iteration: 2489; Percent complete: 62.2%; Average loss: 4.3364\n","Iteration: 2490; Percent complete: 62.3%; Average loss: 4.1964\n","Iteration: 2491; Percent complete: 62.3%; Average loss: 4.4635\n","Iteration: 2492; Percent complete: 62.3%; Average loss: 4.1956\n","Iteration: 2493; Percent complete: 62.3%; Average loss: 4.3974\n","Iteration: 2494; Percent complete: 62.4%; Average loss: 4.2082\n","Iteration: 2495; Percent complete: 62.4%; Average loss: 4.1736\n","Iteration: 2496; Percent complete: 62.4%; Average loss: 4.1837\n","Iteration: 2497; Percent complete: 62.4%; Average loss: 4.1399\n","Iteration: 2498; Percent complete: 62.5%; Average loss: 4.0786\n","Iteration: 2499; Percent complete: 62.5%; Average loss: 4.2457\n","Iteration: 2500; Percent complete: 62.5%; Average loss: 4.3509\n","Iteration: 2501; Percent complete: 62.5%; Average loss: 4.1455\n","Iteration: 2502; Percent complete: 62.5%; Average loss: 4.3485\n","Iteration: 2503; Percent complete: 62.6%; Average loss: 4.1087\n","Iteration: 2504; Percent complete: 62.6%; Average loss: 4.3356\n","Iteration: 2505; Percent complete: 62.6%; Average loss: 4.1234\n","Iteration: 2506; Percent complete: 62.6%; Average loss: 4.3837\n","Iteration: 2507; Percent complete: 62.7%; Average loss: 4.0455\n","Iteration: 2508; Percent complete: 62.7%; Average loss: 4.1438\n","Iteration: 2509; Percent complete: 62.7%; Average loss: 4.3413\n","Iteration: 2510; Percent complete: 62.7%; Average loss: 4.1582\n","Iteration: 2511; Percent complete: 62.8%; Average loss: 4.5738\n","Iteration: 2512; Percent complete: 62.8%; Average loss: 4.1069\n","Iteration: 2513; Percent complete: 62.8%; Average loss: 4.3526\n","Iteration: 2514; Percent complete: 62.8%; Average loss: 4.1370\n","Iteration: 2515; Percent complete: 62.9%; Average loss: 4.3555\n","Iteration: 2516; Percent complete: 62.9%; Average loss: 4.1135\n","Iteration: 2517; Percent complete: 62.9%; Average loss: 4.4875\n","Iteration: 2518; Percent complete: 62.9%; Average loss: 4.3276\n","Iteration: 2519; Percent complete: 63.0%; Average loss: 4.3109\n","Iteration: 2520; Percent complete: 63.0%; Average loss: 4.2088\n","Iteration: 2521; Percent complete: 63.0%; Average loss: 4.1973\n","Iteration: 2522; Percent complete: 63.0%; Average loss: 4.1499\n","Iteration: 2523; Percent complete: 63.1%; Average loss: 4.2227\n","Iteration: 2524; Percent complete: 63.1%; Average loss: 4.4396\n","Iteration: 2525; Percent complete: 63.1%; Average loss: 4.1141\n","Iteration: 2526; Percent complete: 63.1%; Average loss: 4.4645\n","Iteration: 2527; Percent complete: 63.2%; Average loss: 4.2184\n","Iteration: 2528; Percent complete: 63.2%; Average loss: 4.3586\n","Iteration: 2529; Percent complete: 63.2%; Average loss: 4.0981\n","Iteration: 2530; Percent complete: 63.2%; Average loss: 4.2345\n","Iteration: 2531; Percent complete: 63.3%; Average loss: 4.2283\n","Iteration: 2532; Percent complete: 63.3%; Average loss: 4.2615\n","Iteration: 2533; Percent complete: 63.3%; Average loss: 4.2288\n","Iteration: 2534; Percent complete: 63.3%; Average loss: 4.2736\n","Iteration: 2535; Percent complete: 63.4%; Average loss: 4.1596\n","Iteration: 2536; Percent complete: 63.4%; Average loss: 4.1766\n","Iteration: 2537; Percent complete: 63.4%; Average loss: 4.1120\n","Iteration: 2538; Percent complete: 63.4%; Average loss: 4.2439\n","Iteration: 2539; Percent complete: 63.5%; Average loss: 4.3449\n","Iteration: 2540; Percent complete: 63.5%; Average loss: 4.2459\n","Iteration: 2541; Percent complete: 63.5%; Average loss: 4.4079\n","Iteration: 2542; Percent complete: 63.5%; Average loss: 4.4063\n","Iteration: 2543; Percent complete: 63.6%; Average loss: 4.0957\n","Iteration: 2544; Percent complete: 63.6%; Average loss: 4.1398\n","Iteration: 2545; Percent complete: 63.6%; Average loss: 4.4633\n","Iteration: 2546; Percent complete: 63.6%; Average loss: 4.0857\n","Iteration: 2547; Percent complete: 63.7%; Average loss: 4.3589\n","Iteration: 2548; Percent complete: 63.7%; Average loss: 4.1990\n","Iteration: 2549; Percent complete: 63.7%; Average loss: 4.3849\n","Iteration: 2550; Percent complete: 63.7%; Average loss: 4.3414\n","Iteration: 2551; Percent complete: 63.8%; Average loss: 3.9766\n","Iteration: 2552; Percent complete: 63.8%; Average loss: 4.3133\n","Iteration: 2553; Percent complete: 63.8%; Average loss: 4.1801\n","Iteration: 2554; Percent complete: 63.8%; Average loss: 4.3930\n","Iteration: 2555; Percent complete: 63.9%; Average loss: 3.9791\n","Iteration: 2556; Percent complete: 63.9%; Average loss: 4.1827\n","Iteration: 2557; Percent complete: 63.9%; Average loss: 4.3838\n","Iteration: 2558; Percent complete: 63.9%; Average loss: 4.2170\n","Iteration: 2559; Percent complete: 64.0%; Average loss: 4.2852\n","Iteration: 2560; Percent complete: 64.0%; Average loss: 4.3740\n","Iteration: 2561; Percent complete: 64.0%; Average loss: 4.2473\n","Iteration: 2562; Percent complete: 64.0%; Average loss: 4.2859\n","Iteration: 2563; Percent complete: 64.1%; Average loss: 4.1060\n","Iteration: 2564; Percent complete: 64.1%; Average loss: 4.0927\n","Iteration: 2565; Percent complete: 64.1%; Average loss: 4.3173\n","Iteration: 2566; Percent complete: 64.1%; Average loss: 4.2552\n","Iteration: 2567; Percent complete: 64.2%; Average loss: 4.3148\n","Iteration: 2568; Percent complete: 64.2%; Average loss: 4.1150\n","Iteration: 2569; Percent complete: 64.2%; Average loss: 4.0110\n","Iteration: 2570; Percent complete: 64.2%; Average loss: 4.3385\n","Iteration: 2571; Percent complete: 64.3%; Average loss: 4.2808\n","Iteration: 2572; Percent complete: 64.3%; Average loss: 4.4556\n","Iteration: 2573; Percent complete: 64.3%; Average loss: 4.0460\n","Iteration: 2574; Percent complete: 64.3%; Average loss: 4.1081\n","Iteration: 2575; Percent complete: 64.4%; Average loss: 3.9992\n","Iteration: 2576; Percent complete: 64.4%; Average loss: 4.4543\n","Iteration: 2577; Percent complete: 64.4%; Average loss: 4.1328\n","Iteration: 2578; Percent complete: 64.5%; Average loss: 4.3959\n","Iteration: 2579; Percent complete: 64.5%; Average loss: 4.4019\n","Iteration: 2580; Percent complete: 64.5%; Average loss: 4.2647\n","Iteration: 2581; Percent complete: 64.5%; Average loss: 4.1315\n","Iteration: 2582; Percent complete: 64.5%; Average loss: 4.0238\n","Iteration: 2583; Percent complete: 64.6%; Average loss: 4.3037\n","Iteration: 2584; Percent complete: 64.6%; Average loss: 4.0494\n","Iteration: 2585; Percent complete: 64.6%; Average loss: 4.2018\n","Iteration: 2586; Percent complete: 64.6%; Average loss: 4.3031\n","Iteration: 2587; Percent complete: 64.7%; Average loss: 4.4394\n","Iteration: 2588; Percent complete: 64.7%; Average loss: 4.1665\n","Iteration: 2589; Percent complete: 64.7%; Average loss: 4.3476\n","Iteration: 2590; Percent complete: 64.8%; Average loss: 4.1066\n","Iteration: 2591; Percent complete: 64.8%; Average loss: 4.3283\n","Iteration: 2592; Percent complete: 64.8%; Average loss: 4.0084\n","Iteration: 2593; Percent complete: 64.8%; Average loss: 4.3321\n","Iteration: 2594; Percent complete: 64.8%; Average loss: 4.2683\n","Iteration: 2595; Percent complete: 64.9%; Average loss: 4.3148\n","Iteration: 2596; Percent complete: 64.9%; Average loss: 4.1518\n","Iteration: 2597; Percent complete: 64.9%; Average loss: 4.0876\n","Iteration: 2598; Percent complete: 65.0%; Average loss: 4.1854\n","Iteration: 2599; Percent complete: 65.0%; Average loss: 4.0474\n","Iteration: 2600; Percent complete: 65.0%; Average loss: 4.3387\n","Iteration: 2601; Percent complete: 65.0%; Average loss: 4.2942\n","Iteration: 2602; Percent complete: 65.0%; Average loss: 3.9461\n","Iteration: 2603; Percent complete: 65.1%; Average loss: 4.4651\n","Iteration: 2604; Percent complete: 65.1%; Average loss: 4.2851\n","Iteration: 2605; Percent complete: 65.1%; Average loss: 4.3162\n","Iteration: 2606; Percent complete: 65.1%; Average loss: 4.0388\n","Iteration: 2607; Percent complete: 65.2%; Average loss: 4.2501\n","Iteration: 2608; Percent complete: 65.2%; Average loss: 4.3419\n","Iteration: 2609; Percent complete: 65.2%; Average loss: 4.1137\n","Iteration: 2610; Percent complete: 65.2%; Average loss: 4.4069\n","Iteration: 2611; Percent complete: 65.3%; Average loss: 4.4467\n","Iteration: 2612; Percent complete: 65.3%; Average loss: 4.2199\n","Iteration: 2613; Percent complete: 65.3%; Average loss: 4.1972\n","Iteration: 2614; Percent complete: 65.3%; Average loss: 4.0861\n","Iteration: 2615; Percent complete: 65.4%; Average loss: 4.1706\n","Iteration: 2616; Percent complete: 65.4%; Average loss: 4.0331\n","Iteration: 2617; Percent complete: 65.4%; Average loss: 3.8455\n","Iteration: 2618; Percent complete: 65.5%; Average loss: 4.0973\n","Iteration: 2619; Percent complete: 65.5%; Average loss: 4.2462\n","Iteration: 2620; Percent complete: 65.5%; Average loss: 4.2455\n","Iteration: 2621; Percent complete: 65.5%; Average loss: 4.3550\n","Iteration: 2622; Percent complete: 65.5%; Average loss: 4.2928\n","Iteration: 2623; Percent complete: 65.6%; Average loss: 4.2473\n","Iteration: 2624; Percent complete: 65.6%; Average loss: 4.0136\n","Iteration: 2625; Percent complete: 65.6%; Average loss: 4.1762\n","Iteration: 2626; Percent complete: 65.6%; Average loss: 4.3940\n","Iteration: 2627; Percent complete: 65.7%; Average loss: 4.1389\n","Iteration: 2628; Percent complete: 65.7%; Average loss: 4.1551\n","Iteration: 2629; Percent complete: 65.7%; Average loss: 4.3402\n","Iteration: 2630; Percent complete: 65.8%; Average loss: 4.2121\n","Iteration: 2631; Percent complete: 65.8%; Average loss: 3.9416\n","Iteration: 2632; Percent complete: 65.8%; Average loss: 4.1569\n","Iteration: 2633; Percent complete: 65.8%; Average loss: 4.1928\n","Iteration: 2634; Percent complete: 65.8%; Average loss: 4.1298\n","Iteration: 2635; Percent complete: 65.9%; Average loss: 4.2296\n","Iteration: 2636; Percent complete: 65.9%; Average loss: 4.3373\n","Iteration: 2637; Percent complete: 65.9%; Average loss: 4.1470\n","Iteration: 2638; Percent complete: 66.0%; Average loss: 4.2651\n","Iteration: 2639; Percent complete: 66.0%; Average loss: 4.2877\n","Iteration: 2640; Percent complete: 66.0%; Average loss: 4.2059\n","Iteration: 2641; Percent complete: 66.0%; Average loss: 4.1354\n","Iteration: 2642; Percent complete: 66.0%; Average loss: 4.1532\n","Iteration: 2643; Percent complete: 66.1%; Average loss: 4.2444\n","Iteration: 2644; Percent complete: 66.1%; Average loss: 4.1342\n","Iteration: 2645; Percent complete: 66.1%; Average loss: 4.2676\n","Iteration: 2646; Percent complete: 66.1%; Average loss: 4.2191\n","Iteration: 2647; Percent complete: 66.2%; Average loss: 4.1566\n","Iteration: 2648; Percent complete: 66.2%; Average loss: 4.3063\n","Iteration: 2649; Percent complete: 66.2%; Average loss: 4.2111\n","Iteration: 2650; Percent complete: 66.2%; Average loss: 4.3190\n","Iteration: 2651; Percent complete: 66.3%; Average loss: 4.2918\n","Iteration: 2652; Percent complete: 66.3%; Average loss: 4.2993\n","Iteration: 2653; Percent complete: 66.3%; Average loss: 4.3059\n","Iteration: 2654; Percent complete: 66.3%; Average loss: 3.8774\n","Iteration: 2655; Percent complete: 66.4%; Average loss: 4.2165\n","Iteration: 2656; Percent complete: 66.4%; Average loss: 4.2389\n","Iteration: 2657; Percent complete: 66.4%; Average loss: 4.2286\n","Iteration: 2658; Percent complete: 66.5%; Average loss: 3.9068\n","Iteration: 2659; Percent complete: 66.5%; Average loss: 4.0669\n","Iteration: 2660; Percent complete: 66.5%; Average loss: 4.0970\n","Iteration: 2661; Percent complete: 66.5%; Average loss: 4.2233\n","Iteration: 2662; Percent complete: 66.5%; Average loss: 4.1172\n","Iteration: 2663; Percent complete: 66.6%; Average loss: 4.1226\n","Iteration: 2664; Percent complete: 66.6%; Average loss: 4.3274\n","Iteration: 2665; Percent complete: 66.6%; Average loss: 3.9816\n","Iteration: 2666; Percent complete: 66.6%; Average loss: 4.1491\n","Iteration: 2667; Percent complete: 66.7%; Average loss: 4.2761\n","Iteration: 2668; Percent complete: 66.7%; Average loss: 4.0829\n","Iteration: 2669; Percent complete: 66.7%; Average loss: 4.2436\n","Iteration: 2670; Percent complete: 66.8%; Average loss: 4.2778\n","Iteration: 2671; Percent complete: 66.8%; Average loss: 4.1066\n","Iteration: 2672; Percent complete: 66.8%; Average loss: 4.3321\n","Iteration: 2673; Percent complete: 66.8%; Average loss: 4.0571\n","Iteration: 2674; Percent complete: 66.8%; Average loss: 4.4568\n","Iteration: 2675; Percent complete: 66.9%; Average loss: 4.1806\n","Iteration: 2676; Percent complete: 66.9%; Average loss: 4.2678\n","Iteration: 2677; Percent complete: 66.9%; Average loss: 4.2391\n","Iteration: 2678; Percent complete: 67.0%; Average loss: 4.5387\n","Iteration: 2679; Percent complete: 67.0%; Average loss: 4.1289\n","Iteration: 2680; Percent complete: 67.0%; Average loss: 4.2832\n","Iteration: 2681; Percent complete: 67.0%; Average loss: 4.1372\n","Iteration: 2682; Percent complete: 67.0%; Average loss: 4.2890\n","Iteration: 2683; Percent complete: 67.1%; Average loss: 4.2561\n","Iteration: 2684; Percent complete: 67.1%; Average loss: 4.2944\n","Iteration: 2685; Percent complete: 67.1%; Average loss: 4.1900\n","Iteration: 2686; Percent complete: 67.2%; Average loss: 4.1608\n","Iteration: 2687; Percent complete: 67.2%; Average loss: 4.0670\n","Iteration: 2688; Percent complete: 67.2%; Average loss: 4.1112\n","Iteration: 2689; Percent complete: 67.2%; Average loss: 4.1303\n","Iteration: 2690; Percent complete: 67.2%; Average loss: 4.2387\n","Iteration: 2691; Percent complete: 67.3%; Average loss: 4.3169\n","Iteration: 2692; Percent complete: 67.3%; Average loss: 4.3602\n","Iteration: 2693; Percent complete: 67.3%; Average loss: 3.9277\n","Iteration: 2694; Percent complete: 67.3%; Average loss: 4.1344\n","Iteration: 2695; Percent complete: 67.4%; Average loss: 4.2576\n","Iteration: 2696; Percent complete: 67.4%; Average loss: 4.0515\n","Iteration: 2697; Percent complete: 67.4%; Average loss: 4.1106\n","Iteration: 2698; Percent complete: 67.5%; Average loss: 4.2975\n","Iteration: 2699; Percent complete: 67.5%; Average loss: 3.9792\n","Iteration: 2700; Percent complete: 67.5%; Average loss: 4.0896\n","Iteration: 2701; Percent complete: 67.5%; Average loss: 4.3360\n","Iteration: 2702; Percent complete: 67.5%; Average loss: 4.2112\n","Iteration: 2703; Percent complete: 67.6%; Average loss: 4.2829\n","Iteration: 2704; Percent complete: 67.6%; Average loss: 4.1696\n","Iteration: 2705; Percent complete: 67.6%; Average loss: 4.1775\n","Iteration: 2706; Percent complete: 67.7%; Average loss: 4.0546\n","Iteration: 2707; Percent complete: 67.7%; Average loss: 3.9664\n","Iteration: 2708; Percent complete: 67.7%; Average loss: 4.1692\n","Iteration: 2709; Percent complete: 67.7%; Average loss: 4.3468\n","Iteration: 2710; Percent complete: 67.8%; Average loss: 4.3505\n","Iteration: 2711; Percent complete: 67.8%; Average loss: 4.0345\n","Iteration: 2712; Percent complete: 67.8%; Average loss: 4.0824\n","Iteration: 2713; Percent complete: 67.8%; Average loss: 4.3129\n","Iteration: 2714; Percent complete: 67.8%; Average loss: 4.1306\n","Iteration: 2715; Percent complete: 67.9%; Average loss: 4.0850\n","Iteration: 2716; Percent complete: 67.9%; Average loss: 4.3878\n","Iteration: 2717; Percent complete: 67.9%; Average loss: 4.0929\n","Iteration: 2718; Percent complete: 68.0%; Average loss: 4.3236\n","Iteration: 2719; Percent complete: 68.0%; Average loss: 3.9217\n","Iteration: 2720; Percent complete: 68.0%; Average loss: 4.2405\n","Iteration: 2721; Percent complete: 68.0%; Average loss: 4.1521\n","Iteration: 2722; Percent complete: 68.0%; Average loss: 4.0484\n","Iteration: 2723; Percent complete: 68.1%; Average loss: 3.9684\n","Iteration: 2724; Percent complete: 68.1%; Average loss: 4.0941\n","Iteration: 2725; Percent complete: 68.1%; Average loss: 4.0146\n","Iteration: 2726; Percent complete: 68.2%; Average loss: 4.1330\n","Iteration: 2727; Percent complete: 68.2%; Average loss: 4.1594\n","Iteration: 2728; Percent complete: 68.2%; Average loss: 3.9731\n","Iteration: 2729; Percent complete: 68.2%; Average loss: 4.0939\n","Iteration: 2730; Percent complete: 68.2%; Average loss: 4.2555\n","Iteration: 2731; Percent complete: 68.3%; Average loss: 4.1739\n","Iteration: 2732; Percent complete: 68.3%; Average loss: 3.9903\n","Iteration: 2733; Percent complete: 68.3%; Average loss: 4.0831\n","Iteration: 2734; Percent complete: 68.3%; Average loss: 4.4440\n","Iteration: 2735; Percent complete: 68.4%; Average loss: 4.0548\n","Iteration: 2736; Percent complete: 68.4%; Average loss: 4.1278\n","Iteration: 2737; Percent complete: 68.4%; Average loss: 4.1241\n","Iteration: 2738; Percent complete: 68.5%; Average loss: 4.1322\n","Iteration: 2739; Percent complete: 68.5%; Average loss: 4.4736\n","Iteration: 2740; Percent complete: 68.5%; Average loss: 4.2265\n","Iteration: 2741; Percent complete: 68.5%; Average loss: 4.1109\n","Iteration: 2742; Percent complete: 68.5%; Average loss: 4.2101\n","Iteration: 2743; Percent complete: 68.6%; Average loss: 4.2977\n","Iteration: 2744; Percent complete: 68.6%; Average loss: 4.1645\n","Iteration: 2745; Percent complete: 68.6%; Average loss: 4.2570\n","Iteration: 2746; Percent complete: 68.7%; Average loss: 4.1240\n","Iteration: 2747; Percent complete: 68.7%; Average loss: 4.1830\n","Iteration: 2748; Percent complete: 68.7%; Average loss: 4.2128\n","Iteration: 2749; Percent complete: 68.7%; Average loss: 4.4901\n","Iteration: 2750; Percent complete: 68.8%; Average loss: 4.0867\n","Iteration: 2751; Percent complete: 68.8%; Average loss: 4.1141\n","Iteration: 2752; Percent complete: 68.8%; Average loss: 4.3279\n","Iteration: 2753; Percent complete: 68.8%; Average loss: 4.1410\n","Iteration: 2754; Percent complete: 68.8%; Average loss: 3.9839\n","Iteration: 2755; Percent complete: 68.9%; Average loss: 4.0668\n","Iteration: 2756; Percent complete: 68.9%; Average loss: 4.0971\n","Iteration: 2757; Percent complete: 68.9%; Average loss: 4.0865\n","Iteration: 2758; Percent complete: 69.0%; Average loss: 4.2231\n","Iteration: 2759; Percent complete: 69.0%; Average loss: 4.1248\n","Iteration: 2760; Percent complete: 69.0%; Average loss: 4.4190\n","Iteration: 2761; Percent complete: 69.0%; Average loss: 3.9863\n","Iteration: 2762; Percent complete: 69.0%; Average loss: 4.3368\n","Iteration: 2763; Percent complete: 69.1%; Average loss: 4.3516\n","Iteration: 2764; Percent complete: 69.1%; Average loss: 4.1939\n","Iteration: 2765; Percent complete: 69.1%; Average loss: 4.1600\n","Iteration: 2766; Percent complete: 69.2%; Average loss: 4.0549\n","Iteration: 2767; Percent complete: 69.2%; Average loss: 4.1641\n","Iteration: 2768; Percent complete: 69.2%; Average loss: 4.2451\n","Iteration: 2769; Percent complete: 69.2%; Average loss: 3.9951\n","Iteration: 2770; Percent complete: 69.2%; Average loss: 4.0811\n","Iteration: 2771; Percent complete: 69.3%; Average loss: 4.2270\n","Iteration: 2772; Percent complete: 69.3%; Average loss: 4.1529\n","Iteration: 2773; Percent complete: 69.3%; Average loss: 4.1618\n","Iteration: 2774; Percent complete: 69.3%; Average loss: 4.2003\n","Iteration: 2775; Percent complete: 69.4%; Average loss: 4.2337\n","Iteration: 2776; Percent complete: 69.4%; Average loss: 4.3635\n","Iteration: 2777; Percent complete: 69.4%; Average loss: 4.2435\n","Iteration: 2778; Percent complete: 69.5%; Average loss: 4.2956\n","Iteration: 2779; Percent complete: 69.5%; Average loss: 4.0728\n","Iteration: 2780; Percent complete: 69.5%; Average loss: 4.2571\n","Iteration: 2781; Percent complete: 69.5%; Average loss: 4.0744\n","Iteration: 2782; Percent complete: 69.5%; Average loss: 4.1811\n","Iteration: 2783; Percent complete: 69.6%; Average loss: 3.9529\n","Iteration: 2784; Percent complete: 69.6%; Average loss: 4.0053\n","Iteration: 2785; Percent complete: 69.6%; Average loss: 4.0102\n","Iteration: 2786; Percent complete: 69.7%; Average loss: 4.1119\n","Iteration: 2787; Percent complete: 69.7%; Average loss: 4.1581\n","Iteration: 2788; Percent complete: 69.7%; Average loss: 4.0602\n","Iteration: 2789; Percent complete: 69.7%; Average loss: 4.1040\n","Iteration: 2790; Percent complete: 69.8%; Average loss: 4.2659\n","Iteration: 2791; Percent complete: 69.8%; Average loss: 3.8670\n","Iteration: 2792; Percent complete: 69.8%; Average loss: 4.2239\n","Iteration: 2793; Percent complete: 69.8%; Average loss: 4.3738\n","Iteration: 2794; Percent complete: 69.8%; Average loss: 4.0048\n","Iteration: 2795; Percent complete: 69.9%; Average loss: 4.4009\n","Iteration: 2796; Percent complete: 69.9%; Average loss: 4.1982\n","Iteration: 2797; Percent complete: 69.9%; Average loss: 4.0420\n","Iteration: 2798; Percent complete: 70.0%; Average loss: 4.1079\n","Iteration: 2799; Percent complete: 70.0%; Average loss: 4.2286\n","Iteration: 2800; Percent complete: 70.0%; Average loss: 4.2116\n","Iteration: 2801; Percent complete: 70.0%; Average loss: 4.3868\n","Iteration: 2802; Percent complete: 70.0%; Average loss: 4.0770\n","Iteration: 2803; Percent complete: 70.1%; Average loss: 4.4122\n","Iteration: 2804; Percent complete: 70.1%; Average loss: 4.1560\n","Iteration: 2805; Percent complete: 70.1%; Average loss: 4.2140\n","Iteration: 2806; Percent complete: 70.2%; Average loss: 4.1084\n","Iteration: 2807; Percent complete: 70.2%; Average loss: 4.1995\n","Iteration: 2808; Percent complete: 70.2%; Average loss: 4.2012\n","Iteration: 2809; Percent complete: 70.2%; Average loss: 4.2586\n","Iteration: 2810; Percent complete: 70.2%; Average loss: 3.9782\n","Iteration: 2811; Percent complete: 70.3%; Average loss: 4.2085\n","Iteration: 2812; Percent complete: 70.3%; Average loss: 4.4596\n","Iteration: 2813; Percent complete: 70.3%; Average loss: 4.1207\n","Iteration: 2814; Percent complete: 70.3%; Average loss: 3.8822\n","Iteration: 2815; Percent complete: 70.4%; Average loss: 4.2748\n","Iteration: 2816; Percent complete: 70.4%; Average loss: 3.9618\n","Iteration: 2817; Percent complete: 70.4%; Average loss: 4.0377\n","Iteration: 2818; Percent complete: 70.5%; Average loss: 4.1436\n","Iteration: 2819; Percent complete: 70.5%; Average loss: 4.1387\n","Iteration: 2820; Percent complete: 70.5%; Average loss: 4.6022\n","Iteration: 2821; Percent complete: 70.5%; Average loss: 4.2267\n","Iteration: 2822; Percent complete: 70.5%; Average loss: 4.1564\n","Iteration: 2823; Percent complete: 70.6%; Average loss: 4.1321\n","Iteration: 2824; Percent complete: 70.6%; Average loss: 3.9083\n","Iteration: 2825; Percent complete: 70.6%; Average loss: 4.1284\n","Iteration: 2826; Percent complete: 70.7%; Average loss: 4.0856\n","Iteration: 2827; Percent complete: 70.7%; Average loss: 4.2252\n","Iteration: 2828; Percent complete: 70.7%; Average loss: 4.0962\n","Iteration: 2829; Percent complete: 70.7%; Average loss: 4.1054\n","Iteration: 2830; Percent complete: 70.8%; Average loss: 4.2945\n","Iteration: 2831; Percent complete: 70.8%; Average loss: 4.3139\n","Iteration: 2832; Percent complete: 70.8%; Average loss: 4.0252\n","Iteration: 2833; Percent complete: 70.8%; Average loss: 4.1832\n","Iteration: 2834; Percent complete: 70.9%; Average loss: 4.0327\n","Iteration: 2835; Percent complete: 70.9%; Average loss: 4.1036\n","Iteration: 2836; Percent complete: 70.9%; Average loss: 4.2307\n","Iteration: 2837; Percent complete: 70.9%; Average loss: 4.1524\n","Iteration: 2838; Percent complete: 71.0%; Average loss: 4.1741\n","Iteration: 2839; Percent complete: 71.0%; Average loss: 4.4143\n","Iteration: 2840; Percent complete: 71.0%; Average loss: 4.2732\n","Iteration: 2841; Percent complete: 71.0%; Average loss: 4.1097\n","Iteration: 2842; Percent complete: 71.0%; Average loss: 3.9901\n","Iteration: 2843; Percent complete: 71.1%; Average loss: 3.9804\n","Iteration: 2844; Percent complete: 71.1%; Average loss: 4.1271\n","Iteration: 2845; Percent complete: 71.1%; Average loss: 4.0972\n","Iteration: 2846; Percent complete: 71.2%; Average loss: 4.2827\n","Iteration: 2847; Percent complete: 71.2%; Average loss: 4.3899\n","Iteration: 2848; Percent complete: 71.2%; Average loss: 4.3269\n","Iteration: 2849; Percent complete: 71.2%; Average loss: 4.0937\n","Iteration: 2850; Percent complete: 71.2%; Average loss: 4.0449\n","Iteration: 2851; Percent complete: 71.3%; Average loss: 4.0804\n","Iteration: 2852; Percent complete: 71.3%; Average loss: 4.3366\n","Iteration: 2853; Percent complete: 71.3%; Average loss: 4.0382\n","Iteration: 2854; Percent complete: 71.4%; Average loss: 4.1298\n","Iteration: 2855; Percent complete: 71.4%; Average loss: 4.3118\n","Iteration: 2856; Percent complete: 71.4%; Average loss: 4.0808\n","Iteration: 2857; Percent complete: 71.4%; Average loss: 4.0749\n","Iteration: 2858; Percent complete: 71.5%; Average loss: 4.1924\n","Iteration: 2859; Percent complete: 71.5%; Average loss: 3.9884\n","Iteration: 2860; Percent complete: 71.5%; Average loss: 4.2316\n","Iteration: 2861; Percent complete: 71.5%; Average loss: 4.2826\n","Iteration: 2862; Percent complete: 71.5%; Average loss: 4.2988\n","Iteration: 2863; Percent complete: 71.6%; Average loss: 4.2766\n","Iteration: 2864; Percent complete: 71.6%; Average loss: 4.2171\n","Iteration: 2865; Percent complete: 71.6%; Average loss: 4.0842\n","Iteration: 2866; Percent complete: 71.7%; Average loss: 3.8898\n","Iteration: 2867; Percent complete: 71.7%; Average loss: 4.0876\n","Iteration: 2868; Percent complete: 71.7%; Average loss: 4.2397\n","Iteration: 2869; Percent complete: 71.7%; Average loss: 3.9331\n","Iteration: 2870; Percent complete: 71.8%; Average loss: 4.1931\n","Iteration: 2871; Percent complete: 71.8%; Average loss: 4.1838\n","Iteration: 2872; Percent complete: 71.8%; Average loss: 4.1384\n","Iteration: 2873; Percent complete: 71.8%; Average loss: 3.8493\n","Iteration: 2874; Percent complete: 71.9%; Average loss: 4.0809\n","Iteration: 2875; Percent complete: 71.9%; Average loss: 4.2929\n","Iteration: 2876; Percent complete: 71.9%; Average loss: 4.2318\n","Iteration: 2877; Percent complete: 71.9%; Average loss: 4.2337\n","Iteration: 2878; Percent complete: 72.0%; Average loss: 3.9748\n","Iteration: 2879; Percent complete: 72.0%; Average loss: 4.2331\n","Iteration: 2880; Percent complete: 72.0%; Average loss: 4.3801\n","Iteration: 2881; Percent complete: 72.0%; Average loss: 4.3051\n","Iteration: 2882; Percent complete: 72.0%; Average loss: 4.4040\n","Iteration: 2883; Percent complete: 72.1%; Average loss: 4.0222\n","Iteration: 2884; Percent complete: 72.1%; Average loss: 3.9999\n","Iteration: 2885; Percent complete: 72.1%; Average loss: 4.2642\n","Iteration: 2886; Percent complete: 72.2%; Average loss: 4.1116\n","Iteration: 2887; Percent complete: 72.2%; Average loss: 4.0100\n","Iteration: 2888; Percent complete: 72.2%; Average loss: 4.2150\n","Iteration: 2889; Percent complete: 72.2%; Average loss: 4.3113\n","Iteration: 2890; Percent complete: 72.2%; Average loss: 4.2718\n","Iteration: 2891; Percent complete: 72.3%; Average loss: 4.2075\n","Iteration: 2892; Percent complete: 72.3%; Average loss: 4.1587\n","Iteration: 2893; Percent complete: 72.3%; Average loss: 4.1141\n","Iteration: 2894; Percent complete: 72.4%; Average loss: 4.2870\n","Iteration: 2895; Percent complete: 72.4%; Average loss: 4.2090\n","Iteration: 2896; Percent complete: 72.4%; Average loss: 4.2202\n","Iteration: 2897; Percent complete: 72.4%; Average loss: 4.2349\n","Iteration: 2898; Percent complete: 72.5%; Average loss: 4.3435\n","Iteration: 2899; Percent complete: 72.5%; Average loss: 4.3092\n","Iteration: 2900; Percent complete: 72.5%; Average loss: 4.1522\n","Iteration: 2901; Percent complete: 72.5%; Average loss: 4.0900\n","Iteration: 2902; Percent complete: 72.5%; Average loss: 4.1268\n","Iteration: 2903; Percent complete: 72.6%; Average loss: 4.0340\n","Iteration: 2904; Percent complete: 72.6%; Average loss: 4.3093\n","Iteration: 2905; Percent complete: 72.6%; Average loss: 4.0377\n","Iteration: 2906; Percent complete: 72.7%; Average loss: 4.1476\n","Iteration: 2907; Percent complete: 72.7%; Average loss: 4.1163\n","Iteration: 2908; Percent complete: 72.7%; Average loss: 3.9970\n","Iteration: 2909; Percent complete: 72.7%; Average loss: 4.1204\n","Iteration: 2910; Percent complete: 72.8%; Average loss: 3.8891\n","Iteration: 2911; Percent complete: 72.8%; Average loss: 4.2775\n","Iteration: 2912; Percent complete: 72.8%; Average loss: 4.2439\n","Iteration: 2913; Percent complete: 72.8%; Average loss: 4.2582\n","Iteration: 2914; Percent complete: 72.9%; Average loss: 4.0315\n","Iteration: 2915; Percent complete: 72.9%; Average loss: 4.0795\n","Iteration: 2916; Percent complete: 72.9%; Average loss: 4.2584\n","Iteration: 2917; Percent complete: 72.9%; Average loss: 4.1151\n","Iteration: 2918; Percent complete: 73.0%; Average loss: 4.4583\n","Iteration: 2919; Percent complete: 73.0%; Average loss: 4.1437\n","Iteration: 2920; Percent complete: 73.0%; Average loss: 3.8924\n","Iteration: 2921; Percent complete: 73.0%; Average loss: 4.0007\n","Iteration: 2922; Percent complete: 73.0%; Average loss: 3.9697\n","Iteration: 2923; Percent complete: 73.1%; Average loss: 4.1300\n","Iteration: 2924; Percent complete: 73.1%; Average loss: 4.0315\n","Iteration: 2925; Percent complete: 73.1%; Average loss: 4.2659\n","Iteration: 2926; Percent complete: 73.2%; Average loss: 3.9801\n","Iteration: 2927; Percent complete: 73.2%; Average loss: 4.1057\n","Iteration: 2928; Percent complete: 73.2%; Average loss: 4.2829\n","Iteration: 2929; Percent complete: 73.2%; Average loss: 4.2413\n","Iteration: 2930; Percent complete: 73.2%; Average loss: 4.0308\n","Iteration: 2931; Percent complete: 73.3%; Average loss: 4.1756\n","Iteration: 2932; Percent complete: 73.3%; Average loss: 3.8636\n","Iteration: 2933; Percent complete: 73.3%; Average loss: 4.3662\n","Iteration: 2934; Percent complete: 73.4%; Average loss: 4.3119\n","Iteration: 2935; Percent complete: 73.4%; Average loss: 3.9065\n","Iteration: 2936; Percent complete: 73.4%; Average loss: 4.3309\n","Iteration: 2937; Percent complete: 73.4%; Average loss: 4.0486\n","Iteration: 2938; Percent complete: 73.5%; Average loss: 4.2390\n","Iteration: 2939; Percent complete: 73.5%; Average loss: 4.2949\n","Iteration: 2940; Percent complete: 73.5%; Average loss: 4.3677\n","Iteration: 2941; Percent complete: 73.5%; Average loss: 4.1517\n","Iteration: 2942; Percent complete: 73.6%; Average loss: 4.1947\n","Iteration: 2943; Percent complete: 73.6%; Average loss: 4.1302\n","Iteration: 2944; Percent complete: 73.6%; Average loss: 3.9765\n","Iteration: 2945; Percent complete: 73.6%; Average loss: 3.9670\n","Iteration: 2946; Percent complete: 73.7%; Average loss: 4.2940\n","Iteration: 2947; Percent complete: 73.7%; Average loss: 4.1060\n","Iteration: 2948; Percent complete: 73.7%; Average loss: 4.0803\n","Iteration: 2949; Percent complete: 73.7%; Average loss: 3.9783\n","Iteration: 2950; Percent complete: 73.8%; Average loss: 4.3284\n","Iteration: 2951; Percent complete: 73.8%; Average loss: 4.0868\n","Iteration: 2952; Percent complete: 73.8%; Average loss: 4.0433\n","Iteration: 2953; Percent complete: 73.8%; Average loss: 4.1918\n","Iteration: 2954; Percent complete: 73.9%; Average loss: 4.2843\n","Iteration: 2955; Percent complete: 73.9%; Average loss: 4.2441\n","Iteration: 2956; Percent complete: 73.9%; Average loss: 4.1307\n","Iteration: 2957; Percent complete: 73.9%; Average loss: 3.9778\n","Iteration: 2958; Percent complete: 74.0%; Average loss: 4.0975\n","Iteration: 2959; Percent complete: 74.0%; Average loss: 4.1640\n","Iteration: 2960; Percent complete: 74.0%; Average loss: 4.1139\n","Iteration: 2961; Percent complete: 74.0%; Average loss: 4.2602\n","Iteration: 2962; Percent complete: 74.1%; Average loss: 4.1719\n","Iteration: 2963; Percent complete: 74.1%; Average loss: 3.9360\n","Iteration: 2964; Percent complete: 74.1%; Average loss: 4.3321\n","Iteration: 2965; Percent complete: 74.1%; Average loss: 4.1482\n","Iteration: 2966; Percent complete: 74.2%; Average loss: 4.1271\n","Iteration: 2967; Percent complete: 74.2%; Average loss: 4.2551\n","Iteration: 2968; Percent complete: 74.2%; Average loss: 4.2620\n","Iteration: 2969; Percent complete: 74.2%; Average loss: 3.9733\n","Iteration: 2970; Percent complete: 74.2%; Average loss: 4.1660\n","Iteration: 2971; Percent complete: 74.3%; Average loss: 4.1487\n","Iteration: 2972; Percent complete: 74.3%; Average loss: 4.2301\n","Iteration: 2973; Percent complete: 74.3%; Average loss: 4.4084\n","Iteration: 2974; Percent complete: 74.4%; Average loss: 4.2439\n","Iteration: 2975; Percent complete: 74.4%; Average loss: 4.1520\n","Iteration: 2976; Percent complete: 74.4%; Average loss: 4.0943\n","Iteration: 2977; Percent complete: 74.4%; Average loss: 3.9558\n","Iteration: 2978; Percent complete: 74.5%; Average loss: 4.1968\n","Iteration: 2979; Percent complete: 74.5%; Average loss: 4.2680\n","Iteration: 2980; Percent complete: 74.5%; Average loss: 4.3428\n","Iteration: 2981; Percent complete: 74.5%; Average loss: 4.3978\n","Iteration: 2982; Percent complete: 74.6%; Average loss: 4.1205\n","Iteration: 2983; Percent complete: 74.6%; Average loss: 4.3414\n","Iteration: 2984; Percent complete: 74.6%; Average loss: 4.1093\n","Iteration: 2985; Percent complete: 74.6%; Average loss: 4.2474\n","Iteration: 2986; Percent complete: 74.7%; Average loss: 3.9866\n","Iteration: 2987; Percent complete: 74.7%; Average loss: 4.1426\n","Iteration: 2988; Percent complete: 74.7%; Average loss: 4.1542\n","Iteration: 2989; Percent complete: 74.7%; Average loss: 4.2813\n","Iteration: 2990; Percent complete: 74.8%; Average loss: 4.1190\n","Iteration: 2991; Percent complete: 74.8%; Average loss: 3.8850\n","Iteration: 2992; Percent complete: 74.8%; Average loss: 4.0600\n","Iteration: 2993; Percent complete: 74.8%; Average loss: 4.0899\n","Iteration: 2994; Percent complete: 74.9%; Average loss: 4.0181\n","Iteration: 2995; Percent complete: 74.9%; Average loss: 4.0648\n","Iteration: 2996; Percent complete: 74.9%; Average loss: 4.0373\n","Iteration: 2997; Percent complete: 74.9%; Average loss: 4.0272\n","Iteration: 2998; Percent complete: 75.0%; Average loss: 4.2487\n","Iteration: 2999; Percent complete: 75.0%; Average loss: 4.0014\n","Iteration: 3000; Percent complete: 75.0%; Average loss: 4.1659\n","Iteration: 3001; Percent complete: 75.0%; Average loss: 4.0331\n","Iteration: 3002; Percent complete: 75.0%; Average loss: 4.3082\n","Iteration: 3003; Percent complete: 75.1%; Average loss: 4.2044\n","Iteration: 3004; Percent complete: 75.1%; Average loss: 4.0290\n","Iteration: 3005; Percent complete: 75.1%; Average loss: 4.0331\n","Iteration: 3006; Percent complete: 75.1%; Average loss: 4.4186\n","Iteration: 3007; Percent complete: 75.2%; Average loss: 4.1160\n","Iteration: 3008; Percent complete: 75.2%; Average loss: 3.8620\n","Iteration: 3009; Percent complete: 75.2%; Average loss: 3.9952\n","Iteration: 3010; Percent complete: 75.2%; Average loss: 4.2417\n","Iteration: 3011; Percent complete: 75.3%; Average loss: 4.0063\n","Iteration: 3012; Percent complete: 75.3%; Average loss: 4.3441\n","Iteration: 3013; Percent complete: 75.3%; Average loss: 4.1602\n","Iteration: 3014; Percent complete: 75.3%; Average loss: 4.2464\n","Iteration: 3015; Percent complete: 75.4%; Average loss: 4.0910\n","Iteration: 3016; Percent complete: 75.4%; Average loss: 3.9651\n","Iteration: 3017; Percent complete: 75.4%; Average loss: 4.3113\n","Iteration: 3018; Percent complete: 75.4%; Average loss: 4.2241\n","Iteration: 3019; Percent complete: 75.5%; Average loss: 4.2946\n","Iteration: 3020; Percent complete: 75.5%; Average loss: 4.1715\n","Iteration: 3021; Percent complete: 75.5%; Average loss: 4.4279\n","Iteration: 3022; Percent complete: 75.5%; Average loss: 4.1125\n","Iteration: 3023; Percent complete: 75.6%; Average loss: 4.1161\n","Iteration: 3024; Percent complete: 75.6%; Average loss: 4.1964\n","Iteration: 3025; Percent complete: 75.6%; Average loss: 4.0662\n","Iteration: 3026; Percent complete: 75.6%; Average loss: 4.2319\n","Iteration: 3027; Percent complete: 75.7%; Average loss: 4.2102\n","Iteration: 3028; Percent complete: 75.7%; Average loss: 4.2371\n","Iteration: 3029; Percent complete: 75.7%; Average loss: 4.1956\n","Iteration: 3030; Percent complete: 75.8%; Average loss: 4.2309\n","Iteration: 3031; Percent complete: 75.8%; Average loss: 4.0597\n","Iteration: 3032; Percent complete: 75.8%; Average loss: 3.9494\n","Iteration: 3033; Percent complete: 75.8%; Average loss: 4.1553\n","Iteration: 3034; Percent complete: 75.8%; Average loss: 3.9458\n","Iteration: 3035; Percent complete: 75.9%; Average loss: 4.1898\n","Iteration: 3036; Percent complete: 75.9%; Average loss: 4.2132\n","Iteration: 3037; Percent complete: 75.9%; Average loss: 4.0053\n","Iteration: 3038; Percent complete: 75.9%; Average loss: 4.0785\n","Iteration: 3039; Percent complete: 76.0%; Average loss: 4.1555\n","Iteration: 3040; Percent complete: 76.0%; Average loss: 4.2444\n","Iteration: 3041; Percent complete: 76.0%; Average loss: 4.0425\n","Iteration: 3042; Percent complete: 76.0%; Average loss: 4.2830\n","Iteration: 3043; Percent complete: 76.1%; Average loss: 3.9578\n","Iteration: 3044; Percent complete: 76.1%; Average loss: 4.2104\n","Iteration: 3045; Percent complete: 76.1%; Average loss: 4.2205\n","Iteration: 3046; Percent complete: 76.1%; Average loss: 4.2263\n","Iteration: 3047; Percent complete: 76.2%; Average loss: 4.0970\n","Iteration: 3048; Percent complete: 76.2%; Average loss: 4.0962\n","Iteration: 3049; Percent complete: 76.2%; Average loss: 4.0468\n","Iteration: 3050; Percent complete: 76.2%; Average loss: 4.1051\n","Iteration: 3051; Percent complete: 76.3%; Average loss: 4.0020\n","Iteration: 3052; Percent complete: 76.3%; Average loss: 4.0203\n","Iteration: 3053; Percent complete: 76.3%; Average loss: 3.8378\n","Iteration: 3054; Percent complete: 76.3%; Average loss: 3.9908\n","Iteration: 3055; Percent complete: 76.4%; Average loss: 3.7243\n","Iteration: 3056; Percent complete: 76.4%; Average loss: 4.5167\n","Iteration: 3057; Percent complete: 76.4%; Average loss: 3.9225\n","Iteration: 3058; Percent complete: 76.4%; Average loss: 3.8823\n","Iteration: 3059; Percent complete: 76.5%; Average loss: 4.0727\n","Iteration: 3060; Percent complete: 76.5%; Average loss: 4.2081\n","Iteration: 3061; Percent complete: 76.5%; Average loss: 3.8603\n","Iteration: 3062; Percent complete: 76.5%; Average loss: 4.0372\n","Iteration: 3063; Percent complete: 76.6%; Average loss: 4.1172\n","Iteration: 3064; Percent complete: 76.6%; Average loss: 3.9355\n","Iteration: 3065; Percent complete: 76.6%; Average loss: 4.0037\n","Iteration: 3066; Percent complete: 76.6%; Average loss: 4.3002\n","Iteration: 3067; Percent complete: 76.7%; Average loss: 4.2223\n","Iteration: 3068; Percent complete: 76.7%; Average loss: 4.1988\n","Iteration: 3069; Percent complete: 76.7%; Average loss: 4.0248\n","Iteration: 3070; Percent complete: 76.8%; Average loss: 4.2229\n","Iteration: 3071; Percent complete: 76.8%; Average loss: 4.2653\n","Iteration: 3072; Percent complete: 76.8%; Average loss: 4.1507\n","Iteration: 3073; Percent complete: 76.8%; Average loss: 4.1810\n","Iteration: 3074; Percent complete: 76.8%; Average loss: 4.1188\n","Iteration: 3075; Percent complete: 76.9%; Average loss: 4.1885\n","Iteration: 3076; Percent complete: 76.9%; Average loss: 4.0715\n","Iteration: 3077; Percent complete: 76.9%; Average loss: 4.1058\n","Iteration: 3078; Percent complete: 77.0%; Average loss: 3.7917\n","Iteration: 3079; Percent complete: 77.0%; Average loss: 4.1447\n","Iteration: 3080; Percent complete: 77.0%; Average loss: 4.1078\n","Iteration: 3081; Percent complete: 77.0%; Average loss: 3.9772\n","Iteration: 3082; Percent complete: 77.0%; Average loss: 4.3370\n","Iteration: 3083; Percent complete: 77.1%; Average loss: 4.0381\n","Iteration: 3084; Percent complete: 77.1%; Average loss: 4.1241\n","Iteration: 3085; Percent complete: 77.1%; Average loss: 4.1364\n","Iteration: 3086; Percent complete: 77.1%; Average loss: 4.1283\n","Iteration: 3087; Percent complete: 77.2%; Average loss: 4.3112\n","Iteration: 3088; Percent complete: 77.2%; Average loss: 4.1782\n","Iteration: 3089; Percent complete: 77.2%; Average loss: 4.0335\n","Iteration: 3090; Percent complete: 77.2%; Average loss: 3.9409\n","Iteration: 3091; Percent complete: 77.3%; Average loss: 4.1326\n","Iteration: 3092; Percent complete: 77.3%; Average loss: 4.0246\n","Iteration: 3093; Percent complete: 77.3%; Average loss: 4.3082\n","Iteration: 3094; Percent complete: 77.3%; Average loss: 3.8985\n","Iteration: 3095; Percent complete: 77.4%; Average loss: 4.2789\n","Iteration: 3096; Percent complete: 77.4%; Average loss: 4.0459\n","Iteration: 3097; Percent complete: 77.4%; Average loss: 4.0107\n","Iteration: 3098; Percent complete: 77.5%; Average loss: 4.0325\n","Iteration: 3099; Percent complete: 77.5%; Average loss: 4.1266\n","Iteration: 3100; Percent complete: 77.5%; Average loss: 4.0677\n","Iteration: 3101; Percent complete: 77.5%; Average loss: 4.1762\n","Iteration: 3102; Percent complete: 77.5%; Average loss: 4.1663\n","Iteration: 3103; Percent complete: 77.6%; Average loss: 4.0359\n","Iteration: 3104; Percent complete: 77.6%; Average loss: 4.1780\n","Iteration: 3105; Percent complete: 77.6%; Average loss: 4.3504\n","Iteration: 3106; Percent complete: 77.6%; Average loss: 4.0813\n","Iteration: 3107; Percent complete: 77.7%; Average loss: 4.1976\n","Iteration: 3108; Percent complete: 77.7%; Average loss: 4.2014\n","Iteration: 3109; Percent complete: 77.7%; Average loss: 3.9861\n","Iteration: 3110; Percent complete: 77.8%; Average loss: 4.1629\n","Iteration: 3111; Percent complete: 77.8%; Average loss: 4.2665\n","Iteration: 3112; Percent complete: 77.8%; Average loss: 4.2817\n","Iteration: 3113; Percent complete: 77.8%; Average loss: 4.0153\n","Iteration: 3114; Percent complete: 77.8%; Average loss: 3.9592\n","Iteration: 3115; Percent complete: 77.9%; Average loss: 4.0611\n","Iteration: 3116; Percent complete: 77.9%; Average loss: 4.2115\n","Iteration: 3117; Percent complete: 77.9%; Average loss: 4.3023\n","Iteration: 3118; Percent complete: 78.0%; Average loss: 4.0208\n","Iteration: 3119; Percent complete: 78.0%; Average loss: 3.9047\n","Iteration: 3120; Percent complete: 78.0%; Average loss: 3.9913\n","Iteration: 3121; Percent complete: 78.0%; Average loss: 4.1427\n","Iteration: 3122; Percent complete: 78.0%; Average loss: 4.2983\n","Iteration: 3123; Percent complete: 78.1%; Average loss: 4.1839\n","Iteration: 3124; Percent complete: 78.1%; Average loss: 4.1320\n","Iteration: 3125; Percent complete: 78.1%; Average loss: 4.2597\n","Iteration: 3126; Percent complete: 78.1%; Average loss: 4.2009\n","Iteration: 3127; Percent complete: 78.2%; Average loss: 4.0052\n","Iteration: 3128; Percent complete: 78.2%; Average loss: 4.0846\n","Iteration: 3129; Percent complete: 78.2%; Average loss: 4.3289\n","Iteration: 3130; Percent complete: 78.2%; Average loss: 4.1574\n","Iteration: 3131; Percent complete: 78.3%; Average loss: 4.2187\n","Iteration: 3132; Percent complete: 78.3%; Average loss: 3.9898\n","Iteration: 3133; Percent complete: 78.3%; Average loss: 3.8704\n","Iteration: 3134; Percent complete: 78.3%; Average loss: 3.9119\n","Iteration: 3135; Percent complete: 78.4%; Average loss: 4.0412\n","Iteration: 3136; Percent complete: 78.4%; Average loss: 3.7884\n","Iteration: 3137; Percent complete: 78.4%; Average loss: 4.1909\n","Iteration: 3138; Percent complete: 78.5%; Average loss: 3.9836\n","Iteration: 3139; Percent complete: 78.5%; Average loss: 4.1461\n","Iteration: 3140; Percent complete: 78.5%; Average loss: 3.8939\n","Iteration: 3141; Percent complete: 78.5%; Average loss: 4.2130\n","Iteration: 3142; Percent complete: 78.5%; Average loss: 4.0939\n","Iteration: 3143; Percent complete: 78.6%; Average loss: 4.0875\n","Iteration: 3144; Percent complete: 78.6%; Average loss: 4.2745\n","Iteration: 3145; Percent complete: 78.6%; Average loss: 4.0680\n","Iteration: 3146; Percent complete: 78.6%; Average loss: 4.2426\n","Iteration: 3147; Percent complete: 78.7%; Average loss: 4.1149\n","Iteration: 3148; Percent complete: 78.7%; Average loss: 3.8255\n","Iteration: 3149; Percent complete: 78.7%; Average loss: 4.1066\n","Iteration: 3150; Percent complete: 78.8%; Average loss: 3.9875\n","Iteration: 3151; Percent complete: 78.8%; Average loss: 4.0111\n","Iteration: 3152; Percent complete: 78.8%; Average loss: 3.9830\n","Iteration: 3153; Percent complete: 78.8%; Average loss: 4.0852\n","Iteration: 3154; Percent complete: 78.8%; Average loss: 3.9288\n","Iteration: 3155; Percent complete: 78.9%; Average loss: 3.9316\n","Iteration: 3156; Percent complete: 78.9%; Average loss: 4.1480\n","Iteration: 3157; Percent complete: 78.9%; Average loss: 3.9246\n","Iteration: 3158; Percent complete: 79.0%; Average loss: 4.1267\n","Iteration: 3159; Percent complete: 79.0%; Average loss: 4.0140\n","Iteration: 3160; Percent complete: 79.0%; Average loss: 4.1158\n","Iteration: 3161; Percent complete: 79.0%; Average loss: 3.9899\n","Iteration: 3162; Percent complete: 79.0%; Average loss: 4.2705\n","Iteration: 3163; Percent complete: 79.1%; Average loss: 4.1721\n","Iteration: 3164; Percent complete: 79.1%; Average loss: 4.1294\n","Iteration: 3165; Percent complete: 79.1%; Average loss: 4.1867\n","Iteration: 3166; Percent complete: 79.1%; Average loss: 4.0897\n","Iteration: 3167; Percent complete: 79.2%; Average loss: 4.1587\n","Iteration: 3168; Percent complete: 79.2%; Average loss: 3.8769\n","Iteration: 3169; Percent complete: 79.2%; Average loss: 4.2748\n","Iteration: 3170; Percent complete: 79.2%; Average loss: 4.2368\n","Iteration: 3171; Percent complete: 79.3%; Average loss: 4.1317\n","Iteration: 3172; Percent complete: 79.3%; Average loss: 3.9120\n","Iteration: 3173; Percent complete: 79.3%; Average loss: 4.2688\n","Iteration: 3174; Percent complete: 79.3%; Average loss: 4.1584\n","Iteration: 3175; Percent complete: 79.4%; Average loss: 3.9942\n","Iteration: 3176; Percent complete: 79.4%; Average loss: 4.2025\n","Iteration: 3177; Percent complete: 79.4%; Average loss: 4.0111\n","Iteration: 3178; Percent complete: 79.5%; Average loss: 4.1087\n","Iteration: 3179; Percent complete: 79.5%; Average loss: 3.9844\n","Iteration: 3180; Percent complete: 79.5%; Average loss: 3.9528\n","Iteration: 3181; Percent complete: 79.5%; Average loss: 4.2831\n","Iteration: 3182; Percent complete: 79.5%; Average loss: 4.0067\n","Iteration: 3183; Percent complete: 79.6%; Average loss: 4.1564\n","Iteration: 3184; Percent complete: 79.6%; Average loss: 4.1277\n","Iteration: 3185; Percent complete: 79.6%; Average loss: 4.2754\n","Iteration: 3186; Percent complete: 79.7%; Average loss: 4.1731\n","Iteration: 3187; Percent complete: 79.7%; Average loss: 3.8554\n","Iteration: 3188; Percent complete: 79.7%; Average loss: 4.3336\n","Iteration: 3189; Percent complete: 79.7%; Average loss: 4.1067\n","Iteration: 3190; Percent complete: 79.8%; Average loss: 4.2927\n","Iteration: 3191; Percent complete: 79.8%; Average loss: 4.1866\n","Iteration: 3192; Percent complete: 79.8%; Average loss: 4.1534\n","Iteration: 3193; Percent complete: 79.8%; Average loss: 4.2046\n","Iteration: 3194; Percent complete: 79.8%; Average loss: 4.3173\n","Iteration: 3195; Percent complete: 79.9%; Average loss: 4.2597\n","Iteration: 3196; Percent complete: 79.9%; Average loss: 4.1874\n","Iteration: 3197; Percent complete: 79.9%; Average loss: 4.1163\n","Iteration: 3198; Percent complete: 80.0%; Average loss: 4.0546\n","Iteration: 3199; Percent complete: 80.0%; Average loss: 3.8222\n","Iteration: 3200; Percent complete: 80.0%; Average loss: 4.1231\n","Iteration: 3201; Percent complete: 80.0%; Average loss: 4.2696\n","Iteration: 3202; Percent complete: 80.0%; Average loss: 3.8478\n","Iteration: 3203; Percent complete: 80.1%; Average loss: 4.2058\n","Iteration: 3204; Percent complete: 80.1%; Average loss: 4.2456\n","Iteration: 3205; Percent complete: 80.1%; Average loss: 4.1612\n","Iteration: 3206; Percent complete: 80.2%; Average loss: 3.9934\n","Iteration: 3207; Percent complete: 80.2%; Average loss: 4.3132\n","Iteration: 3208; Percent complete: 80.2%; Average loss: 4.0339\n","Iteration: 3209; Percent complete: 80.2%; Average loss: 4.0584\n","Iteration: 3210; Percent complete: 80.2%; Average loss: 3.8395\n","Iteration: 3211; Percent complete: 80.3%; Average loss: 4.0089\n","Iteration: 3212; Percent complete: 80.3%; Average loss: 4.1714\n","Iteration: 3213; Percent complete: 80.3%; Average loss: 4.1158\n","Iteration: 3214; Percent complete: 80.3%; Average loss: 4.3272\n","Iteration: 3215; Percent complete: 80.4%; Average loss: 3.9420\n","Iteration: 3216; Percent complete: 80.4%; Average loss: 4.0523\n","Iteration: 3217; Percent complete: 80.4%; Average loss: 3.8494\n","Iteration: 3218; Percent complete: 80.5%; Average loss: 4.3583\n","Iteration: 3219; Percent complete: 80.5%; Average loss: 3.9467\n","Iteration: 3220; Percent complete: 80.5%; Average loss: 4.1784\n","Iteration: 3221; Percent complete: 80.5%; Average loss: 4.2685\n","Iteration: 3222; Percent complete: 80.5%; Average loss: 3.9357\n","Iteration: 3223; Percent complete: 80.6%; Average loss: 3.8568\n","Iteration: 3224; Percent complete: 80.6%; Average loss: 3.8986\n","Iteration: 3225; Percent complete: 80.6%; Average loss: 4.0876\n","Iteration: 3226; Percent complete: 80.7%; Average loss: 4.1254\n","Iteration: 3227; Percent complete: 80.7%; Average loss: 4.1847\n","Iteration: 3228; Percent complete: 80.7%; Average loss: 4.1721\n","Iteration: 3229; Percent complete: 80.7%; Average loss: 4.1952\n","Iteration: 3230; Percent complete: 80.8%; Average loss: 4.0408\n","Iteration: 3231; Percent complete: 80.8%; Average loss: 3.7988\n","Iteration: 3232; Percent complete: 80.8%; Average loss: 3.9488\n","Iteration: 3233; Percent complete: 80.8%; Average loss: 3.8847\n","Iteration: 3234; Percent complete: 80.8%; Average loss: 3.9249\n","Iteration: 3235; Percent complete: 80.9%; Average loss: 3.9560\n","Iteration: 3236; Percent complete: 80.9%; Average loss: 3.9223\n","Iteration: 3237; Percent complete: 80.9%; Average loss: 3.8321\n","Iteration: 3238; Percent complete: 81.0%; Average loss: 3.9158\n","Iteration: 3239; Percent complete: 81.0%; Average loss: 4.1576\n","Iteration: 3240; Percent complete: 81.0%; Average loss: 4.2167\n","Iteration: 3241; Percent complete: 81.0%; Average loss: 4.0731\n","Iteration: 3242; Percent complete: 81.0%; Average loss: 3.9423\n","Iteration: 3243; Percent complete: 81.1%; Average loss: 4.1966\n","Iteration: 3244; Percent complete: 81.1%; Average loss: 3.8637\n","Iteration: 3245; Percent complete: 81.1%; Average loss: 3.9027\n","Iteration: 3246; Percent complete: 81.2%; Average loss: 4.1580\n","Iteration: 3247; Percent complete: 81.2%; Average loss: 4.3618\n","Iteration: 3248; Percent complete: 81.2%; Average loss: 3.8976\n","Iteration: 3249; Percent complete: 81.2%; Average loss: 3.9071\n","Iteration: 3250; Percent complete: 81.2%; Average loss: 3.8381\n","Iteration: 3251; Percent complete: 81.3%; Average loss: 3.9225\n","Iteration: 3252; Percent complete: 81.3%; Average loss: 4.1737\n","Iteration: 3253; Percent complete: 81.3%; Average loss: 4.2691\n","Iteration: 3254; Percent complete: 81.3%; Average loss: 4.2944\n","Iteration: 3255; Percent complete: 81.4%; Average loss: 4.0613\n","Iteration: 3256; Percent complete: 81.4%; Average loss: 4.0418\n","Iteration: 3257; Percent complete: 81.4%; Average loss: 4.0597\n","Iteration: 3258; Percent complete: 81.5%; Average loss: 4.0942\n","Iteration: 3259; Percent complete: 81.5%; Average loss: 4.2296\n","Iteration: 3260; Percent complete: 81.5%; Average loss: 3.9222\n","Iteration: 3261; Percent complete: 81.5%; Average loss: 3.9758\n","Iteration: 3262; Percent complete: 81.5%; Average loss: 4.0005\n","Iteration: 3263; Percent complete: 81.6%; Average loss: 4.1244\n","Iteration: 3264; Percent complete: 81.6%; Average loss: 4.0658\n","Iteration: 3265; Percent complete: 81.6%; Average loss: 3.9937\n","Iteration: 3266; Percent complete: 81.7%; Average loss: 3.8953\n","Iteration: 3267; Percent complete: 81.7%; Average loss: 3.9281\n","Iteration: 3268; Percent complete: 81.7%; Average loss: 4.4370\n","Iteration: 3269; Percent complete: 81.7%; Average loss: 4.2129\n","Iteration: 3270; Percent complete: 81.8%; Average loss: 4.1807\n","Iteration: 3271; Percent complete: 81.8%; Average loss: 4.0405\n","Iteration: 3272; Percent complete: 81.8%; Average loss: 3.8964\n","Iteration: 3273; Percent complete: 81.8%; Average loss: 4.1441\n","Iteration: 3274; Percent complete: 81.8%; Average loss: 4.1269\n","Iteration: 3275; Percent complete: 81.9%; Average loss: 3.9829\n","Iteration: 3276; Percent complete: 81.9%; Average loss: 3.9595\n","Iteration: 3277; Percent complete: 81.9%; Average loss: 4.1282\n","Iteration: 3278; Percent complete: 82.0%; Average loss: 4.0058\n","Iteration: 3279; Percent complete: 82.0%; Average loss: 4.1281\n","Iteration: 3280; Percent complete: 82.0%; Average loss: 4.0942\n","Iteration: 3281; Percent complete: 82.0%; Average loss: 4.2339\n","Iteration: 3282; Percent complete: 82.0%; Average loss: 4.0377\n","Iteration: 3283; Percent complete: 82.1%; Average loss: 4.0537\n","Iteration: 3284; Percent complete: 82.1%; Average loss: 3.9287\n","Iteration: 3285; Percent complete: 82.1%; Average loss: 4.1220\n","Iteration: 3286; Percent complete: 82.2%; Average loss: 4.1424\n","Iteration: 3287; Percent complete: 82.2%; Average loss: 4.1853\n","Iteration: 3288; Percent complete: 82.2%; Average loss: 4.0405\n","Iteration: 3289; Percent complete: 82.2%; Average loss: 4.1608\n","Iteration: 3290; Percent complete: 82.2%; Average loss: 4.0830\n","Iteration: 3291; Percent complete: 82.3%; Average loss: 3.9647\n","Iteration: 3292; Percent complete: 82.3%; Average loss: 4.2843\n","Iteration: 3293; Percent complete: 82.3%; Average loss: 3.9449\n","Iteration: 3294; Percent complete: 82.3%; Average loss: 4.1087\n","Iteration: 3295; Percent complete: 82.4%; Average loss: 3.8847\n","Iteration: 3296; Percent complete: 82.4%; Average loss: 4.3075\n","Iteration: 3297; Percent complete: 82.4%; Average loss: 4.0151\n","Iteration: 3298; Percent complete: 82.5%; Average loss: 4.0200\n","Iteration: 3299; Percent complete: 82.5%; Average loss: 3.9392\n","Iteration: 3300; Percent complete: 82.5%; Average loss: 3.6220\n","Iteration: 3301; Percent complete: 82.5%; Average loss: 4.3042\n","Iteration: 3302; Percent complete: 82.5%; Average loss: 3.9712\n","Iteration: 3303; Percent complete: 82.6%; Average loss: 4.1036\n","Iteration: 3304; Percent complete: 82.6%; Average loss: 4.1014\n","Iteration: 3305; Percent complete: 82.6%; Average loss: 4.2372\n","Iteration: 3306; Percent complete: 82.7%; Average loss: 3.9094\n","Iteration: 3307; Percent complete: 82.7%; Average loss: 3.9741\n","Iteration: 3308; Percent complete: 82.7%; Average loss: 4.3517\n","Iteration: 3309; Percent complete: 82.7%; Average loss: 3.6713\n","Iteration: 3310; Percent complete: 82.8%; Average loss: 4.1570\n","Iteration: 3311; Percent complete: 82.8%; Average loss: 3.9581\n","Iteration: 3312; Percent complete: 82.8%; Average loss: 4.1186\n","Iteration: 3313; Percent complete: 82.8%; Average loss: 4.0785\n","Iteration: 3314; Percent complete: 82.8%; Average loss: 3.9870\n","Iteration: 3315; Percent complete: 82.9%; Average loss: 3.9873\n","Iteration: 3316; Percent complete: 82.9%; Average loss: 4.2440\n","Iteration: 3317; Percent complete: 82.9%; Average loss: 4.0310\n","Iteration: 3318; Percent complete: 83.0%; Average loss: 4.1433\n","Iteration: 3319; Percent complete: 83.0%; Average loss: 4.2482\n","Iteration: 3320; Percent complete: 83.0%; Average loss: 3.9949\n","Iteration: 3321; Percent complete: 83.0%; Average loss: 4.2333\n","Iteration: 3322; Percent complete: 83.0%; Average loss: 3.9056\n","Iteration: 3323; Percent complete: 83.1%; Average loss: 4.1415\n","Iteration: 3324; Percent complete: 83.1%; Average loss: 4.1546\n","Iteration: 3325; Percent complete: 83.1%; Average loss: 3.7701\n","Iteration: 3326; Percent complete: 83.2%; Average loss: 4.1431\n","Iteration: 3327; Percent complete: 83.2%; Average loss: 4.1058\n","Iteration: 3328; Percent complete: 83.2%; Average loss: 4.0274\n","Iteration: 3329; Percent complete: 83.2%; Average loss: 4.0683\n","Iteration: 3330; Percent complete: 83.2%; Average loss: 4.0964\n","Iteration: 3331; Percent complete: 83.3%; Average loss: 4.1278\n","Iteration: 3332; Percent complete: 83.3%; Average loss: 4.1280\n","Iteration: 3333; Percent complete: 83.3%; Average loss: 4.1742\n","Iteration: 3334; Percent complete: 83.4%; Average loss: 4.0960\n","Iteration: 3335; Percent complete: 83.4%; Average loss: 4.1532\n","Iteration: 3336; Percent complete: 83.4%; Average loss: 4.0793\n","Iteration: 3337; Percent complete: 83.4%; Average loss: 4.0246\n","Iteration: 3338; Percent complete: 83.5%; Average loss: 4.0173\n","Iteration: 3339; Percent complete: 83.5%; Average loss: 4.1385\n","Iteration: 3340; Percent complete: 83.5%; Average loss: 4.1176\n","Iteration: 3341; Percent complete: 83.5%; Average loss: 4.1032\n","Iteration: 3342; Percent complete: 83.5%; Average loss: 3.9205\n","Iteration: 3343; Percent complete: 83.6%; Average loss: 4.0865\n","Iteration: 3344; Percent complete: 83.6%; Average loss: 3.8257\n","Iteration: 3345; Percent complete: 83.6%; Average loss: 4.0299\n","Iteration: 3346; Percent complete: 83.7%; Average loss: 4.3964\n","Iteration: 3347; Percent complete: 83.7%; Average loss: 3.9420\n","Iteration: 3348; Percent complete: 83.7%; Average loss: 3.8899\n","Iteration: 3349; Percent complete: 83.7%; Average loss: 4.0944\n","Iteration: 3350; Percent complete: 83.8%; Average loss: 4.2150\n","Iteration: 3351; Percent complete: 83.8%; Average loss: 4.1236\n","Iteration: 3352; Percent complete: 83.8%; Average loss: 4.0802\n","Iteration: 3353; Percent complete: 83.8%; Average loss: 4.2308\n","Iteration: 3354; Percent complete: 83.9%; Average loss: 3.9412\n","Iteration: 3355; Percent complete: 83.9%; Average loss: 3.9546\n","Iteration: 3356; Percent complete: 83.9%; Average loss: 4.1770\n","Iteration: 3357; Percent complete: 83.9%; Average loss: 3.9282\n","Iteration: 3358; Percent complete: 84.0%; Average loss: 4.0871\n","Iteration: 3359; Percent complete: 84.0%; Average loss: 4.0492\n","Iteration: 3360; Percent complete: 84.0%; Average loss: 4.0983\n","Iteration: 3361; Percent complete: 84.0%; Average loss: 4.0504\n","Iteration: 3362; Percent complete: 84.0%; Average loss: 3.8814\n","Iteration: 3363; Percent complete: 84.1%; Average loss: 4.1650\n","Iteration: 3364; Percent complete: 84.1%; Average loss: 3.9528\n","Iteration: 3365; Percent complete: 84.1%; Average loss: 4.0590\n","Iteration: 3366; Percent complete: 84.2%; Average loss: 4.1752\n","Iteration: 3367; Percent complete: 84.2%; Average loss: 4.0432\n","Iteration: 3368; Percent complete: 84.2%; Average loss: 4.2131\n","Iteration: 3369; Percent complete: 84.2%; Average loss: 3.8615\n","Iteration: 3370; Percent complete: 84.2%; Average loss: 3.9979\n","Iteration: 3371; Percent complete: 84.3%; Average loss: 3.9269\n","Iteration: 3372; Percent complete: 84.3%; Average loss: 4.1297\n","Iteration: 3373; Percent complete: 84.3%; Average loss: 3.9761\n","Iteration: 3374; Percent complete: 84.4%; Average loss: 4.1293\n","Iteration: 3375; Percent complete: 84.4%; Average loss: 4.1320\n","Iteration: 3376; Percent complete: 84.4%; Average loss: 4.2046\n","Iteration: 3377; Percent complete: 84.4%; Average loss: 3.8453\n","Iteration: 3378; Percent complete: 84.5%; Average loss: 3.9181\n","Iteration: 3379; Percent complete: 84.5%; Average loss: 4.2967\n","Iteration: 3380; Percent complete: 84.5%; Average loss: 4.2081\n","Iteration: 3381; Percent complete: 84.5%; Average loss: 4.2168\n","Iteration: 3382; Percent complete: 84.5%; Average loss: 4.0115\n","Iteration: 3383; Percent complete: 84.6%; Average loss: 4.1538\n","Iteration: 3384; Percent complete: 84.6%; Average loss: 4.2208\n","Iteration: 3385; Percent complete: 84.6%; Average loss: 4.0402\n","Iteration: 3386; Percent complete: 84.7%; Average loss: 4.1131\n","Iteration: 3387; Percent complete: 84.7%; Average loss: 4.2377\n","Iteration: 3388; Percent complete: 84.7%; Average loss: 3.9325\n","Iteration: 3389; Percent complete: 84.7%; Average loss: 3.9805\n","Iteration: 3390; Percent complete: 84.8%; Average loss: 4.1256\n","Iteration: 3391; Percent complete: 84.8%; Average loss: 4.1163\n","Iteration: 3392; Percent complete: 84.8%; Average loss: 4.2099\n","Iteration: 3393; Percent complete: 84.8%; Average loss: 4.1017\n","Iteration: 3394; Percent complete: 84.9%; Average loss: 4.0738\n","Iteration: 3395; Percent complete: 84.9%; Average loss: 4.0971\n","Iteration: 3396; Percent complete: 84.9%; Average loss: 3.9565\n","Iteration: 3397; Percent complete: 84.9%; Average loss: 3.8205\n","Iteration: 3398; Percent complete: 85.0%; Average loss: 4.0566\n","Iteration: 3399; Percent complete: 85.0%; Average loss: 4.1755\n","Iteration: 3400; Percent complete: 85.0%; Average loss: 3.9801\n","Iteration: 3401; Percent complete: 85.0%; Average loss: 4.0834\n","Iteration: 3402; Percent complete: 85.0%; Average loss: 4.1381\n","Iteration: 3403; Percent complete: 85.1%; Average loss: 4.1944\n","Iteration: 3404; Percent complete: 85.1%; Average loss: 3.8761\n","Iteration: 3405; Percent complete: 85.1%; Average loss: 4.0237\n","Iteration: 3406; Percent complete: 85.2%; Average loss: 4.2089\n","Iteration: 3407; Percent complete: 85.2%; Average loss: 3.9398\n","Iteration: 3408; Percent complete: 85.2%; Average loss: 4.0979\n","Iteration: 3409; Percent complete: 85.2%; Average loss: 3.9030\n","Iteration: 3410; Percent complete: 85.2%; Average loss: 4.1130\n","Iteration: 3411; Percent complete: 85.3%; Average loss: 3.8732\n","Iteration: 3412; Percent complete: 85.3%; Average loss: 4.1762\n","Iteration: 3413; Percent complete: 85.3%; Average loss: 4.3063\n","Iteration: 3414; Percent complete: 85.4%; Average loss: 4.0307\n","Iteration: 3415; Percent complete: 85.4%; Average loss: 3.9922\n","Iteration: 3416; Percent complete: 85.4%; Average loss: 4.1677\n","Iteration: 3417; Percent complete: 85.4%; Average loss: 4.0662\n","Iteration: 3418; Percent complete: 85.5%; Average loss: 3.9845\n","Iteration: 3419; Percent complete: 85.5%; Average loss: 3.9016\n","Iteration: 3420; Percent complete: 85.5%; Average loss: 3.8924\n","Iteration: 3421; Percent complete: 85.5%; Average loss: 4.2062\n","Iteration: 3422; Percent complete: 85.5%; Average loss: 4.1911\n","Iteration: 3423; Percent complete: 85.6%; Average loss: 4.0853\n","Iteration: 3424; Percent complete: 85.6%; Average loss: 4.0106\n","Iteration: 3425; Percent complete: 85.6%; Average loss: 4.0155\n","Iteration: 3426; Percent complete: 85.7%; Average loss: 4.0169\n","Iteration: 3427; Percent complete: 85.7%; Average loss: 4.0696\n","Iteration: 3428; Percent complete: 85.7%; Average loss: 4.0574\n","Iteration: 3429; Percent complete: 85.7%; Average loss: 4.0340\n","Iteration: 3430; Percent complete: 85.8%; Average loss: 4.1386\n","Iteration: 3431; Percent complete: 85.8%; Average loss: 4.0583\n","Iteration: 3432; Percent complete: 85.8%; Average loss: 4.2000\n","Iteration: 3433; Percent complete: 85.8%; Average loss: 3.8946\n","Iteration: 3434; Percent complete: 85.9%; Average loss: 4.1295\n","Iteration: 3435; Percent complete: 85.9%; Average loss: 4.0953\n","Iteration: 3436; Percent complete: 85.9%; Average loss: 3.9235\n","Iteration: 3437; Percent complete: 85.9%; Average loss: 4.1348\n","Iteration: 3438; Percent complete: 86.0%; Average loss: 3.8415\n","Iteration: 3439; Percent complete: 86.0%; Average loss: 4.1014\n","Iteration: 3440; Percent complete: 86.0%; Average loss: 3.8001\n","Iteration: 3441; Percent complete: 86.0%; Average loss: 4.2997\n","Iteration: 3442; Percent complete: 86.1%; Average loss: 3.9018\n","Iteration: 3443; Percent complete: 86.1%; Average loss: 3.9518\n","Iteration: 3444; Percent complete: 86.1%; Average loss: 3.9488\n","Iteration: 3445; Percent complete: 86.1%; Average loss: 4.0046\n","Iteration: 3446; Percent complete: 86.2%; Average loss: 4.0132\n","Iteration: 3447; Percent complete: 86.2%; Average loss: 4.0513\n","Iteration: 3448; Percent complete: 86.2%; Average loss: 4.0806\n","Iteration: 3449; Percent complete: 86.2%; Average loss: 4.1503\n","Iteration: 3450; Percent complete: 86.2%; Average loss: 4.2363\n","Iteration: 3451; Percent complete: 86.3%; Average loss: 3.7754\n","Iteration: 3452; Percent complete: 86.3%; Average loss: 4.2825\n","Iteration: 3453; Percent complete: 86.3%; Average loss: 4.1210\n","Iteration: 3454; Percent complete: 86.4%; Average loss: 4.0570\n","Iteration: 3455; Percent complete: 86.4%; Average loss: 3.7970\n","Iteration: 3456; Percent complete: 86.4%; Average loss: 4.1662\n","Iteration: 3457; Percent complete: 86.4%; Average loss: 3.6932\n","Iteration: 3458; Percent complete: 86.5%; Average loss: 3.9880\n","Iteration: 3459; Percent complete: 86.5%; Average loss: 3.7878\n","Iteration: 3460; Percent complete: 86.5%; Average loss: 3.8906\n","Iteration: 3461; Percent complete: 86.5%; Average loss: 4.0986\n","Iteration: 3462; Percent complete: 86.6%; Average loss: 3.8827\n","Iteration: 3463; Percent complete: 86.6%; Average loss: 4.1374\n","Iteration: 3464; Percent complete: 86.6%; Average loss: 4.2404\n","Iteration: 3465; Percent complete: 86.6%; Average loss: 4.1495\n","Iteration: 3466; Percent complete: 86.7%; Average loss: 4.0604\n","Iteration: 3467; Percent complete: 86.7%; Average loss: 3.9914\n","Iteration: 3468; Percent complete: 86.7%; Average loss: 3.8626\n","Iteration: 3469; Percent complete: 86.7%; Average loss: 3.9569\n","Iteration: 3470; Percent complete: 86.8%; Average loss: 3.7118\n","Iteration: 3471; Percent complete: 86.8%; Average loss: 3.9362\n","Iteration: 3472; Percent complete: 86.8%; Average loss: 4.0611\n","Iteration: 3473; Percent complete: 86.8%; Average loss: 4.0589\n","Iteration: 3474; Percent complete: 86.9%; Average loss: 4.1322\n","Iteration: 3475; Percent complete: 86.9%; Average loss: 4.0195\n","Iteration: 3476; Percent complete: 86.9%; Average loss: 4.1473\n","Iteration: 3477; Percent complete: 86.9%; Average loss: 4.1847\n","Iteration: 3478; Percent complete: 87.0%; Average loss: 4.1753\n","Iteration: 3479; Percent complete: 87.0%; Average loss: 3.9622\n","Iteration: 3480; Percent complete: 87.0%; Average loss: 3.9173\n","Iteration: 3481; Percent complete: 87.0%; Average loss: 3.8884\n","Iteration: 3482; Percent complete: 87.1%; Average loss: 3.9778\n","Iteration: 3483; Percent complete: 87.1%; Average loss: 4.0130\n","Iteration: 3484; Percent complete: 87.1%; Average loss: 4.0826\n","Iteration: 3485; Percent complete: 87.1%; Average loss: 4.2751\n","Iteration: 3486; Percent complete: 87.2%; Average loss: 4.2178\n","Iteration: 3487; Percent complete: 87.2%; Average loss: 3.9536\n","Iteration: 3488; Percent complete: 87.2%; Average loss: 4.1718\n","Iteration: 3489; Percent complete: 87.2%; Average loss: 4.2964\n","Iteration: 3490; Percent complete: 87.2%; Average loss: 4.2235\n","Iteration: 3491; Percent complete: 87.3%; Average loss: 4.1608\n","Iteration: 3492; Percent complete: 87.3%; Average loss: 3.9500\n","Iteration: 3493; Percent complete: 87.3%; Average loss: 3.9286\n","Iteration: 3494; Percent complete: 87.4%; Average loss: 4.0808\n","Iteration: 3495; Percent complete: 87.4%; Average loss: 4.1076\n","Iteration: 3496; Percent complete: 87.4%; Average loss: 4.1055\n","Iteration: 3497; Percent complete: 87.4%; Average loss: 3.9559\n","Iteration: 3498; Percent complete: 87.5%; Average loss: 3.9275\n","Iteration: 3499; Percent complete: 87.5%; Average loss: 3.9856\n","Iteration: 3500; Percent complete: 87.5%; Average loss: 3.9505\n","Iteration: 3501; Percent complete: 87.5%; Average loss: 4.2302\n","Iteration: 3502; Percent complete: 87.5%; Average loss: 3.8175\n","Iteration: 3503; Percent complete: 87.6%; Average loss: 4.0392\n","Iteration: 3504; Percent complete: 87.6%; Average loss: 4.0491\n","Iteration: 3505; Percent complete: 87.6%; Average loss: 3.9228\n","Iteration: 3506; Percent complete: 87.6%; Average loss: 4.1315\n","Iteration: 3507; Percent complete: 87.7%; Average loss: 3.8405\n","Iteration: 3508; Percent complete: 87.7%; Average loss: 4.1199\n","Iteration: 3509; Percent complete: 87.7%; Average loss: 4.1700\n","Iteration: 3510; Percent complete: 87.8%; Average loss: 3.8002\n","Iteration: 3511; Percent complete: 87.8%; Average loss: 4.0065\n","Iteration: 3512; Percent complete: 87.8%; Average loss: 3.9606\n","Iteration: 3513; Percent complete: 87.8%; Average loss: 3.8757\n","Iteration: 3514; Percent complete: 87.8%; Average loss: 3.9887\n","Iteration: 3515; Percent complete: 87.9%; Average loss: 4.0071\n","Iteration: 3516; Percent complete: 87.9%; Average loss: 4.0067\n","Iteration: 3517; Percent complete: 87.9%; Average loss: 3.8733\n","Iteration: 3518; Percent complete: 87.9%; Average loss: 3.8036\n","Iteration: 3519; Percent complete: 88.0%; Average loss: 4.1549\n","Iteration: 3520; Percent complete: 88.0%; Average loss: 4.3037\n","Iteration: 3521; Percent complete: 88.0%; Average loss: 3.9472\n","Iteration: 3522; Percent complete: 88.0%; Average loss: 4.0892\n","Iteration: 3523; Percent complete: 88.1%; Average loss: 3.9996\n","Iteration: 3524; Percent complete: 88.1%; Average loss: 4.0961\n","Iteration: 3525; Percent complete: 88.1%; Average loss: 4.0507\n","Iteration: 3526; Percent complete: 88.1%; Average loss: 4.0522\n","Iteration: 3527; Percent complete: 88.2%; Average loss: 4.0692\n","Iteration: 3528; Percent complete: 88.2%; Average loss: 4.2039\n","Iteration: 3529; Percent complete: 88.2%; Average loss: 4.1715\n","Iteration: 3530; Percent complete: 88.2%; Average loss: 4.1032\n","Iteration: 3531; Percent complete: 88.3%; Average loss: 4.1514\n","Iteration: 3532; Percent complete: 88.3%; Average loss: 3.9824\n","Iteration: 3533; Percent complete: 88.3%; Average loss: 4.1953\n","Iteration: 3534; Percent complete: 88.3%; Average loss: 3.8740\n","Iteration: 3535; Percent complete: 88.4%; Average loss: 3.9178\n","Iteration: 3536; Percent complete: 88.4%; Average loss: 4.0925\n","Iteration: 3537; Percent complete: 88.4%; Average loss: 4.0986\n","Iteration: 3538; Percent complete: 88.4%; Average loss: 4.1939\n","Iteration: 3539; Percent complete: 88.5%; Average loss: 3.7187\n","Iteration: 3540; Percent complete: 88.5%; Average loss: 4.4116\n","Iteration: 3541; Percent complete: 88.5%; Average loss: 4.0152\n","Iteration: 3542; Percent complete: 88.5%; Average loss: 3.9232\n","Iteration: 3543; Percent complete: 88.6%; Average loss: 4.1305\n","Iteration: 3544; Percent complete: 88.6%; Average loss: 3.9430\n","Iteration: 3545; Percent complete: 88.6%; Average loss: 4.0027\n","Iteration: 3546; Percent complete: 88.6%; Average loss: 3.9531\n","Iteration: 3547; Percent complete: 88.7%; Average loss: 4.0873\n","Iteration: 3548; Percent complete: 88.7%; Average loss: 4.0194\n","Iteration: 3549; Percent complete: 88.7%; Average loss: 4.0795\n","Iteration: 3550; Percent complete: 88.8%; Average loss: 4.0099\n","Iteration: 3551; Percent complete: 88.8%; Average loss: 3.8772\n","Iteration: 3552; Percent complete: 88.8%; Average loss: 4.1427\n","Iteration: 3553; Percent complete: 88.8%; Average loss: 3.9846\n","Iteration: 3554; Percent complete: 88.8%; Average loss: 4.0137\n","Iteration: 3555; Percent complete: 88.9%; Average loss: 4.0218\n","Iteration: 3556; Percent complete: 88.9%; Average loss: 4.0184\n","Iteration: 3557; Percent complete: 88.9%; Average loss: 4.1393\n","Iteration: 3558; Percent complete: 88.9%; Average loss: 3.9280\n","Iteration: 3559; Percent complete: 89.0%; Average loss: 4.2692\n","Iteration: 3560; Percent complete: 89.0%; Average loss: 4.1545\n","Iteration: 3561; Percent complete: 89.0%; Average loss: 3.8590\n","Iteration: 3562; Percent complete: 89.0%; Average loss: 4.1087\n","Iteration: 3563; Percent complete: 89.1%; Average loss: 4.0746\n","Iteration: 3564; Percent complete: 89.1%; Average loss: 3.9174\n","Iteration: 3565; Percent complete: 89.1%; Average loss: 4.2019\n","Iteration: 3566; Percent complete: 89.1%; Average loss: 3.9524\n","Iteration: 3567; Percent complete: 89.2%; Average loss: 3.9867\n","Iteration: 3568; Percent complete: 89.2%; Average loss: 4.0090\n","Iteration: 3569; Percent complete: 89.2%; Average loss: 4.1104\n","Iteration: 3570; Percent complete: 89.2%; Average loss: 4.0493\n","Iteration: 3571; Percent complete: 89.3%; Average loss: 4.1164\n","Iteration: 3572; Percent complete: 89.3%; Average loss: 4.2340\n","Iteration: 3573; Percent complete: 89.3%; Average loss: 3.8461\n","Iteration: 3574; Percent complete: 89.3%; Average loss: 3.9162\n","Iteration: 3575; Percent complete: 89.4%; Average loss: 4.1724\n","Iteration: 3576; Percent complete: 89.4%; Average loss: 4.2387\n","Iteration: 3577; Percent complete: 89.4%; Average loss: 4.0540\n","Iteration: 3578; Percent complete: 89.5%; Average loss: 3.9947\n","Iteration: 3579; Percent complete: 89.5%; Average loss: 4.1046\n","Iteration: 3580; Percent complete: 89.5%; Average loss: 4.1971\n","Iteration: 3581; Percent complete: 89.5%; Average loss: 3.9611\n","Iteration: 3582; Percent complete: 89.5%; Average loss: 4.0751\n","Iteration: 3583; Percent complete: 89.6%; Average loss: 4.0016\n","Iteration: 3584; Percent complete: 89.6%; Average loss: 3.8232\n","Iteration: 3585; Percent complete: 89.6%; Average loss: 4.0867\n","Iteration: 3586; Percent complete: 89.6%; Average loss: 3.8741\n","Iteration: 3587; Percent complete: 89.7%; Average loss: 3.9545\n","Iteration: 3588; Percent complete: 89.7%; Average loss: 3.7668\n","Iteration: 3589; Percent complete: 89.7%; Average loss: 4.1263\n","Iteration: 3590; Percent complete: 89.8%; Average loss: 3.9418\n","Iteration: 3591; Percent complete: 89.8%; Average loss: 4.0997\n","Iteration: 3592; Percent complete: 89.8%; Average loss: 4.0365\n","Iteration: 3593; Percent complete: 89.8%; Average loss: 4.0203\n","Iteration: 3594; Percent complete: 89.8%; Average loss: 4.0257\n","Iteration: 3595; Percent complete: 89.9%; Average loss: 4.2792\n","Iteration: 3596; Percent complete: 89.9%; Average loss: 4.1241\n","Iteration: 3597; Percent complete: 89.9%; Average loss: 4.0140\n","Iteration: 3598; Percent complete: 90.0%; Average loss: 3.9722\n","Iteration: 3599; Percent complete: 90.0%; Average loss: 4.1451\n","Iteration: 3600; Percent complete: 90.0%; Average loss: 4.0536\n","Iteration: 3601; Percent complete: 90.0%; Average loss: 4.2038\n","Iteration: 3602; Percent complete: 90.0%; Average loss: 3.9472\n","Iteration: 3603; Percent complete: 90.1%; Average loss: 4.0237\n","Iteration: 3604; Percent complete: 90.1%; Average loss: 4.2189\n","Iteration: 3605; Percent complete: 90.1%; Average loss: 4.0875\n","Iteration: 3606; Percent complete: 90.1%; Average loss: 4.1047\n","Iteration: 3607; Percent complete: 90.2%; Average loss: 3.9487\n","Iteration: 3608; Percent complete: 90.2%; Average loss: 3.7645\n","Iteration: 3609; Percent complete: 90.2%; Average loss: 3.8990\n","Iteration: 3610; Percent complete: 90.2%; Average loss: 3.9875\n","Iteration: 3611; Percent complete: 90.3%; Average loss: 3.8650\n","Iteration: 3612; Percent complete: 90.3%; Average loss: 3.7776\n","Iteration: 3613; Percent complete: 90.3%; Average loss: 3.8615\n","Iteration: 3614; Percent complete: 90.3%; Average loss: 3.9333\n","Iteration: 3615; Percent complete: 90.4%; Average loss: 3.9520\n","Iteration: 3616; Percent complete: 90.4%; Average loss: 3.7828\n","Iteration: 3617; Percent complete: 90.4%; Average loss: 3.9302\n","Iteration: 3618; Percent complete: 90.5%; Average loss: 3.9540\n","Iteration: 3619; Percent complete: 90.5%; Average loss: 3.9633\n","Iteration: 3620; Percent complete: 90.5%; Average loss: 3.8717\n","Iteration: 3621; Percent complete: 90.5%; Average loss: 4.0538\n","Iteration: 3622; Percent complete: 90.5%; Average loss: 4.0336\n","Iteration: 3623; Percent complete: 90.6%; Average loss: 3.9693\n","Iteration: 3624; Percent complete: 90.6%; Average loss: 3.8471\n","Iteration: 3625; Percent complete: 90.6%; Average loss: 3.9217\n","Iteration: 3626; Percent complete: 90.6%; Average loss: 4.2367\n","Iteration: 3627; Percent complete: 90.7%; Average loss: 3.9628\n","Iteration: 3628; Percent complete: 90.7%; Average loss: 4.2835\n","Iteration: 3629; Percent complete: 90.7%; Average loss: 4.0536\n","Iteration: 3630; Percent complete: 90.8%; Average loss: 4.2918\n","Iteration: 3631; Percent complete: 90.8%; Average loss: 3.9783\n","Iteration: 3632; Percent complete: 90.8%; Average loss: 4.0536\n","Iteration: 3633; Percent complete: 90.8%; Average loss: 4.0354\n","Iteration: 3634; Percent complete: 90.8%; Average loss: 3.8234\n","Iteration: 3635; Percent complete: 90.9%; Average loss: 3.9850\n","Iteration: 3636; Percent complete: 90.9%; Average loss: 3.9349\n","Iteration: 3637; Percent complete: 90.9%; Average loss: 4.1170\n","Iteration: 3638; Percent complete: 91.0%; Average loss: 4.0654\n","Iteration: 3639; Percent complete: 91.0%; Average loss: 4.0980\n","Iteration: 3640; Percent complete: 91.0%; Average loss: 4.1390\n","Iteration: 3641; Percent complete: 91.0%; Average loss: 3.9810\n","Iteration: 3642; Percent complete: 91.0%; Average loss: 4.0199\n","Iteration: 3643; Percent complete: 91.1%; Average loss: 4.1521\n","Iteration: 3644; Percent complete: 91.1%; Average loss: 4.0034\n","Iteration: 3645; Percent complete: 91.1%; Average loss: 3.8940\n","Iteration: 3646; Percent complete: 91.1%; Average loss: 4.1573\n","Iteration: 3647; Percent complete: 91.2%; Average loss: 3.9916\n","Iteration: 3648; Percent complete: 91.2%; Average loss: 4.0640\n","Iteration: 3649; Percent complete: 91.2%; Average loss: 4.0352\n","Iteration: 3650; Percent complete: 91.2%; Average loss: 4.0131\n","Iteration: 3651; Percent complete: 91.3%; Average loss: 3.9744\n","Iteration: 3652; Percent complete: 91.3%; Average loss: 4.0284\n","Iteration: 3653; Percent complete: 91.3%; Average loss: 3.9086\n","Iteration: 3654; Percent complete: 91.3%; Average loss: 4.0116\n","Iteration: 3655; Percent complete: 91.4%; Average loss: 4.2249\n","Iteration: 3656; Percent complete: 91.4%; Average loss: 3.8836\n","Iteration: 3657; Percent complete: 91.4%; Average loss: 3.8726\n","Iteration: 3658; Percent complete: 91.5%; Average loss: 4.3136\n","Iteration: 3659; Percent complete: 91.5%; Average loss: 3.9070\n","Iteration: 3660; Percent complete: 91.5%; Average loss: 4.3244\n","Iteration: 3661; Percent complete: 91.5%; Average loss: 3.8978\n","Iteration: 3662; Percent complete: 91.5%; Average loss: 3.9786\n","Iteration: 3663; Percent complete: 91.6%; Average loss: 3.8151\n","Iteration: 3664; Percent complete: 91.6%; Average loss: 4.0354\n","Iteration: 3665; Percent complete: 91.6%; Average loss: 4.2771\n","Iteration: 3666; Percent complete: 91.6%; Average loss: 4.0424\n","Iteration: 3667; Percent complete: 91.7%; Average loss: 3.9660\n","Iteration: 3668; Percent complete: 91.7%; Average loss: 3.9135\n","Iteration: 3669; Percent complete: 91.7%; Average loss: 3.9112\n","Iteration: 3670; Percent complete: 91.8%; Average loss: 4.0646\n","Iteration: 3671; Percent complete: 91.8%; Average loss: 3.9066\n","Iteration: 3672; Percent complete: 91.8%; Average loss: 4.1010\n","Iteration: 3673; Percent complete: 91.8%; Average loss: 4.1670\n","Iteration: 3674; Percent complete: 91.8%; Average loss: 3.9338\n","Iteration: 3675; Percent complete: 91.9%; Average loss: 4.1236\n","Iteration: 3676; Percent complete: 91.9%; Average loss: 4.0441\n","Iteration: 3677; Percent complete: 91.9%; Average loss: 4.0342\n","Iteration: 3678; Percent complete: 92.0%; Average loss: 3.6908\n","Iteration: 3679; Percent complete: 92.0%; Average loss: 4.0903\n","Iteration: 3680; Percent complete: 92.0%; Average loss: 3.9374\n","Iteration: 3681; Percent complete: 92.0%; Average loss: 4.2026\n","Iteration: 3682; Percent complete: 92.0%; Average loss: 3.8340\n","Iteration: 3683; Percent complete: 92.1%; Average loss: 4.1546\n","Iteration: 3684; Percent complete: 92.1%; Average loss: 3.9060\n","Iteration: 3685; Percent complete: 92.1%; Average loss: 3.7824\n","Iteration: 3686; Percent complete: 92.2%; Average loss: 3.9512\n","Iteration: 3687; Percent complete: 92.2%; Average loss: 4.1025\n","Iteration: 3688; Percent complete: 92.2%; Average loss: 3.9944\n","Iteration: 3689; Percent complete: 92.2%; Average loss: 4.1496\n","Iteration: 3690; Percent complete: 92.2%; Average loss: 4.1304\n","Iteration: 3691; Percent complete: 92.3%; Average loss: 4.2298\n","Iteration: 3692; Percent complete: 92.3%; Average loss: 3.9862\n","Iteration: 3693; Percent complete: 92.3%; Average loss: 4.0512\n","Iteration: 3694; Percent complete: 92.3%; Average loss: 4.0236\n","Iteration: 3695; Percent complete: 92.4%; Average loss: 4.0873\n","Iteration: 3696; Percent complete: 92.4%; Average loss: 3.9687\n","Iteration: 3697; Percent complete: 92.4%; Average loss: 4.0479\n","Iteration: 3698; Percent complete: 92.5%; Average loss: 4.0470\n","Iteration: 3699; Percent complete: 92.5%; Average loss: 3.7319\n","Iteration: 3700; Percent complete: 92.5%; Average loss: 3.9452\n","Iteration: 3701; Percent complete: 92.5%; Average loss: 4.1225\n","Iteration: 3702; Percent complete: 92.5%; Average loss: 4.0824\n","Iteration: 3703; Percent complete: 92.6%; Average loss: 4.0284\n","Iteration: 3704; Percent complete: 92.6%; Average loss: 4.0369\n","Iteration: 3705; Percent complete: 92.6%; Average loss: 3.8854\n","Iteration: 3706; Percent complete: 92.7%; Average loss: 4.1354\n","Iteration: 3707; Percent complete: 92.7%; Average loss: 3.9384\n","Iteration: 3708; Percent complete: 92.7%; Average loss: 4.1604\n","Iteration: 3709; Percent complete: 92.7%; Average loss: 3.9462\n","Iteration: 3710; Percent complete: 92.8%; Average loss: 3.9935\n","Iteration: 3711; Percent complete: 92.8%; Average loss: 3.6898\n","Iteration: 3712; Percent complete: 92.8%; Average loss: 3.8472\n","Iteration: 3713; Percent complete: 92.8%; Average loss: 4.2387\n","Iteration: 3714; Percent complete: 92.8%; Average loss: 3.8518\n","Iteration: 3715; Percent complete: 92.9%; Average loss: 4.1461\n","Iteration: 3716; Percent complete: 92.9%; Average loss: 4.3211\n","Iteration: 3717; Percent complete: 92.9%; Average loss: 4.2896\n","Iteration: 3718; Percent complete: 93.0%; Average loss: 3.7988\n","Iteration: 3719; Percent complete: 93.0%; Average loss: 4.0846\n","Iteration: 3720; Percent complete: 93.0%; Average loss: 4.0291\n","Iteration: 3721; Percent complete: 93.0%; Average loss: 4.2038\n","Iteration: 3722; Percent complete: 93.0%; Average loss: 3.8950\n","Iteration: 3723; Percent complete: 93.1%; Average loss: 4.0958\n","Iteration: 3724; Percent complete: 93.1%; Average loss: 4.2235\n","Iteration: 3725; Percent complete: 93.1%; Average loss: 4.1772\n","Iteration: 3726; Percent complete: 93.2%; Average loss: 3.8919\n","Iteration: 3727; Percent complete: 93.2%; Average loss: 4.0391\n","Iteration: 3728; Percent complete: 93.2%; Average loss: 3.9829\n","Iteration: 3729; Percent complete: 93.2%; Average loss: 3.9844\n","Iteration: 3730; Percent complete: 93.2%; Average loss: 4.0525\n","Iteration: 3731; Percent complete: 93.3%; Average loss: 3.8850\n","Iteration: 3732; Percent complete: 93.3%; Average loss: 4.0057\n","Iteration: 3733; Percent complete: 93.3%; Average loss: 3.9687\n","Iteration: 3734; Percent complete: 93.3%; Average loss: 3.8520\n","Iteration: 3735; Percent complete: 93.4%; Average loss: 3.9881\n","Iteration: 3736; Percent complete: 93.4%; Average loss: 4.0703\n","Iteration: 3737; Percent complete: 93.4%; Average loss: 4.1122\n","Iteration: 3738; Percent complete: 93.5%; Average loss: 3.9537\n","Iteration: 3739; Percent complete: 93.5%; Average loss: 4.1431\n","Iteration: 3740; Percent complete: 93.5%; Average loss: 3.9440\n","Iteration: 3741; Percent complete: 93.5%; Average loss: 3.9874\n","Iteration: 3742; Percent complete: 93.5%; Average loss: 3.9308\n","Iteration: 3743; Percent complete: 93.6%; Average loss: 4.1249\n","Iteration: 3744; Percent complete: 93.6%; Average loss: 4.1375\n","Iteration: 3745; Percent complete: 93.6%; Average loss: 3.9152\n","Iteration: 3746; Percent complete: 93.7%; Average loss: 3.6762\n","Iteration: 3747; Percent complete: 93.7%; Average loss: 4.0903\n","Iteration: 3748; Percent complete: 93.7%; Average loss: 4.0371\n","Iteration: 3749; Percent complete: 93.7%; Average loss: 3.9756\n","Iteration: 3750; Percent complete: 93.8%; Average loss: 3.8514\n","Iteration: 3751; Percent complete: 93.8%; Average loss: 3.9944\n","Iteration: 3752; Percent complete: 93.8%; Average loss: 3.8339\n","Iteration: 3753; Percent complete: 93.8%; Average loss: 3.8979\n","Iteration: 3754; Percent complete: 93.8%; Average loss: 3.9592\n","Iteration: 3755; Percent complete: 93.9%; Average loss: 4.0646\n","Iteration: 3756; Percent complete: 93.9%; Average loss: 3.9510\n","Iteration: 3757; Percent complete: 93.9%; Average loss: 3.9081\n","Iteration: 3758; Percent complete: 94.0%; Average loss: 3.9919\n","Iteration: 3759; Percent complete: 94.0%; Average loss: 4.0337\n","Iteration: 3760; Percent complete: 94.0%; Average loss: 3.7773\n","Iteration: 3761; Percent complete: 94.0%; Average loss: 3.6923\n","Iteration: 3762; Percent complete: 94.0%; Average loss: 4.0791\n","Iteration: 3763; Percent complete: 94.1%; Average loss: 3.7898\n","Iteration: 3764; Percent complete: 94.1%; Average loss: 3.9614\n","Iteration: 3765; Percent complete: 94.1%; Average loss: 3.8602\n","Iteration: 3766; Percent complete: 94.2%; Average loss: 4.1171\n","Iteration: 3767; Percent complete: 94.2%; Average loss: 3.8563\n","Iteration: 3768; Percent complete: 94.2%; Average loss: 4.3945\n","Iteration: 3769; Percent complete: 94.2%; Average loss: 3.6839\n","Iteration: 3770; Percent complete: 94.2%; Average loss: 4.1284\n","Iteration: 3771; Percent complete: 94.3%; Average loss: 3.8874\n","Iteration: 3772; Percent complete: 94.3%; Average loss: 3.8648\n","Iteration: 3773; Percent complete: 94.3%; Average loss: 4.0117\n","Iteration: 3774; Percent complete: 94.3%; Average loss: 3.8761\n","Iteration: 3775; Percent complete: 94.4%; Average loss: 3.8426\n","Iteration: 3776; Percent complete: 94.4%; Average loss: 3.9787\n","Iteration: 3777; Percent complete: 94.4%; Average loss: 4.1009\n","Iteration: 3778; Percent complete: 94.5%; Average loss: 4.0742\n","Iteration: 3779; Percent complete: 94.5%; Average loss: 4.1389\n","Iteration: 3780; Percent complete: 94.5%; Average loss: 4.2042\n","Iteration: 3781; Percent complete: 94.5%; Average loss: 3.7931\n","Iteration: 3782; Percent complete: 94.5%; Average loss: 4.0164\n","Iteration: 3783; Percent complete: 94.6%; Average loss: 4.0648\n","Iteration: 3784; Percent complete: 94.6%; Average loss: 4.1664\n","Iteration: 3785; Percent complete: 94.6%; Average loss: 4.1331\n","Iteration: 3786; Percent complete: 94.7%; Average loss: 3.9750\n","Iteration: 3787; Percent complete: 94.7%; Average loss: 3.8987\n","Iteration: 3788; Percent complete: 94.7%; Average loss: 3.8557\n","Iteration: 3789; Percent complete: 94.7%; Average loss: 3.8569\n","Iteration: 3790; Percent complete: 94.8%; Average loss: 3.9250\n","Iteration: 3791; Percent complete: 94.8%; Average loss: 3.9957\n","Iteration: 3792; Percent complete: 94.8%; Average loss: 4.1001\n","Iteration: 3793; Percent complete: 94.8%; Average loss: 3.8584\n","Iteration: 3794; Percent complete: 94.8%; Average loss: 4.3081\n","Iteration: 3795; Percent complete: 94.9%; Average loss: 3.9831\n","Iteration: 3796; Percent complete: 94.9%; Average loss: 4.0007\n","Iteration: 3797; Percent complete: 94.9%; Average loss: 3.8075\n","Iteration: 3798; Percent complete: 95.0%; Average loss: 4.0513\n","Iteration: 3799; Percent complete: 95.0%; Average loss: 4.1075\n","Iteration: 3800; Percent complete: 95.0%; Average loss: 4.3459\n","Iteration: 3801; Percent complete: 95.0%; Average loss: 4.2026\n","Iteration: 3802; Percent complete: 95.0%; Average loss: 4.2024\n","Iteration: 3803; Percent complete: 95.1%; Average loss: 3.8192\n","Iteration: 3804; Percent complete: 95.1%; Average loss: 4.1076\n","Iteration: 3805; Percent complete: 95.1%; Average loss: 3.8908\n","Iteration: 3806; Percent complete: 95.2%; Average loss: 4.0970\n","Iteration: 3807; Percent complete: 95.2%; Average loss: 3.9771\n","Iteration: 3808; Percent complete: 95.2%; Average loss: 3.8850\n","Iteration: 3809; Percent complete: 95.2%; Average loss: 4.1028\n","Iteration: 3810; Percent complete: 95.2%; Average loss: 3.9905\n","Iteration: 3811; Percent complete: 95.3%; Average loss: 3.9597\n","Iteration: 3812; Percent complete: 95.3%; Average loss: 4.1930\n","Iteration: 3813; Percent complete: 95.3%; Average loss: 3.8768\n","Iteration: 3814; Percent complete: 95.3%; Average loss: 4.1968\n","Iteration: 3815; Percent complete: 95.4%; Average loss: 4.0533\n","Iteration: 3816; Percent complete: 95.4%; Average loss: 3.7930\n","Iteration: 3817; Percent complete: 95.4%; Average loss: 4.0222\n","Iteration: 3818; Percent complete: 95.5%; Average loss: 4.0405\n","Iteration: 3819; Percent complete: 95.5%; Average loss: 3.8445\n","Iteration: 3820; Percent complete: 95.5%; Average loss: 4.0362\n","Iteration: 3821; Percent complete: 95.5%; Average loss: 4.0700\n","Iteration: 3822; Percent complete: 95.5%; Average loss: 3.8717\n","Iteration: 3823; Percent complete: 95.6%; Average loss: 3.7229\n","Iteration: 3824; Percent complete: 95.6%; Average loss: 3.8647\n","Iteration: 3825; Percent complete: 95.6%; Average loss: 3.9708\n","Iteration: 3826; Percent complete: 95.7%; Average loss: 3.8091\n","Iteration: 3827; Percent complete: 95.7%; Average loss: 4.0672\n","Iteration: 3828; Percent complete: 95.7%; Average loss: 3.7939\n","Iteration: 3829; Percent complete: 95.7%; Average loss: 3.9876\n","Iteration: 3830; Percent complete: 95.8%; Average loss: 4.0278\n","Iteration: 3831; Percent complete: 95.8%; Average loss: 4.1441\n","Iteration: 3832; Percent complete: 95.8%; Average loss: 4.0732\n","Iteration: 3833; Percent complete: 95.8%; Average loss: 3.9360\n","Iteration: 3834; Percent complete: 95.9%; Average loss: 3.9029\n","Iteration: 3835; Percent complete: 95.9%; Average loss: 4.1555\n","Iteration: 3836; Percent complete: 95.9%; Average loss: 3.8197\n","Iteration: 3837; Percent complete: 95.9%; Average loss: 4.0014\n","Iteration: 3838; Percent complete: 96.0%; Average loss: 3.7255\n","Iteration: 3839; Percent complete: 96.0%; Average loss: 4.0625\n","Iteration: 3840; Percent complete: 96.0%; Average loss: 4.0455\n","Iteration: 3841; Percent complete: 96.0%; Average loss: 4.0584\n","Iteration: 3842; Percent complete: 96.0%; Average loss: 4.0397\n","Iteration: 3843; Percent complete: 96.1%; Average loss: 3.8928\n","Iteration: 3844; Percent complete: 96.1%; Average loss: 4.0200\n","Iteration: 3845; Percent complete: 96.1%; Average loss: 4.0301\n","Iteration: 3846; Percent complete: 96.2%; Average loss: 4.0806\n","Iteration: 3847; Percent complete: 96.2%; Average loss: 3.7856\n","Iteration: 3848; Percent complete: 96.2%; Average loss: 4.0951\n","Iteration: 3849; Percent complete: 96.2%; Average loss: 3.8739\n","Iteration: 3850; Percent complete: 96.2%; Average loss: 4.0653\n","Iteration: 3851; Percent complete: 96.3%; Average loss: 3.8228\n","Iteration: 3852; Percent complete: 96.3%; Average loss: 4.0958\n","Iteration: 3853; Percent complete: 96.3%; Average loss: 4.1018\n","Iteration: 3854; Percent complete: 96.4%; Average loss: 3.9788\n","Iteration: 3855; Percent complete: 96.4%; Average loss: 3.9733\n","Iteration: 3856; Percent complete: 96.4%; Average loss: 4.1419\n","Iteration: 3857; Percent complete: 96.4%; Average loss: 3.9608\n","Iteration: 3858; Percent complete: 96.5%; Average loss: 3.9826\n","Iteration: 3859; Percent complete: 96.5%; Average loss: 3.8274\n","Iteration: 3860; Percent complete: 96.5%; Average loss: 4.1077\n","Iteration: 3861; Percent complete: 96.5%; Average loss: 3.9117\n","Iteration: 3862; Percent complete: 96.5%; Average loss: 4.1438\n","Iteration: 3863; Percent complete: 96.6%; Average loss: 3.9861\n","Iteration: 3864; Percent complete: 96.6%; Average loss: 4.0790\n","Iteration: 3865; Percent complete: 96.6%; Average loss: 3.9868\n","Iteration: 3866; Percent complete: 96.7%; Average loss: 3.8521\n","Iteration: 3867; Percent complete: 96.7%; Average loss: 3.8966\n","Iteration: 3868; Percent complete: 96.7%; Average loss: 4.1067\n","Iteration: 3869; Percent complete: 96.7%; Average loss: 4.0175\n","Iteration: 3870; Percent complete: 96.8%; Average loss: 3.7233\n","Iteration: 3871; Percent complete: 96.8%; Average loss: 4.3461\n","Iteration: 3872; Percent complete: 96.8%; Average loss: 3.9053\n","Iteration: 3873; Percent complete: 96.8%; Average loss: 3.8853\n","Iteration: 3874; Percent complete: 96.9%; Average loss: 3.9529\n","Iteration: 3875; Percent complete: 96.9%; Average loss: 3.9505\n","Iteration: 3876; Percent complete: 96.9%; Average loss: 3.7347\n","Iteration: 3877; Percent complete: 96.9%; Average loss: 3.8319\n","Iteration: 3878; Percent complete: 97.0%; Average loss: 3.9738\n","Iteration: 3879; Percent complete: 97.0%; Average loss: 4.0799\n","Iteration: 3880; Percent complete: 97.0%; Average loss: 4.0069\n","Iteration: 3881; Percent complete: 97.0%; Average loss: 3.8062\n","Iteration: 3882; Percent complete: 97.0%; Average loss: 4.0100\n","Iteration: 3883; Percent complete: 97.1%; Average loss: 4.0680\n","Iteration: 3884; Percent complete: 97.1%; Average loss: 3.8950\n","Iteration: 3885; Percent complete: 97.1%; Average loss: 4.0517\n","Iteration: 3886; Percent complete: 97.2%; Average loss: 4.0302\n","Iteration: 3887; Percent complete: 97.2%; Average loss: 3.8986\n","Iteration: 3888; Percent complete: 97.2%; Average loss: 4.0919\n","Iteration: 3889; Percent complete: 97.2%; Average loss: 4.0597\n","Iteration: 3890; Percent complete: 97.2%; Average loss: 4.1560\n","Iteration: 3891; Percent complete: 97.3%; Average loss: 3.9407\n","Iteration: 3892; Percent complete: 97.3%; Average loss: 4.0361\n","Iteration: 3893; Percent complete: 97.3%; Average loss: 3.9900\n","Iteration: 3894; Percent complete: 97.4%; Average loss: 3.9173\n","Iteration: 3895; Percent complete: 97.4%; Average loss: 3.9997\n","Iteration: 3896; Percent complete: 97.4%; Average loss: 3.9515\n","Iteration: 3897; Percent complete: 97.4%; Average loss: 3.9137\n","Iteration: 3898; Percent complete: 97.5%; Average loss: 3.8343\n","Iteration: 3899; Percent complete: 97.5%; Average loss: 4.0428\n","Iteration: 3900; Percent complete: 97.5%; Average loss: 4.0670\n","Iteration: 3901; Percent complete: 97.5%; Average loss: 3.9396\n","Iteration: 3902; Percent complete: 97.5%; Average loss: 3.9015\n","Iteration: 3903; Percent complete: 97.6%; Average loss: 4.0617\n","Iteration: 3904; Percent complete: 97.6%; Average loss: 3.8762\n","Iteration: 3905; Percent complete: 97.6%; Average loss: 3.9715\n","Iteration: 3906; Percent complete: 97.7%; Average loss: 4.0878\n","Iteration: 3907; Percent complete: 97.7%; Average loss: 3.9683\n","Iteration: 3908; Percent complete: 97.7%; Average loss: 3.9051\n","Iteration: 3909; Percent complete: 97.7%; Average loss: 3.6285\n","Iteration: 3910; Percent complete: 97.8%; Average loss: 3.8188\n","Iteration: 3911; Percent complete: 97.8%; Average loss: 4.1497\n","Iteration: 3912; Percent complete: 97.8%; Average loss: 4.0721\n","Iteration: 3913; Percent complete: 97.8%; Average loss: 3.7165\n","Iteration: 3914; Percent complete: 97.9%; Average loss: 3.7264\n","Iteration: 3915; Percent complete: 97.9%; Average loss: 4.0126\n","Iteration: 3916; Percent complete: 97.9%; Average loss: 4.0817\n","Iteration: 3917; Percent complete: 97.9%; Average loss: 3.6912\n","Iteration: 3918; Percent complete: 98.0%; Average loss: 3.8784\n","Iteration: 3919; Percent complete: 98.0%; Average loss: 3.9993\n","Iteration: 3920; Percent complete: 98.0%; Average loss: 3.9836\n","Iteration: 3921; Percent complete: 98.0%; Average loss: 3.9388\n","Iteration: 3922; Percent complete: 98.0%; Average loss: 3.8371\n","Iteration: 3923; Percent complete: 98.1%; Average loss: 3.8370\n","Iteration: 3924; Percent complete: 98.1%; Average loss: 4.0683\n","Iteration: 3925; Percent complete: 98.1%; Average loss: 3.7731\n","Iteration: 3926; Percent complete: 98.2%; Average loss: 3.9743\n","Iteration: 3927; Percent complete: 98.2%; Average loss: 3.6829\n","Iteration: 3928; Percent complete: 98.2%; Average loss: 3.9106\n","Iteration: 3929; Percent complete: 98.2%; Average loss: 4.0803\n","Iteration: 3930; Percent complete: 98.2%; Average loss: 4.0671\n","Iteration: 3931; Percent complete: 98.3%; Average loss: 4.1786\n","Iteration: 3932; Percent complete: 98.3%; Average loss: 3.9193\n","Iteration: 3933; Percent complete: 98.3%; Average loss: 3.8833\n","Iteration: 3934; Percent complete: 98.4%; Average loss: 4.1478\n","Iteration: 3935; Percent complete: 98.4%; Average loss: 4.0969\n","Iteration: 3936; Percent complete: 98.4%; Average loss: 3.7162\n","Iteration: 3937; Percent complete: 98.4%; Average loss: 3.8586\n","Iteration: 3938; Percent complete: 98.5%; Average loss: 4.1809\n","Iteration: 3939; Percent complete: 98.5%; Average loss: 4.0179\n","Iteration: 3940; Percent complete: 98.5%; Average loss: 3.9472\n","Iteration: 3941; Percent complete: 98.5%; Average loss: 4.0482\n","Iteration: 3942; Percent complete: 98.6%; Average loss: 3.9223\n","Iteration: 3943; Percent complete: 98.6%; Average loss: 4.2268\n","Iteration: 3944; Percent complete: 98.6%; Average loss: 4.1011\n","Iteration: 3945; Percent complete: 98.6%; Average loss: 3.9606\n","Iteration: 3946; Percent complete: 98.7%; Average loss: 3.9788\n","Iteration: 3947; Percent complete: 98.7%; Average loss: 4.1318\n","Iteration: 3948; Percent complete: 98.7%; Average loss: 3.8200\n","Iteration: 3949; Percent complete: 98.7%; Average loss: 4.0693\n","Iteration: 3950; Percent complete: 98.8%; Average loss: 4.0142\n","Iteration: 3951; Percent complete: 98.8%; Average loss: 3.8985\n","Iteration: 3952; Percent complete: 98.8%; Average loss: 4.0069\n","Iteration: 3953; Percent complete: 98.8%; Average loss: 4.1382\n","Iteration: 3954; Percent complete: 98.9%; Average loss: 3.5932\n","Iteration: 3955; Percent complete: 98.9%; Average loss: 3.8953\n","Iteration: 3956; Percent complete: 98.9%; Average loss: 4.0863\n","Iteration: 3957; Percent complete: 98.9%; Average loss: 3.9232\n","Iteration: 3958; Percent complete: 99.0%; Average loss: 3.8919\n","Iteration: 3959; Percent complete: 99.0%; Average loss: 3.8427\n","Iteration: 3960; Percent complete: 99.0%; Average loss: 3.8925\n","Iteration: 3961; Percent complete: 99.0%; Average loss: 3.9847\n","Iteration: 3962; Percent complete: 99.1%; Average loss: 3.9764\n","Iteration: 3963; Percent complete: 99.1%; Average loss: 3.6312\n","Iteration: 3964; Percent complete: 99.1%; Average loss: 3.8753\n","Iteration: 3965; Percent complete: 99.1%; Average loss: 4.1166\n","Iteration: 3966; Percent complete: 99.2%; Average loss: 3.9751\n","Iteration: 3967; Percent complete: 99.2%; Average loss: 3.6848\n","Iteration: 3968; Percent complete: 99.2%; Average loss: 3.8420\n","Iteration: 3969; Percent complete: 99.2%; Average loss: 3.9328\n","Iteration: 3970; Percent complete: 99.2%; Average loss: 3.9737\n","Iteration: 3971; Percent complete: 99.3%; Average loss: 4.3080\n","Iteration: 3972; Percent complete: 99.3%; Average loss: 3.9791\n","Iteration: 3973; Percent complete: 99.3%; Average loss: 3.8164\n","Iteration: 3974; Percent complete: 99.4%; Average loss: 3.9426\n","Iteration: 3975; Percent complete: 99.4%; Average loss: 3.9597\n","Iteration: 3976; Percent complete: 99.4%; Average loss: 3.8369\n","Iteration: 3977; Percent complete: 99.4%; Average loss: 3.9565\n","Iteration: 3978; Percent complete: 99.5%; Average loss: 4.1117\n","Iteration: 3979; Percent complete: 99.5%; Average loss: 3.8757\n","Iteration: 3980; Percent complete: 99.5%; Average loss: 4.0986\n","Iteration: 3981; Percent complete: 99.5%; Average loss: 3.8084\n","Iteration: 3982; Percent complete: 99.6%; Average loss: 3.7472\n","Iteration: 3983; Percent complete: 99.6%; Average loss: 4.0250\n","Iteration: 3984; Percent complete: 99.6%; Average loss: 4.0470\n","Iteration: 3985; Percent complete: 99.6%; Average loss: 4.0373\n","Iteration: 3986; Percent complete: 99.7%; Average loss: 4.0525\n","Iteration: 3987; Percent complete: 99.7%; Average loss: 4.1849\n","Iteration: 3988; Percent complete: 99.7%; Average loss: 3.9439\n","Iteration: 3989; Percent complete: 99.7%; Average loss: 3.7903\n","Iteration: 3990; Percent complete: 99.8%; Average loss: 4.0228\n","Iteration: 3991; Percent complete: 99.8%; Average loss: 3.9388\n","Iteration: 3992; Percent complete: 99.8%; Average loss: 3.9272\n","Iteration: 3993; Percent complete: 99.8%; Average loss: 3.8643\n","Iteration: 3994; Percent complete: 99.9%; Average loss: 3.9066\n","Iteration: 3995; Percent complete: 99.9%; Average loss: 3.7387\n","Iteration: 3996; Percent complete: 99.9%; Average loss: 4.0714\n","Iteration: 3997; Percent complete: 99.9%; Average loss: 3.7697\n","Iteration: 3998; Percent complete: 100.0%; Average loss: 3.9599\n","Iteration: 3999; Percent complete: 100.0%; Average loss: 4.0342\n","Iteration: 4000; Percent complete: 100.0%; Average loss: 3.8756\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uMwIUS6W4hcD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"a208a3a5-9251-458f-fd2d-ec4cadb0a3af","executionInfo":{"status":"ok","timestamp":1572124256451,"user_tz":-780,"elapsed":114071,"user":{"displayName":"Dylan Webb","photoUrl":"","userId":"17339420517359398809"}}},"source":["!python run.py results all"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Added valid_rare.json to dataset\n","0 pairs trimmed, 7913 remain\n","Added train.json to dataset\n","0 pairs trimmed, 134565 remain\n","Added test_freq.json to dataset\n","0 pairs trimmed, 142521 remain\n","Added valid_freq.json to dataset\n","0 pairs trimmed, 150238 remain\n","Added test_rare.json to dataset\n","0 pairs trimmed, 158155 remain\n","Building encoder and decoder ...\n","Models built and ready to go!\n","Initialising Environment...\n","50_epochs (1).tar\n","Building ADEM model ...\n","2_epochs.tar\n","Building Adversarial_Discriminator model ...\n","Building encoder and decoder ...\n","Models built and ready to go!\n","Initialising Environment...\n","50_epochs (1).tar\n","Building ADEM model ...\n","2_epochs.tar\n","Building Adversarial_Discriminator model ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"524jzpVm_4p9","colab_type":"code","colab":{}},"source":["!python run.py chat rl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTRqRtXNZvl9","colab_type":"code","outputId":"b8979a2d-56fc-4237-bac1-df0462f9dee6","executionInfo":{"status":"ok","timestamp":1571833825506,"user_tz":-780,"elapsed":133532,"user":{"displayName":"Chester Holt","photoUrl":"","userId":"05242144140561707601"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run.py train adem\n","#currently error at test_epoch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Added valid_rare.json to dataset\n","0 pairs trimmed, 6496 remain\n","Added train.json to dataset\n","0 pairs trimmed, 111659 remain\n","Added test_freq.json to dataset\n","0 pairs trimmed, 118376 remain\n","Added valid_freq.json to dataset\n","0 pairs trimmed, 124839 remain\n","Added test_rare.json to dataset\n","0 pairs trimmed, 131528 remain\n","Start preparing training data ...\n","Counting words...\n","Counted words: 35218\n","Added valid_rare.json to dataset\n","0 pairs trimmed, 6496 remain\n","Added train.json to dataset\n","0 pairs trimmed, 111659 remain\n","Added test_freq.json to dataset\n","0 pairs trimmed, 118376 remain\n","Added valid_freq.json to dataset\n","0 pairs trimmed, 124839 remain\n","Added test_rare.json to dataset\n","0 pairs trimmed, 131528 remain\n","4431 pairs trimmed, 2286 remain\n","Training...\n","Train Epoch: 1 [1280/131528 (1%)]\tLoss: 6.39\n","Train Epoch: 1 [2560/131528 (2%)]\tLoss: 5.59\n","Train Epoch: 1 [3840/131528 (3%)]\tLoss: 5.26\n","Train Epoch: 1 [5120/131528 (4%)]\tLoss: 5.09\n","Train Epoch: 1 [6400/131528 (5%)]\tLoss: 4.96\n","Train Epoch: 1 [7680/131528 (6%)]\tLoss: 4.85\n","Train Epoch: 1 [8960/131528 (7%)]\tLoss: 4.77\n","Train Epoch: 1 [10240/131528 (8%)]\tLoss: 4.72\n","Train Epoch: 1 [11520/131528 (9%)]\tLoss: 4.69\n","Train Epoch: 1 [12800/131528 (10%)]\tLoss: 4.66\n","Train Epoch: 1 [14080/131528 (11%)]\tLoss: 4.62\n","Train Epoch: 1 [15360/131528 (12%)]\tLoss: 4.59\n","Train Epoch: 1 [16640/131528 (13%)]\tLoss: 4.57\n","Train Epoch: 1 [17920/131528 (14%)]\tLoss: 4.55\n","Train Epoch: 1 [19200/131528 (15%)]\tLoss: 4.55\n","Train Epoch: 1 [20480/131528 (16%)]\tLoss: 4.53\n","Train Epoch: 1 [21760/131528 (17%)]\tLoss: 4.51\n","Train Epoch: 1 [23040/131528 (18%)]\tLoss: 4.51\n","Train Epoch: 1 [24320/131528 (18%)]\tLoss: 4.50\n","Train Epoch: 1 [25600/131528 (19%)]\tLoss: 4.50\n","Train Epoch: 1 [26880/131528 (20%)]\tLoss: 4.49\n","Train Epoch: 1 [28160/131528 (21%)]\tLoss: 4.48\n","Train Epoch: 1 [29440/131528 (22%)]\tLoss: 4.47\n","Train Epoch: 1 [30720/131528 (23%)]\tLoss: 4.47\n","Train Epoch: 1 [32000/131528 (24%)]\tLoss: 4.46\n","Train Epoch: 1 [33280/131528 (25%)]\tLoss: 4.45\n","Train Epoch: 1 [34560/131528 (26%)]\tLoss: 4.43\n","Train Epoch: 1 [35840/131528 (27%)]\tLoss: 4.43\n","Train Epoch: 1 [37120/131528 (28%)]\tLoss: 4.42\n","Train Epoch: 1 [38400/131528 (29%)]\tLoss: 4.42\n","Train Epoch: 1 [39680/131528 (30%)]\tLoss: 4.42\n","Train Epoch: 1 [40960/131528 (31%)]\tLoss: 4.41\n","Train Epoch: 1 [42240/131528 (32%)]\tLoss: 4.41\n","Train Epoch: 1 [43520/131528 (33%)]\tLoss: 4.40\n","Train Epoch: 1 [44800/131528 (34%)]\tLoss: 4.39\n","Train Epoch: 1 [46080/131528 (35%)]\tLoss: 4.39\n","Train Epoch: 1 [47360/131528 (36%)]\tLoss: 4.39\n","Train Epoch: 1 [48640/131528 (37%)]\tLoss: 4.38\n","Train Epoch: 1 [49920/131528 (38%)]\tLoss: 4.38\n","Train Epoch: 1 [51200/131528 (39%)]\tLoss: 4.38\n","Train Epoch: 1 [52480/131528 (40%)]\tLoss: 4.37\n","Train Epoch: 1 [53760/131528 (41%)]\tLoss: 4.37\n","Train Epoch: 1 [55040/131528 (42%)]\tLoss: 4.36\n","Train Epoch: 1 [56320/131528 (43%)]\tLoss: 4.36\n","Train Epoch: 1 [57600/131528 (44%)]\tLoss: 4.36\n","Train Epoch: 1 [58880/131528 (45%)]\tLoss: 4.35\n","Train Epoch: 1 [60160/131528 (46%)]\tLoss: 4.35\n","Train Epoch: 1 [61440/131528 (47%)]\tLoss: 4.35\n","Train Epoch: 1 [62720/131528 (48%)]\tLoss: 4.35\n","Train Epoch: 1 [64000/131528 (49%)]\tLoss: 4.35\n","Train Epoch: 1 [65280/131528 (50%)]\tLoss: 4.35\n","Train Epoch: 1 [66560/131528 (51%)]\tLoss: 4.35\n","Train Epoch: 1 [67840/131528 (52%)]\tLoss: 4.34\n","Train Epoch: 1 [69120/131528 (53%)]\tLoss: 4.33\n","Train Epoch: 1 [70400/131528 (54%)]\tLoss: 4.33\n","Train Epoch: 1 [71680/131528 (54%)]\tLoss: 4.33\n","Train Epoch: 1 [72960/131528 (55%)]\tLoss: 4.32\n","Train Epoch: 1 [74240/131528 (56%)]\tLoss: 4.32\n","Train Epoch: 1 [75520/131528 (57%)]\tLoss: 4.32\n","Train Epoch: 1 [76800/131528 (58%)]\tLoss: 4.32\n","Train Epoch: 1 [78080/131528 (59%)]\tLoss: 4.32\n","Train Epoch: 1 [79360/131528 (60%)]\tLoss: 4.32\n","Train Epoch: 1 [80640/131528 (61%)]\tLoss: 4.31\n","Train Epoch: 1 [81920/131528 (62%)]\tLoss: 4.31\n","Train Epoch: 1 [83200/131528 (63%)]\tLoss: 4.31\n","Train Epoch: 1 [84480/131528 (64%)]\tLoss: 4.30\n","Train Epoch: 1 [85760/131528 (65%)]\tLoss: 4.30\n","Train Epoch: 1 [87040/131528 (66%)]\tLoss: 4.29\n","Train Epoch: 1 [88320/131528 (67%)]\tLoss: 4.29\n","Train Epoch: 1 [89600/131528 (68%)]\tLoss: 4.29\n","Train Epoch: 1 [90880/131528 (69%)]\tLoss: 4.29\n","Train Epoch: 1 [92160/131528 (70%)]\tLoss: 4.29\n","Train Epoch: 1 [93440/131528 (71%)]\tLoss: 4.28\n","Train Epoch: 1 [94720/131528 (72%)]\tLoss: 4.28\n","Train Epoch: 1 [96000/131528 (73%)]\tLoss: 4.28\n","Train Epoch: 1 [97280/131528 (74%)]\tLoss: 4.28\n","Train Epoch: 1 [98560/131528 (75%)]\tLoss: 4.27\n","Train Epoch: 1 [99840/131528 (76%)]\tLoss: 4.27\n","Train Epoch: 1 [101120/131528 (77%)]\tLoss: 4.27\n","Train Epoch: 1 [102400/131528 (78%)]\tLoss: 4.27\n","Train Epoch: 1 [103680/131528 (79%)]\tLoss: 4.26\n","Train Epoch: 1 [104960/131528 (80%)]\tLoss: 4.26\n","Train Epoch: 1 [106240/131528 (81%)]\tLoss: 4.26\n","Train Epoch: 1 [107520/131528 (82%)]\tLoss: 4.25\n","Train Epoch: 1 [108800/131528 (83%)]\tLoss: 4.25\n","Train Epoch: 1 [110080/131528 (84%)]\tLoss: 4.25\n","Train Epoch: 1 [111360/131528 (85%)]\tLoss: 4.25\n","Train Epoch: 1 [112640/131528 (86%)]\tLoss: 4.25\n","Train Epoch: 1 [113920/131528 (87%)]\tLoss: 4.25\n","Train Epoch: 1 [115200/131528 (88%)]\tLoss: 4.24\n","Train Epoch: 1 [116480/131528 (89%)]\tLoss: 4.24\n","Train Epoch: 1 [117760/131528 (90%)]\tLoss: 4.24\n","Train Epoch: 1 [119040/131528 (91%)]\tLoss: 4.24\n","Train Epoch: 1 [120320/131528 (91%)]\tLoss: 4.24\n","Train Epoch: 1 [121600/131528 (92%)]\tLoss: 4.24\n","Train Epoch: 1 [122880/131528 (93%)]\tLoss: 4.24\n","Train Epoch: 1 [124160/131528 (94%)]\tLoss: 4.23\n","Train Epoch: 1 [125440/131528 (95%)]\tLoss: 4.23\n","Train Epoch: 1 [126720/131528 (96%)]\tLoss: 4.23\n","Train Epoch: 1 [128000/131528 (97%)]\tLoss: 4.23\n","Train Epoch: 1 [129280/131528 (98%)]\tLoss: 4.23\n","Train Epoch: 1 [130560/131528 (99%)]\tLoss: 4.23\n","\n","Evaluating trained model ...\n","\n","Test set: Accuracy: 1506/2286 (66%)\n","\n","Train Epoch: 2 [1280/131528 (1%)]\tLoss: 3.92\n","Train Epoch: 2 [2560/131528 (2%)]\tLoss: 4.01\n","Train Epoch: 2 [3840/131528 (3%)]\tLoss: 4.00\n","Train Epoch: 2 [5120/131528 (4%)]\tLoss: 4.03\n","Train Epoch: 2 [6400/131528 (5%)]\tLoss: 4.01\n","Train Epoch: 2 [7680/131528 (6%)]\tLoss: 3.98\n","Train Epoch: 2 [8960/131528 (7%)]\tLoss: 3.96\n","Train Epoch: 2 [10240/131528 (8%)]\tLoss: 3.97\n","Train Epoch: 2 [11520/131528 (9%)]\tLoss: 3.99\n","Train Epoch: 2 [12800/131528 (10%)]\tLoss: 4.00\n","Train Epoch: 2 [14080/131528 (11%)]\tLoss: 4.00\n","Train Epoch: 2 [15360/131528 (12%)]\tLoss: 4.00\n","Train Epoch: 2 [16640/131528 (13%)]\tLoss: 4.00\n","Train Epoch: 2 [17920/131528 (14%)]\tLoss: 4.00\n","Train Epoch: 2 [19200/131528 (15%)]\tLoss: 3.99\n","Train Epoch: 2 [20480/131528 (16%)]\tLoss: 4.00\n","Train Epoch: 2 [21760/131528 (17%)]\tLoss: 4.01\n","Train Epoch: 2 [23040/131528 (18%)]\tLoss: 4.00\n","Train Epoch: 2 [24320/131528 (18%)]\tLoss: 4.00\n","Train Epoch: 2 [25600/131528 (19%)]\tLoss: 4.00\n","Train Epoch: 2 [26880/131528 (20%)]\tLoss: 3.99\n","Train Epoch: 2 [28160/131528 (21%)]\tLoss: 4.00\n","Train Epoch: 2 [29440/131528 (22%)]\tLoss: 3.99\n","Train Epoch: 2 [30720/131528 (23%)]\tLoss: 3.99\n","Train Epoch: 2 [32000/131528 (24%)]\tLoss: 4.00\n","Train Epoch: 2 [33280/131528 (25%)]\tLoss: 3.99\n","Train Epoch: 2 [34560/131528 (26%)]\tLoss: 3.99\n","Train Epoch: 2 [35840/131528 (27%)]\tLoss: 3.99\n","Train Epoch: 2 [37120/131528 (28%)]\tLoss: 3.99\n","Train Epoch: 2 [38400/131528 (29%)]\tLoss: 3.99\n","Train Epoch: 2 [39680/131528 (30%)]\tLoss: 3.99\n","Train Epoch: 2 [40960/131528 (31%)]\tLoss: 3.99\n","Train Epoch: 2 [42240/131528 (32%)]\tLoss: 3.99\n","Train Epoch: 2 [43520/131528 (33%)]\tLoss: 3.99\n","Train Epoch: 2 [44800/131528 (34%)]\tLoss: 3.98\n","Train Epoch: 2 [46080/131528 (35%)]\tLoss: 3.98\n","Train Epoch: 2 [47360/131528 (36%)]\tLoss: 3.98\n","Train Epoch: 2 [48640/131528 (37%)]\tLoss: 3.97\n","Train Epoch: 2 [49920/131528 (38%)]\tLoss: 3.97\n","Train Epoch: 2 [51200/131528 (39%)]\tLoss: 3.97\n","Train Epoch: 2 [52480/131528 (40%)]\tLoss: 3.97\n","Train Epoch: 2 [53760/131528 (41%)]\tLoss: 3.97\n","Train Epoch: 2 [55040/131528 (42%)]\tLoss: 3.97\n","Train Epoch: 2 [56320/131528 (43%)]\tLoss: 3.97\n","Train Epoch: 2 [57600/131528 (44%)]\tLoss: 3.97\n","Train Epoch: 2 [58880/131528 (45%)]\tLoss: 3.97\n","Train Epoch: 2 [60160/131528 (46%)]\tLoss: 3.96\n","Train Epoch: 2 [61440/131528 (47%)]\tLoss: 3.97\n","Train Epoch: 2 [62720/131528 (48%)]\tLoss: 3.97\n","Train Epoch: 2 [64000/131528 (49%)]\tLoss: 3.97\n","Train Epoch: 2 [65280/131528 (50%)]\tLoss: 3.97\n","Train Epoch: 2 [66560/131528 (51%)]\tLoss: 3.97\n","Train Epoch: 2 [67840/131528 (52%)]\tLoss: 3.97\n","Train Epoch: 2 [69120/131528 (53%)]\tLoss: 3.97\n","Train Epoch: 2 [70400/131528 (54%)]\tLoss: 3.97\n","Train Epoch: 2 [71680/131528 (54%)]\tLoss: 3.97\n","Train Epoch: 2 [72960/131528 (55%)]\tLoss: 3.97\n","Train Epoch: 2 [74240/131528 (56%)]\tLoss: 3.97\n","Train Epoch: 2 [75520/131528 (57%)]\tLoss: 3.96\n","Train Epoch: 2 [76800/131528 (58%)]\tLoss: 3.96\n","Train Epoch: 2 [78080/131528 (59%)]\tLoss: 3.96\n","Train Epoch: 2 [79360/131528 (60%)]\tLoss: 3.96\n","Train Epoch: 2 [80640/131528 (61%)]\tLoss: 3.96\n","Train Epoch: 2 [81920/131528 (62%)]\tLoss: 3.96\n","Train Epoch: 2 [83200/131528 (63%)]\tLoss: 3.96\n","Train Epoch: 2 [84480/131528 (64%)]\tLoss: 3.96\n","Train Epoch: 2 [85760/131528 (65%)]\tLoss: 3.96\n","Train Epoch: 2 [87040/131528 (66%)]\tLoss: 3.96\n","Train Epoch: 2 [88320/131528 (67%)]\tLoss: 3.97\n","Train Epoch: 2 [89600/131528 (68%)]\tLoss: 3.97\n","Train Epoch: 2 [90880/131528 (69%)]\tLoss: 3.96\n","Train Epoch: 2 [92160/131528 (70%)]\tLoss: 3.97\n","Train Epoch: 2 [93440/131528 (71%)]\tLoss: 3.97\n","Train Epoch: 2 [94720/131528 (72%)]\tLoss: 3.97\n","Train Epoch: 2 [96000/131528 (73%)]\tLoss: 3.96\n","Train Epoch: 2 [97280/131528 (74%)]\tLoss: 3.96\n","Train Epoch: 2 [98560/131528 (75%)]\tLoss: 3.96\n","Train Epoch: 2 [99840/131528 (76%)]\tLoss: 3.96\n","Train Epoch: 2 [101120/131528 (77%)]\tLoss: 3.96\n","Train Epoch: 2 [102400/131528 (78%)]\tLoss: 3.96\n","Train Epoch: 2 [103680/131528 (79%)]\tLoss: 3.96\n","Train Epoch: 2 [104960/131528 (80%)]\tLoss: 3.96\n","Train Epoch: 2 [106240/131528 (81%)]\tLoss: 3.96\n","Train Epoch: 2 [107520/131528 (82%)]\tLoss: 3.96\n","Train Epoch: 2 [108800/131528 (83%)]\tLoss: 3.96\n","Train Epoch: 2 [110080/131528 (84%)]\tLoss: 3.96\n","Train Epoch: 2 [111360/131528 (85%)]\tLoss: 3.96\n","Train Epoch: 2 [112640/131528 (86%)]\tLoss: 3.96\n","Train Epoch: 2 [113920/131528 (87%)]\tLoss: 3.96\n","Train Epoch: 2 [115200/131528 (88%)]\tLoss: 3.96\n","Train Epoch: 2 [116480/131528 (89%)]\tLoss: 3.96\n","Train Epoch: 2 [117760/131528 (90%)]\tLoss: 3.96\n","Train Epoch: 2 [119040/131528 (91%)]\tLoss: 3.96\n","Train Epoch: 2 [120320/131528 (91%)]\tLoss: 3.95\n","Train Epoch: 2 [121600/131528 (92%)]\tLoss: 3.95\n","Train Epoch: 2 [122880/131528 (93%)]\tLoss: 3.95\n","Train Epoch: 2 [124160/131528 (94%)]\tLoss: 3.95\n","Train Epoch: 2 [125440/131528 (95%)]\tLoss: 3.95\n","Train Epoch: 2 [126720/131528 (96%)]\tLoss: 3.95\n","Train Epoch: 2 [128000/131528 (97%)]\tLoss: 3.95\n","Train Epoch: 2 [129280/131528 (98%)]\tLoss: 3.95\n","Train Epoch: 2 [130560/131528 (99%)]\tLoss: 3.95\n","\n","Evaluating trained model ...\n","\n","Test set: Accuracy: 1554/2286 (68%)\n","\n","Train Epoch: 3 [1280/131528 (1%)]\tLoss: 3.74\n","Train Epoch: 3 [2560/131528 (2%)]\tLoss: 3.79\n","Train Epoch: 3 [3840/131528 (3%)]\tLoss: 3.79\n","Train Epoch: 3 [5120/131528 (4%)]\tLoss: 3.81\n","Train Epoch: 3 [6400/131528 (5%)]\tLoss: 3.77\n","Train Epoch: 3 [7680/131528 (6%)]\tLoss: 3.79\n","Train Epoch: 3 [8960/131528 (7%)]\tLoss: 3.82\n","Train Epoch: 3 [10240/131528 (8%)]\tLoss: 3.84\n","Train Epoch: 3 [11520/131528 (9%)]\tLoss: 3.83\n","Train Epoch: 3 [12800/131528 (10%)]\tLoss: 3.84\n","Train Epoch: 3 [14080/131528 (11%)]\tLoss: 3.82\n","Train Epoch: 3 [15360/131528 (12%)]\tLoss: 3.83\n","Train Epoch: 3 [16640/131528 (13%)]\tLoss: 3.83\n","Train Epoch: 3 [17920/131528 (14%)]\tLoss: 3.83\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4IOXrQrZZg0E","colab_type":"code","outputId":"15b6f613-e1c1-4b1a-90c2-01fbe321b549","executionInfo":{"status":"ok","timestamp":1572128315561,"user_tz":-780,"elapsed":2552119,"user":{"displayName":"Dylan Webb","photoUrl":"","userId":"17339420517359398809"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run.py train discriminator"],"execution_count":4,"outputs":[{"output_type":"stream","text":["6000_checkpoint.tar\n","39099 pairs trimmed, 87553 remain\n","==============63==============\n","5149 pairs trimmed, 2807 remain\n","==============0==============\n","evaluating trained model ...\n","\n","Test set accuracy: correctly guess human: 37/64 (58%) ; correctly guess bot: 27/64 (42%)\n","\n","Test set accuracy: correctly guess human: 32/64 (50%) ; correctly guess bot: 32/64 (50%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 29/64 (45%)\n","\n","Test set accuracy: correctly guess human: 33/64 (52%) ; correctly guess bot: 31/64 (48%)\n","\n","Test set accuracy: correctly guess human: 31/64 (48%) ; correctly guess bot: 33/64 (52%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 29/64 (45%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 29/64 (45%)\n","\n","Test set accuracy: correctly guess human: 26/64 (41%) ; correctly guess bot: 38/64 (59%)\n","\n","Test set accuracy: correctly guess human: 37/64 (58%) ; correctly guess bot: 27/64 (42%)\n","\n","Test set accuracy: correctly guess human: 29/64 (45%) ; correctly guess bot: 35/64 (55%)\n","\n","Test set accuracy: correctly guess human: 33/64 (52%) ; correctly guess bot: 31/64 (48%)\n","\n","Test set accuracy: correctly guess human: 39/64 (61%) ; correctly guess bot: 26/64 (41%)\n","\n","Test set accuracy: correctly guess human: 39/64 (61%) ; correctly guess bot: 25/64 (39%)\n","\n","Test set accuracy: correctly guess human: 29/64 (45%) ; correctly guess bot: 36/64 (56%)\n","\n","Test set accuracy: correctly guess human: 29/64 (45%) ; correctly guess bot: 35/64 (55%)\n","\n","Test set accuracy: correctly guess human: 20/64 (31%) ; correctly guess bot: 44/64 (69%)\n","\n","Test set accuracy: correctly guess human: 27/64 (42%) ; correctly guess bot: 37/64 (58%)\n","\n","Test set accuracy: correctly guess human: 30/64 (47%) ; correctly guess bot: 34/64 (53%)\n","\n","Test set accuracy: correctly guess human: 32/64 (50%) ; correctly guess bot: 32/64 (50%)\n","\n","Test set accuracy: correctly guess human: 30/64 (47%) ; correctly guess bot: 34/64 (53%)\n","\n","Test set accuracy: correctly guess human: 39/64 (61%) ; correctly guess bot: 25/64 (39%)\n","\n","Test set accuracy: correctly guess human: 30/64 (47%) ; correctly guess bot: 34/64 (53%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 30/64 (47%)\n","\n","Test set accuracy: correctly guess human: 32/64 (50%) ; correctly guess bot: 32/64 (50%)\n","\n","Test set accuracy: correctly guess human: 26/64 (41%) ; correctly guess bot: 38/64 (59%)\n","\n","Test set accuracy: correctly guess human: 31/64 (48%) ; correctly guess bot: 33/64 (52%)\n","\n","Test set accuracy: correctly guess human: 28/64 (44%) ; correctly guess bot: 36/64 (56%)\n","\n","Test set accuracy: correctly guess human: 29/64 (45%) ; correctly guess bot: 35/64 (55%)\n","\n","Test set accuracy: correctly guess human: 37/64 (58%) ; correctly guess bot: 27/64 (42%)\n","\n","Test set accuracy: correctly guess human: 25/64 (39%) ; correctly guess bot: 40/64 (62%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 29/64 (45%)\n","\n","Test set accuracy: correctly guess human: 35/64 (55%) ; correctly guess bot: 29/64 (45%)\n","\n","Test set accuracy: correctly guess human: 30/64 (47%) ; correctly guess bot: 34/64 (53%)\n","\n","Test set accuracy: correctly guess human: 31/64 (48%) ; correctly guess bot: 33/64 (52%)\n","\n","Test set accuracy: correctly guess human: 36/64 (56%) ; correctly guess bot: 28/64 (44%)\n","\n","Test set accuracy: correctly guess human: 38/64 (59%) ; correctly guess bot: 26/64 (41%)\n","\n","Test set accuracy: correctly guess human: 34/64 (53%) ; correctly guess bot: 30/64 (47%)\n","\n","Test set accuracy: correctly guess human: 36/64 (56%) ; correctly guess bot: 28/64 (44%)\n","\n","Test set accuracy: correctly guess human: 27/64 (42%) ; correctly guess bot: 37/64 (58%)\n","\n","Test set accuracy: correctly guess human: 34/64 (53%) ; correctly guess bot: 30/64 (47%)\n","\n","Test set accuracy: correctly guess human: 33/64 (52%) ; correctly guess bot: 31/64 (48%)\n","\n","Test set accuracy: correctly guess human: 38/64 (59%) ; correctly guess bot: 26/64 (41%)\n","\n","Test set accuracy: correctly guess human: 37/64 (58%) ; correctly guess bot: 28/64 (44%)\n","\n","Test set accuracy: correctly guess human: 28/64 (44%) ; correctly guess bot: 27/64 (42%)\n","Train Epoch: 1 [64/87490 (0%)]\tLoss: 7.29\n","Train Epoch: 1 [128/87490 (0%)]\tLoss: 7.18\n","Train Epoch: 1 [192/87490 (0%)]\tLoss: 7.14\n","Train Epoch: 1 [256/87490 (0%)]\tLoss: 7.11\n","Train Epoch: 1 [320/87490 (0%)]\tLoss: 7.08\n","Train Epoch: 1 [384/87490 (0%)]\tLoss: 7.07\n","Train Epoch: 1 [448/87490 (1%)]\tLoss: 7.05\n","Train Epoch: 1 [512/87490 (1%)]\tLoss: 7.04\n","Train Epoch: 1 [576/87490 (1%)]\tLoss: 7.04\n","Train Epoch: 1 [640/87490 (1%)]\tLoss: 7.03\n","Train Epoch: 1 [704/87490 (1%)]\tLoss: 7.02\n","Train Epoch: 1 [768/87490 (1%)]\tLoss: 7.02\n","Train Epoch: 1 [832/87490 (1%)]\tLoss: 7.01\n","Train Epoch: 1 [896/87490 (1%)]\tLoss: 7.01\n","Train Epoch: 1 [960/87490 (1%)]\tLoss: 7.00\n","Train Epoch: 1 [1024/87490 (1%)]\tLoss: 7.00\n","Train Epoch: 1 [1088/87490 (1%)]\tLoss: 7.00\n","Train Epoch: 1 [1152/87490 (1%)]\tLoss: 7.00\n","Train Epoch: 1 [1216/87490 (1%)]\tLoss: 6.99\n","Train Epoch: 1 [1280/87490 (1%)]\tLoss: 6.99\n","Train Epoch: 1 [1344/87490 (2%)]\tLoss: 6.99\n","Train Epoch: 1 [1408/87490 (2%)]\tLoss: 6.99\n","Train Epoch: 1 [1472/87490 (2%)]\tLoss: 6.99\n","Train Epoch: 1 [1536/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1600/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1664/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1728/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1792/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1856/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1920/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [1984/87490 (2%)]\tLoss: 6.98\n","Train Epoch: 1 [2048/87490 (2%)]\tLoss: 6.97\n","Train Epoch: 1 [2112/87490 (2%)]\tLoss: 6.97\n","Train Epoch: 1 [2176/87490 (2%)]\tLoss: 6.97\n","Train Epoch: 1 [2240/87490 (3%)]\tLoss: 6.97\n","Train Epoch: 1 [2304/87490 (3%)]\tLoss: 6.97\n","Train Epoch: 1 [2368/87490 (3%)]\tLoss: 6.97\n","Train Epoch: 1 [2432/87490 (3%)]\tLoss: 6.97\n","Train Epoch: 1 [2496/87490 (3%)]\tLoss: 6.97\n","Train Epoch: 1 [2560/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2624/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2688/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2752/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2816/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2880/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [2944/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [3008/87490 (3%)]\tLoss: 6.96\n","Train Epoch: 1 [3072/87490 (4%)]\tLoss: 6.96\n","Train Epoch: 1 [3136/87490 (4%)]\tLoss: 6.96\n","Train Epoch: 1 [3200/87490 (4%)]\tLoss: 6.96\n","Train Epoch: 1 [3264/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3328/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3392/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3456/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3520/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3584/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3648/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3712/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3776/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3840/87490 (4%)]\tLoss: 6.95\n","Train Epoch: 1 [3904/87490 (4%)]\tLoss: 6.94\n","Train Epoch: 1 [3968/87490 (5%)]\tLoss: 6.94\n","Train Epoch: 1 [4032/87490 (5%)]\tLoss: 6.94\n","Train Epoch: 1 [4096/87490 (5%)]\tLoss: 6.93\n","Train Epoch: 1 [4160/87490 (5%)]\tLoss: 6.93\n","Train Epoch: 1 [4224/87490 (5%)]\tLoss: 6.93\n","Train Epoch: 1 [4288/87490 (5%)]\tLoss: 6.93\n","Train Epoch: 1 [4352/87490 (5%)]\tLoss: 6.92\n","Train Epoch: 1 [4416/87490 (5%)]\tLoss: 6.92\n","Train Epoch: 1 [4480/87490 (5%)]\tLoss: 6.91\n","Train Epoch: 1 [4544/87490 (5%)]\tLoss: 6.91\n","Train Epoch: 1 [4608/87490 (5%)]\tLoss: 6.90\n","Train Epoch: 1 [4672/87490 (5%)]\tLoss: 6.89\n","Train Epoch: 1 [4736/87490 (5%)]\tLoss: 6.88\n","Train Epoch: 1 [4800/87490 (5%)]\tLoss: 6.87\n","Train Epoch: 1 [4864/87490 (6%)]\tLoss: 6.85\n","Train Epoch: 1 [4928/87490 (6%)]\tLoss: 6.84\n","Train Epoch: 1 [4992/87490 (6%)]\tLoss: 6.82\n","Train Epoch: 1 [5056/87490 (6%)]\tLoss: 6.80\n","Train Epoch: 1 [5120/87490 (6%)]\tLoss: 6.78\n","Train Epoch: 1 [5184/87490 (6%)]\tLoss: 6.76\n","Train Epoch: 1 [5248/87490 (6%)]\tLoss: 6.74\n","Train Epoch: 1 [5312/87490 (6%)]\tLoss: 6.72\n","Train Epoch: 1 [5376/87490 (6%)]\tLoss: 6.69\n","Train Epoch: 1 [5440/87490 (6%)]\tLoss: 6.66\n","Train Epoch: 1 [5504/87490 (6%)]\tLoss: 6.63\n","Train Epoch: 1 [5568/87490 (6%)]\tLoss: 6.60\n","Train Epoch: 1 [5632/87490 (6%)]\tLoss: 6.57\n","Train Epoch: 1 [5696/87490 (7%)]\tLoss: 6.54\n","Train Epoch: 1 [5760/87490 (7%)]\tLoss: 6.50\n","Train Epoch: 1 [5824/87490 (7%)]\tLoss: 6.46\n","Train Epoch: 1 [5888/87490 (7%)]\tLoss: 6.43\n","Train Epoch: 1 [5952/87490 (7%)]\tLoss: 6.39\n","Train Epoch: 1 [6016/87490 (7%)]\tLoss: 6.35\n","Train Epoch: 1 [6080/87490 (7%)]\tLoss: 6.32\n","Train Epoch: 1 [6144/87490 (7%)]\tLoss: 6.28\n","Train Epoch: 1 [6208/87490 (7%)]\tLoss: 6.23\n","Train Epoch: 1 [6272/87490 (7%)]\tLoss: 6.19\n","Train Epoch: 1 [6336/87490 (7%)]\tLoss: 6.15\n","Train Epoch: 1 [6400/87490 (7%)]\tLoss: 6.11\n","Train Epoch: 1 [6464/87490 (7%)]\tLoss: 6.07\n","Train Epoch: 1 [6528/87490 (7%)]\tLoss: 6.03\n","Train Epoch: 1 [6592/87490 (8%)]\tLoss: 5.99\n","Train Epoch: 1 [6656/87490 (8%)]\tLoss: 5.94\n","Train Epoch: 1 [6720/87490 (8%)]\tLoss: 5.90\n","Train Epoch: 1 [6784/87490 (8%)]\tLoss: 5.86\n","Train Epoch: 1 [6848/87490 (8%)]\tLoss: 5.81\n","Train Epoch: 1 [6912/87490 (8%)]\tLoss: 5.77\n","Train Epoch: 1 [6976/87490 (8%)]\tLoss: 5.73\n","Train Epoch: 1 [7040/87490 (8%)]\tLoss: 5.69\n","Train Epoch: 1 [7104/87490 (8%)]\tLoss: 5.65\n","Train Epoch: 1 [7168/87490 (8%)]\tLoss: 5.60\n","Train Epoch: 1 [7232/87490 (8%)]\tLoss: 5.56\n","Train Epoch: 1 [7296/87490 (8%)]\tLoss: 5.52\n","Train Epoch: 1 [7360/87490 (8%)]\tLoss: 5.49\n","Train Epoch: 1 [7424/87490 (8%)]\tLoss: 5.45\n","Train Epoch: 1 [7488/87490 (9%)]\tLoss: 5.41\n","Train Epoch: 1 [7552/87490 (9%)]\tLoss: 5.37\n","Train Epoch: 1 [7616/87490 (9%)]\tLoss: 5.34\n","Train Epoch: 1 [7680/87490 (9%)]\tLoss: 5.30\n","Train Epoch: 1 [7744/87490 (9%)]\tLoss: 5.27\n","Train Epoch: 1 [7808/87490 (9%)]\tLoss: 5.23\n","Train Epoch: 1 [7872/87490 (9%)]\tLoss: 5.20\n","Train Epoch: 1 [7936/87490 (9%)]\tLoss: 5.16\n","Train Epoch: 1 [8000/87490 (9%)]\tLoss: 5.12\n","Train Epoch: 1 [8064/87490 (9%)]\tLoss: 5.10\n","Train Epoch: 1 [8128/87490 (9%)]\tLoss: 5.06\n","Train Epoch: 1 [8192/87490 (9%)]\tLoss: 5.03\n","Train Epoch: 1 [8256/87490 (9%)]\tLoss: 5.00\n","Train Epoch: 1 [8320/87490 (10%)]\tLoss: 4.97\n","Train Epoch: 1 [8384/87490 (10%)]\tLoss: 4.94\n","Train Epoch: 1 [8448/87490 (10%)]\tLoss: 4.90\n","Train Epoch: 1 [8512/87490 (10%)]\tLoss: 4.88\n","Train Epoch: 1 [8576/87490 (10%)]\tLoss: 4.85\n","Train Epoch: 1 [8640/87490 (10%)]\tLoss: 4.82\n","Train Epoch: 1 [8704/87490 (10%)]\tLoss: 4.79\n","Train Epoch: 1 [8768/87490 (10%)]\tLoss: 4.76\n","Train Epoch: 1 [8832/87490 (10%)]\tLoss: 4.73\n","Train Epoch: 1 [8896/87490 (10%)]\tLoss: 4.71\n","Train Epoch: 1 [8960/87490 (10%)]\tLoss: 4.68\n","Train Epoch: 1 [9024/87490 (10%)]\tLoss: 4.65\n","Train Epoch: 1 [9088/87490 (10%)]\tLoss: 4.62\n","Train Epoch: 1 [9152/87490 (10%)]\tLoss: 4.59\n","Train Epoch: 1 [9216/87490 (11%)]\tLoss: 4.57\n","Train Epoch: 1 [9280/87490 (11%)]\tLoss: 4.54\n","Train Epoch: 1 [9344/87490 (11%)]\tLoss: 4.53\n","Train Epoch: 1 [9408/87490 (11%)]\tLoss: 4.50\n","Train Epoch: 1 [9472/87490 (11%)]\tLoss: 4.48\n","Train Epoch: 1 [9536/87490 (11%)]\tLoss: 4.46\n","Train Epoch: 1 [9600/87490 (11%)]\tLoss: 4.43\n","Train Epoch: 1 [9664/87490 (11%)]\tLoss: 4.41\n","Train Epoch: 1 [9728/87490 (11%)]\tLoss: 4.39\n","Train Epoch: 1 [9792/87490 (11%)]\tLoss: 4.36\n","Train Epoch: 1 [9856/87490 (11%)]\tLoss: 4.34\n","Train Epoch: 1 [9920/87490 (11%)]\tLoss: 4.32\n","Train Epoch: 1 [9984/87490 (11%)]\tLoss: 4.29\n","Train Epoch: 1 [10048/87490 (11%)]\tLoss: 4.27\n","Train Epoch: 1 [10112/87490 (12%)]\tLoss: 4.25\n","Train Epoch: 1 [10176/87490 (12%)]\tLoss: 4.23\n","Train Epoch: 1 [10240/87490 (12%)]\tLoss: 4.20\n","Train Epoch: 1 [10304/87490 (12%)]\tLoss: 4.18\n","Train Epoch: 1 [10368/87490 (12%)]\tLoss: 4.16\n","Train Epoch: 1 [10432/87490 (12%)]\tLoss: 4.14\n","Train Epoch: 1 [10496/87490 (12%)]\tLoss: 4.12\n","Train Epoch: 1 [10560/87490 (12%)]\tLoss: 4.09\n","Train Epoch: 1 [10624/87490 (12%)]\tLoss: 4.07\n","Train Epoch: 1 [10688/87490 (12%)]\tLoss: 4.05\n","Train Epoch: 1 [10752/87490 (12%)]\tLoss: 4.03\n","Train Epoch: 1 [10816/87490 (12%)]\tLoss: 4.01\n","Train Epoch: 1 [10880/87490 (12%)]\tLoss: 3.99\n","Train Epoch: 1 [10944/87490 (13%)]\tLoss: 3.98\n","Train Epoch: 1 [11008/87490 (13%)]\tLoss: 3.96\n","Train Epoch: 1 [11072/87490 (13%)]\tLoss: 3.94\n","Train Epoch: 1 [11136/87490 (13%)]\tLoss: 3.92\n","Train Epoch: 1 [11200/87490 (13%)]\tLoss: 3.90\n","Train Epoch: 1 [11264/87490 (13%)]\tLoss: 3.88\n","Train Epoch: 1 [11328/87490 (13%)]\tLoss: 3.86\n","Train Epoch: 1 [11392/87490 (13%)]\tLoss: 3.84\n","Train Epoch: 1 [11456/87490 (13%)]\tLoss: 3.83\n","Train Epoch: 1 [11520/87490 (13%)]\tLoss: 3.81\n","Train Epoch: 1 [11584/87490 (13%)]\tLoss: 3.79\n","Train Epoch: 1 [11648/87490 (13%)]\tLoss: 3.77\n","Train Epoch: 1 [11712/87490 (13%)]\tLoss: 3.75\n","Train Epoch: 1 [11776/87490 (13%)]\tLoss: 3.74\n","Train Epoch: 1 [11840/87490 (14%)]\tLoss: 3.72\n","Train Epoch: 1 [11904/87490 (14%)]\tLoss: 3.70\n","Train Epoch: 1 [11968/87490 (14%)]\tLoss: 3.69\n","Train Epoch: 1 [12032/87490 (14%)]\tLoss: 3.67\n","Train Epoch: 1 [12096/87490 (14%)]\tLoss: 3.66\n","Train Epoch: 1 [12160/87490 (14%)]\tLoss: 3.64\n","Train Epoch: 1 [12224/87490 (14%)]\tLoss: 3.62\n","Train Epoch: 1 [12288/87490 (14%)]\tLoss: 3.61\n","Train Epoch: 1 [12352/87490 (14%)]\tLoss: 3.59\n","Train Epoch: 1 [12416/87490 (14%)]\tLoss: 3.58\n","Train Epoch: 1 [12480/87490 (14%)]\tLoss: 3.56\n","Train Epoch: 1 [12544/87490 (14%)]\tLoss: 3.55\n","Train Epoch: 1 [12608/87490 (14%)]\tLoss: 3.53\n","Train Epoch: 1 [12672/87490 (14%)]\tLoss: 3.52\n","Train Epoch: 1 [12736/87490 (15%)]\tLoss: 3.50\n","Train Epoch: 1 [12800/87490 (15%)]\tLoss: 3.49\n","Train Epoch: 1 [12864/87490 (15%)]\tLoss: 3.47\n","Train Epoch: 1 [12928/87490 (15%)]\tLoss: 3.46\n","Train Epoch: 1 [12992/87490 (15%)]\tLoss: 3.44\n","Train Epoch: 1 [13056/87490 (15%)]\tLoss: 3.43\n","Train Epoch: 1 [13120/87490 (15%)]\tLoss: 3.42\n","Train Epoch: 1 [13184/87490 (15%)]\tLoss: 3.40\n","Train Epoch: 1 [13248/87490 (15%)]\tLoss: 3.39\n","Train Epoch: 1 [13312/87490 (15%)]\tLoss: 3.38\n","Train Epoch: 1 [13376/87490 (15%)]\tLoss: 3.36\n","Train Epoch: 1 [13440/87490 (15%)]\tLoss: 3.35\n","Train Epoch: 1 [13504/87490 (15%)]\tLoss: 3.34\n","Train Epoch: 1 [13568/87490 (16%)]\tLoss: 3.33\n","Train Epoch: 1 [13632/87490 (16%)]\tLoss: 3.31\n","Train Epoch: 1 [13696/87490 (16%)]\tLoss: 3.30\n","Train Epoch: 1 [13760/87490 (16%)]\tLoss: 3.29\n","Train Epoch: 1 [13824/87490 (16%)]\tLoss: 3.27\n","Train Epoch: 1 [13888/87490 (16%)]\tLoss: 3.26\n","Train Epoch: 1 [13952/87490 (16%)]\tLoss: 3.25\n","Train Epoch: 1 [14016/87490 (16%)]\tLoss: 3.24\n","Train Epoch: 1 [14080/87490 (16%)]\tLoss: 3.23\n","Train Epoch: 1 [14144/87490 (16%)]\tLoss: 3.22\n","Train Epoch: 1 [14208/87490 (16%)]\tLoss: 3.20\n","Train Epoch: 1 [14272/87490 (16%)]\tLoss: 3.19\n","Train Epoch: 1 [14336/87490 (16%)]\tLoss: 3.18\n","Train Epoch: 1 [14400/87490 (16%)]\tLoss: 3.17\n","Train Epoch: 1 [14464/87490 (17%)]\tLoss: 3.16\n","Train Epoch: 1 [14528/87490 (17%)]\tLoss: 3.15\n","Train Epoch: 1 [14592/87490 (17%)]\tLoss: 3.14\n","Train Epoch: 1 [14656/87490 (17%)]\tLoss: 3.13\n","Train Epoch: 1 [14720/87490 (17%)]\tLoss: 3.12\n","Train Epoch: 1 [14784/87490 (17%)]\tLoss: 3.10\n","Train Epoch: 1 [14848/87490 (17%)]\tLoss: 3.09\n","Train Epoch: 1 [14912/87490 (17%)]\tLoss: 3.08\n","Train Epoch: 1 [14976/87490 (17%)]\tLoss: 3.07\n","Train Epoch: 1 [15040/87490 (17%)]\tLoss: 3.06\n","Train Epoch: 1 [15104/87490 (17%)]\tLoss: 3.05\n","Train Epoch: 1 [15168/87490 (17%)]\tLoss: 3.04\n","Train Epoch: 1 [15232/87490 (17%)]\tLoss: 3.03\n","Train Epoch: 1 [15296/87490 (17%)]\tLoss: 3.02\n","Train Epoch: 1 [15360/87490 (18%)]\tLoss: 3.01\n","Train Epoch: 1 [15424/87490 (18%)]\tLoss: 3.00\n","Train Epoch: 1 [15488/87490 (18%)]\tLoss: 2.99\n","Train Epoch: 1 [15552/87490 (18%)]\tLoss: 2.98\n","Train Epoch: 1 [15616/87490 (18%)]\tLoss: 2.97\n","Train Epoch: 1 [15680/87490 (18%)]\tLoss: 2.96\n","Train Epoch: 1 [15744/87490 (18%)]\tLoss: 2.95\n","Train Epoch: 1 [15808/87490 (18%)]\tLoss: 2.94\n","Train Epoch: 1 [15872/87490 (18%)]\tLoss: 2.93\n","Train Epoch: 1 [15936/87490 (18%)]\tLoss: 2.92\n","Train Epoch: 1 [16000/87490 (18%)]\tLoss: 2.91\n","Train Epoch: 1 [16064/87490 (18%)]\tLoss: 2.90\n","Train Epoch: 1 [16128/87490 (18%)]\tLoss: 2.89\n","Train Epoch: 1 [16192/87490 (19%)]\tLoss: 2.88\n","Train Epoch: 1 [16256/87490 (19%)]\tLoss: 2.87\n","Train Epoch: 1 [16320/87490 (19%)]\tLoss: 2.86\n","Train Epoch: 1 [16384/87490 (19%)]\tLoss: 2.85\n","Train Epoch: 1 [16448/87490 (19%)]\tLoss: 2.84\n","Train Epoch: 1 [16512/87490 (19%)]\tLoss: 2.83\n","Train Epoch: 1 [16576/87490 (19%)]\tLoss: 2.82\n","Train Epoch: 1 [16640/87490 (19%)]\tLoss: 2.81\n","Train Epoch: 1 [16704/87490 (19%)]\tLoss: 2.81\n","Train Epoch: 1 [16768/87490 (19%)]\tLoss: 2.80\n","Train Epoch: 1 [16832/87490 (19%)]\tLoss: 2.79\n","Train Epoch: 1 [16896/87490 (19%)]\tLoss: 2.78\n","Train Epoch: 1 [16960/87490 (19%)]\tLoss: 2.77\n","Train Epoch: 1 [17024/87490 (19%)]\tLoss: 2.76\n","Train Epoch: 1 [17088/87490 (20%)]\tLoss: 2.75\n","Train Epoch: 1 [17152/87490 (20%)]\tLoss: 2.75\n","Train Epoch: 1 [17216/87490 (20%)]\tLoss: 2.74\n","Train Epoch: 1 [17280/87490 (20%)]\tLoss: 2.73\n","Train Epoch: 1 [17344/87490 (20%)]\tLoss: 2.72\n","Train Epoch: 1 [17408/87490 (20%)]\tLoss: 2.72\n","Train Epoch: 1 [17472/87490 (20%)]\tLoss: 2.71\n","Train Epoch: 1 [17536/87490 (20%)]\tLoss: 2.70\n","Train Epoch: 1 [17600/87490 (20%)]\tLoss: 2.69\n","Train Epoch: 1 [17664/87490 (20%)]\tLoss: 2.68\n","Train Epoch: 1 [17728/87490 (20%)]\tLoss: 2.67\n","Train Epoch: 1 [17792/87490 (20%)]\tLoss: 2.67\n","Train Epoch: 1 [17856/87490 (20%)]\tLoss: 2.66\n","Train Epoch: 1 [17920/87490 (20%)]\tLoss: 2.65\n","Train Epoch: 1 [17984/87490 (21%)]\tLoss: 2.64\n","Train Epoch: 1 [18048/87490 (21%)]\tLoss: 2.63\n","Train Epoch: 1 [18112/87490 (21%)]\tLoss: 2.63\n","Train Epoch: 1 [18176/87490 (21%)]\tLoss: 2.62\n","Train Epoch: 1 [18240/87490 (21%)]\tLoss: 2.61\n","Train Epoch: 1 [18304/87490 (21%)]\tLoss: 2.60\n","Train Epoch: 1 [18368/87490 (21%)]\tLoss: 2.59\n","Train Epoch: 1 [18432/87490 (21%)]\tLoss: 2.59\n","Train Epoch: 1 [18496/87490 (21%)]\tLoss: 2.58\n","Train Epoch: 1 [18560/87490 (21%)]\tLoss: 2.57\n","Train Epoch: 1 [18624/87490 (21%)]\tLoss: 2.56\n","Train Epoch: 1 [18688/87490 (21%)]\tLoss: 2.55\n","Train Epoch: 1 [18752/87490 (21%)]\tLoss: 2.55\n","Train Epoch: 1 [18816/87490 (22%)]\tLoss: 2.54\n","Train Epoch: 1 [18880/87490 (22%)]\tLoss: 2.53\n","Train Epoch: 1 [18944/87490 (22%)]\tLoss: 2.52\n","Train Epoch: 1 [19008/87490 (22%)]\tLoss: 2.51\n","Train Epoch: 1 [19072/87490 (22%)]\tLoss: 2.51\n","Train Epoch: 1 [19136/87490 (22%)]\tLoss: 2.50\n","Train Epoch: 1 [19200/87490 (22%)]\tLoss: 2.49\n","Train Epoch: 1 [19264/87490 (22%)]\tLoss: 2.48\n","Train Epoch: 1 [19328/87490 (22%)]\tLoss: 2.48\n","Train Epoch: 1 [19392/87490 (22%)]\tLoss: 2.47\n","Train Epoch: 1 [19456/87490 (22%)]\tLoss: 2.46\n","Train Epoch: 1 [19520/87490 (22%)]\tLoss: 2.45\n","Train Epoch: 1 [19584/87490 (22%)]\tLoss: 2.45\n","Train Epoch: 1 [19648/87490 (22%)]\tLoss: 2.44\n","Train Epoch: 1 [19712/87490 (23%)]\tLoss: 2.43\n","Train Epoch: 1 [19776/87490 (23%)]\tLoss: 2.42\n","Train Epoch: 1 [19840/87490 (23%)]\tLoss: 2.42\n","Train Epoch: 1 [19904/87490 (23%)]\tLoss: 2.41\n","Train Epoch: 1 [19968/87490 (23%)]\tLoss: 2.40\n","Train Epoch: 1 [20032/87490 (23%)]\tLoss: 2.39\n","Train Epoch: 1 [20096/87490 (23%)]\tLoss: 2.39\n","Train Epoch: 1 [20160/87490 (23%)]\tLoss: 2.38\n","Train Epoch: 1 [20224/87490 (23%)]\tLoss: 2.37\n","Train Epoch: 1 [20288/87490 (23%)]\tLoss: 2.36\n","Train Epoch: 1 [20352/87490 (23%)]\tLoss: 2.36\n","Train Epoch: 1 [20416/87490 (23%)]\tLoss: 2.35\n","Train Epoch: 1 [20480/87490 (23%)]\tLoss: 2.34\n","Train Epoch: 1 [20544/87490 (23%)]\tLoss: 2.34\n","Train Epoch: 1 [20608/87490 (24%)]\tLoss: 2.33\n","Train Epoch: 1 [20672/87490 (24%)]\tLoss: 2.32\n","Train Epoch: 1 [20736/87490 (24%)]\tLoss: 2.31\n","Train Epoch: 1 [20800/87490 (24%)]\tLoss: 2.31\n","Train Epoch: 1 [20864/87490 (24%)]\tLoss: 2.30\n","Train Epoch: 1 [20928/87490 (24%)]\tLoss: 2.29\n","Train Epoch: 1 [20992/87490 (24%)]\tLoss: 2.29\n","Train Epoch: 1 [21056/87490 (24%)]\tLoss: 2.28\n","Train Epoch: 1 [21120/87490 (24%)]\tLoss: 2.27\n","Train Epoch: 1 [21184/87490 (24%)]\tLoss: 2.27\n","Train Epoch: 1 [21248/87490 (24%)]\tLoss: 2.26\n","Train Epoch: 1 [21312/87490 (24%)]\tLoss: 2.25\n","Train Epoch: 1 [21376/87490 (24%)]\tLoss: 2.25\n","Train Epoch: 1 [21440/87490 (25%)]\tLoss: 2.24\n","Train Epoch: 1 [21504/87490 (25%)]\tLoss: 2.23\n","Train Epoch: 1 [21568/87490 (25%)]\tLoss: 2.23\n","Train Epoch: 1 [21632/87490 (25%)]\tLoss: 2.22\n","Train Epoch: 1 [21696/87490 (25%)]\tLoss: 2.22\n","Train Epoch: 1 [21760/87490 (25%)]\tLoss: 2.21\n","Train Epoch: 1 [21824/87490 (25%)]\tLoss: 2.20\n","Train Epoch: 1 [21888/87490 (25%)]\tLoss: 2.20\n","Train Epoch: 1 [21952/87490 (25%)]\tLoss: 2.19\n","Train Epoch: 1 [22016/87490 (25%)]\tLoss: 2.18\n","Train Epoch: 1 [22080/87490 (25%)]\tLoss: 2.18\n","Train Epoch: 1 [22144/87490 (25%)]\tLoss: 2.17\n","Train Epoch: 1 [22208/87490 (25%)]\tLoss: 2.17\n","Train Epoch: 1 [22272/87490 (25%)]\tLoss: 2.16\n","Train Epoch: 1 [22336/87490 (26%)]\tLoss: 2.15\n","Train Epoch: 1 [22400/87490 (26%)]\tLoss: 2.15\n","Train Epoch: 1 [22464/87490 (26%)]\tLoss: 2.14\n","Train Epoch: 1 [22528/87490 (26%)]\tLoss: 2.14\n","Train Epoch: 1 [22592/87490 (26%)]\tLoss: 2.13\n","Train Epoch: 1 [22656/87490 (26%)]\tLoss: 2.13\n","Train Epoch: 1 [22720/87490 (26%)]\tLoss: 2.12\n","Train Epoch: 1 [22784/87490 (26%)]\tLoss: 2.11\n","Train Epoch: 1 [22848/87490 (26%)]\tLoss: 2.11\n","Train Epoch: 1 [22912/87490 (26%)]\tLoss: 2.10\n","Train Epoch: 1 [22976/87490 (26%)]\tLoss: 2.10\n","Train Epoch: 1 [23040/87490 (26%)]\tLoss: 2.09\n","Train Epoch: 1 [23104/87490 (26%)]\tLoss: 2.09\n","Train Epoch: 1 [23168/87490 (26%)]\tLoss: 2.08\n","Train Epoch: 1 [23232/87490 (27%)]\tLoss: 2.08\n","Train Epoch: 1 [23296/87490 (27%)]\tLoss: 2.07\n","Train Epoch: 1 [23360/87490 (27%)]\tLoss: 2.06\n","Train Epoch: 1 [23424/87490 (27%)]\tLoss: 2.06\n","Train Epoch: 1 [23488/87490 (27%)]\tLoss: 2.05\n","Train Epoch: 1 [23552/87490 (27%)]\tLoss: 2.05\n","Train Epoch: 1 [23616/87490 (27%)]\tLoss: 2.04\n","Train Epoch: 1 [23680/87490 (27%)]\tLoss: 2.04\n","Train Epoch: 1 [23744/87490 (27%)]\tLoss: 2.03\n","Train Epoch: 1 [23808/87490 (27%)]\tLoss: 2.03\n","Train Epoch: 1 [23872/87490 (27%)]\tLoss: 2.02\n","Train Epoch: 1 [23936/87490 (27%)]\tLoss: 2.02\n","Train Epoch: 1 [24000/87490 (27%)]\tLoss: 2.01\n","Train Epoch: 1 [24064/87490 (28%)]\tLoss: 2.01\n","Train Epoch: 1 [24128/87490 (28%)]\tLoss: 2.00\n","Train Epoch: 1 [24192/87490 (28%)]\tLoss: 2.00\n","Train Epoch: 1 [24256/87490 (28%)]\tLoss: 1.99\n","Train Epoch: 1 [24320/87490 (28%)]\tLoss: 1.99\n","Train Epoch: 1 [24384/87490 (28%)]\tLoss: 1.98\n","Train Epoch: 1 [24448/87490 (28%)]\tLoss: 1.98\n","Train Epoch: 1 [24512/87490 (28%)]\tLoss: 1.97\n","Train Epoch: 1 [24576/87490 (28%)]\tLoss: 1.97\n","Train Epoch: 1 [24640/87490 (28%)]\tLoss: 1.96\n","Train Epoch: 1 [24704/87490 (28%)]\tLoss: 1.96\n","Train Epoch: 1 [24768/87490 (28%)]\tLoss: 1.95\n","Train Epoch: 1 [24832/87490 (28%)]\tLoss: 1.95\n","Train Epoch: 1 [24896/87490 (28%)]\tLoss: 1.94\n","Train Epoch: 1 [24960/87490 (29%)]\tLoss: 1.94\n","Train Epoch: 1 [25024/87490 (29%)]\tLoss: 1.93\n","Train Epoch: 1 [25088/87490 (29%)]\tLoss: 1.93\n","Train Epoch: 1 [25152/87490 (29%)]\tLoss: 1.92\n","Train Epoch: 1 [25216/87490 (29%)]\tLoss: 1.92\n","Train Epoch: 1 [25280/87490 (29%)]\tLoss: 1.91\n","Train Epoch: 1 [25344/87490 (29%)]\tLoss: 1.91\n","Train Epoch: 1 [25408/87490 (29%)]\tLoss: 1.90\n","Train Epoch: 1 [25472/87490 (29%)]\tLoss: 1.90\n","Train Epoch: 1 [25536/87490 (29%)]\tLoss: 1.89\n","Train Epoch: 1 [25600/87490 (29%)]\tLoss: 1.89\n","Train Epoch: 1 [25664/87490 (29%)]\tLoss: 1.89\n","Train Epoch: 1 [25728/87490 (29%)]\tLoss: 1.88\n","Train Epoch: 1 [25792/87490 (29%)]\tLoss: 1.88\n","Train Epoch: 1 [25856/87490 (30%)]\tLoss: 1.87\n","Train Epoch: 1 [25920/87490 (30%)]\tLoss: 1.87\n","Train Epoch: 1 [25984/87490 (30%)]\tLoss: 1.86\n","Train Epoch: 1 [26048/87490 (30%)]\tLoss: 1.86\n","Train Epoch: 1 [26112/87490 (30%)]\tLoss: 1.86\n","Train Epoch: 1 [26176/87490 (30%)]\tLoss: 1.85\n","Train Epoch: 1 [26240/87490 (30%)]\tLoss: 1.85\n","Train Epoch: 1 [26304/87490 (30%)]\tLoss: 1.84\n","Train Epoch: 1 [26368/87490 (30%)]\tLoss: 1.84\n","Train Epoch: 1 [26432/87490 (30%)]\tLoss: 1.83\n","Train Epoch: 1 [26496/87490 (30%)]\tLoss: 1.83\n","Train Epoch: 1 [26560/87490 (30%)]\tLoss: 1.83\n","Train Epoch: 1 [26624/87490 (30%)]\tLoss: 1.82\n","Train Epoch: 1 [26688/87490 (31%)]\tLoss: 1.82\n","Train Epoch: 1 [26752/87490 (31%)]\tLoss: 1.81\n","Train Epoch: 1 [26816/87490 (31%)]\tLoss: 1.81\n","Train Epoch: 1 [26880/87490 (31%)]\tLoss: 1.80\n","Train Epoch: 1 [26944/87490 (31%)]\tLoss: 1.80\n","Train Epoch: 1 [27008/87490 (31%)]\tLoss: 1.80\n","Train Epoch: 1 [27072/87490 (31%)]\tLoss: 1.79\n","Train Epoch: 1 [27136/87490 (31%)]\tLoss: 1.79\n","Train Epoch: 1 [27200/87490 (31%)]\tLoss: 1.78\n","Train Epoch: 1 [27264/87490 (31%)]\tLoss: 1.78\n","Train Epoch: 1 [27328/87490 (31%)]\tLoss: 1.77\n","Train Epoch: 1 [27392/87490 (31%)]\tLoss: 1.77\n","Train Epoch: 1 [27456/87490 (31%)]\tLoss: 1.77\n","Train Epoch: 1 [27520/87490 (31%)]\tLoss: 1.76\n","Train Epoch: 1 [27584/87490 (32%)]\tLoss: 1.76\n","Train Epoch: 1 [27648/87490 (32%)]\tLoss: 1.76\n","Train Epoch: 1 [27712/87490 (32%)]\tLoss: 1.75\n","Train Epoch: 1 [27776/87490 (32%)]\tLoss: 1.75\n","Train Epoch: 1 [27840/87490 (32%)]\tLoss: 1.74\n","Train Epoch: 1 [27904/87490 (32%)]\tLoss: 1.74\n","Train Epoch: 1 [27968/87490 (32%)]\tLoss: 1.74\n","Train Epoch: 1 [28032/87490 (32%)]\tLoss: 1.73\n","Train Epoch: 1 [28096/87490 (32%)]\tLoss: 1.73\n","Train Epoch: 1 [28160/87490 (32%)]\tLoss: 1.73\n","Train Epoch: 1 [28224/87490 (32%)]\tLoss: 1.72\n","Train Epoch: 1 [28288/87490 (32%)]\tLoss: 1.72\n","Train Epoch: 1 [28352/87490 (32%)]\tLoss: 1.71\n","Train Epoch: 1 [28416/87490 (32%)]\tLoss: 1.71\n","Train Epoch: 1 [28480/87490 (33%)]\tLoss: 1.71\n","Train Epoch: 1 [28544/87490 (33%)]\tLoss: 1.70\n","Train Epoch: 1 [28608/87490 (33%)]\tLoss: 1.70\n","Train Epoch: 1 [28672/87490 (33%)]\tLoss: 1.70\n","Train Epoch: 1 [28736/87490 (33%)]\tLoss: 1.69\n","Train Epoch: 1 [28800/87490 (33%)]\tLoss: 1.69\n","Train Epoch: 1 [28864/87490 (33%)]\tLoss: 1.69\n","Train Epoch: 1 [28928/87490 (33%)]\tLoss: 1.68\n","Train Epoch: 1 [28992/87490 (33%)]\tLoss: 1.68\n","Train Epoch: 1 [29056/87490 (33%)]\tLoss: 1.67\n","Train Epoch: 1 [29120/87490 (33%)]\tLoss: 1.67\n","Train Epoch: 1 [29184/87490 (33%)]\tLoss: 1.67\n","Train Epoch: 1 [29248/87490 (33%)]\tLoss: 1.66\n","Train Epoch: 1 [29312/87490 (34%)]\tLoss: 1.66\n","Train Epoch: 1 [29376/87490 (34%)]\tLoss: 1.66\n","Train Epoch: 1 [29440/87490 (34%)]\tLoss: 1.65\n","Train Epoch: 1 [29504/87490 (34%)]\tLoss: 1.65\n","Train Epoch: 1 [29568/87490 (34%)]\tLoss: 1.65\n","Train Epoch: 1 [29632/87490 (34%)]\tLoss: 1.64\n","Train Epoch: 1 [29696/87490 (34%)]\tLoss: 1.64\n","Train Epoch: 1 [29760/87490 (34%)]\tLoss: 1.64\n","Train Epoch: 1 [29824/87490 (34%)]\tLoss: 1.63\n","Train Epoch: 1 [29888/87490 (34%)]\tLoss: 1.63\n","Train Epoch: 1 [29952/87490 (34%)]\tLoss: 1.62\n","Train Epoch: 1 [30016/87490 (34%)]\tLoss: 1.62\n","Train Epoch: 1 [30080/87490 (34%)]\tLoss: 1.62\n","Train Epoch: 1 [30144/87490 (34%)]\tLoss: 1.61\n","Train Epoch: 1 [30208/87490 (35%)]\tLoss: 1.61\n","Train Epoch: 1 [30272/87490 (35%)]\tLoss: 1.61\n","Train Epoch: 1 [30336/87490 (35%)]\tLoss: 1.60\n","Train Epoch: 1 [30400/87490 (35%)]\tLoss: 1.60\n","Train Epoch: 1 [30464/87490 (35%)]\tLoss: 1.60\n","Train Epoch: 1 [30528/87490 (35%)]\tLoss: 1.59\n","Train Epoch: 1 [30592/87490 (35%)]\tLoss: 1.59\n","Train Epoch: 1 [30656/87490 (35%)]\tLoss: 1.59\n","Train Epoch: 1 [30720/87490 (35%)]\tLoss: 1.58\n","Train Epoch: 1 [30784/87490 (35%)]\tLoss: 1.58\n","Train Epoch: 1 [30848/87490 (35%)]\tLoss: 1.58\n","Train Epoch: 1 [30912/87490 (35%)]\tLoss: 1.57\n","Train Epoch: 1 [30976/87490 (35%)]\tLoss: 1.57\n","Train Epoch: 1 [31040/87490 (35%)]\tLoss: 1.57\n","Train Epoch: 1 [31104/87490 (36%)]\tLoss: 1.57\n","Train Epoch: 1 [31168/87490 (36%)]\tLoss: 1.56\n","Train Epoch: 1 [31232/87490 (36%)]\tLoss: 1.56\n","Train Epoch: 1 [31296/87490 (36%)]\tLoss: 1.56\n","Train Epoch: 1 [31360/87490 (36%)]\tLoss: 1.55\n","Train Epoch: 1 [31424/87490 (36%)]\tLoss: 1.55\n","Train Epoch: 1 [31488/87490 (36%)]\tLoss: 1.55\n","Train Epoch: 1 [31552/87490 (36%)]\tLoss: 1.54\n","Train Epoch: 1 [31616/87490 (36%)]\tLoss: 1.54\n","Train Epoch: 1 [31680/87490 (36%)]\tLoss: 1.54\n","Train Epoch: 1 [31744/87490 (36%)]\tLoss: 1.54\n","Train Epoch: 1 [31808/87490 (36%)]\tLoss: 1.53\n","Train Epoch: 1 [31872/87490 (36%)]\tLoss: 1.53\n","Train Epoch: 1 [31936/87490 (37%)]\tLoss: 1.53\n","Train Epoch: 1 [32000/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32064/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32128/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32192/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32256/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32320/87490 (37%)]\tLoss: 1.52\n","Train Epoch: 1 [32384/87490 (37%)]\tLoss: 1.51\n","Train Epoch: 1 [32448/87490 (37%)]\tLoss: 1.51\n","Train Epoch: 1 [32512/87490 (37%)]\tLoss: 1.51\n","Train Epoch: 1 [32576/87490 (37%)]\tLoss: 1.51\n","Train Epoch: 1 [32640/87490 (37%)]\tLoss: 1.50\n","Train Epoch: 1 [32704/87490 (37%)]\tLoss: 1.50\n","Train Epoch: 1 [32768/87490 (37%)]\tLoss: 1.50\n","Train Epoch: 1 [32832/87490 (38%)]\tLoss: 1.49\n","Train Epoch: 1 [32896/87490 (38%)]\tLoss: 1.49\n","Train Epoch: 1 [32960/87490 (38%)]\tLoss: 1.49\n","Train Epoch: 1 [33024/87490 (38%)]\tLoss: 1.49\n","Train Epoch: 1 [33088/87490 (38%)]\tLoss: 1.48\n","Train Epoch: 1 [33152/87490 (38%)]\tLoss: 1.48\n","Train Epoch: 1 [33216/87490 (38%)]\tLoss: 1.48\n","Train Epoch: 1 [33280/87490 (38%)]\tLoss: 1.47\n","Train Epoch: 1 [33344/87490 (38%)]\tLoss: 1.47\n","Train Epoch: 1 [33408/87490 (38%)]\tLoss: 1.47\n","Train Epoch: 1 [33472/87490 (38%)]\tLoss: 1.47\n","Train Epoch: 1 [33536/87490 (38%)]\tLoss: 1.46\n","Train Epoch: 1 [33600/87490 (38%)]\tLoss: 1.46\n","Train Epoch: 1 [33664/87490 (38%)]\tLoss: 1.46\n","Train Epoch: 1 [33728/87490 (39%)]\tLoss: 1.46\n","Train Epoch: 1 [33792/87490 (39%)]\tLoss: 1.45\n","Train Epoch: 1 [33856/87490 (39%)]\tLoss: 1.45\n","Train Epoch: 1 [33920/87490 (39%)]\tLoss: 1.45\n","Train Epoch: 1 [33984/87490 (39%)]\tLoss: 1.45\n","Train Epoch: 1 [34048/87490 (39%)]\tLoss: 1.44\n","Train Epoch: 1 [34112/87490 (39%)]\tLoss: 1.44\n","Train Epoch: 1 [34176/87490 (39%)]\tLoss: 1.44\n","Train Epoch: 1 [34240/87490 (39%)]\tLoss: 1.44\n","Train Epoch: 1 [34304/87490 (39%)]\tLoss: 1.43\n","Train Epoch: 1 [34368/87490 (39%)]\tLoss: 1.43\n","Train Epoch: 1 [34432/87490 (39%)]\tLoss: 1.43\n","Train Epoch: 1 [34496/87490 (39%)]\tLoss: 1.43\n","Train Epoch: 1 [34560/87490 (40%)]\tLoss: 1.42\n","Train Epoch: 1 [34624/87490 (40%)]\tLoss: 1.42\n","Train Epoch: 1 [34688/87490 (40%)]\tLoss: 1.42\n","Train Epoch: 1 [34752/87490 (40%)]\tLoss: 1.42\n","Train Epoch: 1 [34816/87490 (40%)]\tLoss: 1.41\n","Train Epoch: 1 [34880/87490 (40%)]\tLoss: 1.41\n","Train Epoch: 1 [34944/87490 (40%)]\tLoss: 1.41\n","Train Epoch: 1 [35008/87490 (40%)]\tLoss: 1.41\n","Train Epoch: 1 [35072/87490 (40%)]\tLoss: 1.40\n","Train Epoch: 1 [35136/87490 (40%)]\tLoss: 1.40\n","Train Epoch: 1 [35200/87490 (40%)]\tLoss: 1.40\n","Train Epoch: 1 [35264/87490 (40%)]\tLoss: 1.40\n","Train Epoch: 1 [35328/87490 (40%)]\tLoss: 1.39\n","Train Epoch: 1 [35392/87490 (40%)]\tLoss: 1.39\n","Train Epoch: 1 [35456/87490 (41%)]\tLoss: 1.39\n","Train Epoch: 1 [35520/87490 (41%)]\tLoss: 1.39\n","Train Epoch: 1 [35584/87490 (41%)]\tLoss: 1.38\n","Train Epoch: 1 [35648/87490 (41%)]\tLoss: 1.38\n","Train Epoch: 1 [35712/87490 (41%)]\tLoss: 1.38\n","Train Epoch: 1 [35776/87490 (41%)]\tLoss: 1.38\n","Train Epoch: 1 [35840/87490 (41%)]\tLoss: 1.37\n","Train Epoch: 1 [35904/87490 (41%)]\tLoss: 1.37\n","Train Epoch: 1 [35968/87490 (41%)]\tLoss: 1.37\n","Train Epoch: 1 [36032/87490 (41%)]\tLoss: 1.37\n","Train Epoch: 1 [36096/87490 (41%)]\tLoss: 1.37\n","Train Epoch: 1 [36160/87490 (41%)]\tLoss: 1.36\n","Train Epoch: 1 [36224/87490 (41%)]\tLoss: 1.36\n","Train Epoch: 1 [36288/87490 (41%)]\tLoss: 1.36\n","Train Epoch: 1 [36352/87490 (42%)]\tLoss: 1.36\n","Train Epoch: 1 [36416/87490 (42%)]\tLoss: 1.35\n","Train Epoch: 1 [36480/87490 (42%)]\tLoss: 1.35\n","Train Epoch: 1 [36544/87490 (42%)]\tLoss: 1.35\n","Train Epoch: 1 [36608/87490 (42%)]\tLoss: 1.35\n","Train Epoch: 1 [36672/87490 (42%)]\tLoss: 1.35\n","Train Epoch: 1 [36736/87490 (42%)]\tLoss: 1.34\n","Train Epoch: 1 [36800/87490 (42%)]\tLoss: 1.34\n","Train Epoch: 1 [36864/87490 (42%)]\tLoss: 1.34\n","Train Epoch: 1 [36928/87490 (42%)]\tLoss: 1.34\n","Train Epoch: 1 [36992/87490 (42%)]\tLoss: 1.33\n","Train Epoch: 1 [37056/87490 (42%)]\tLoss: 1.33\n","Train Epoch: 1 [37120/87490 (42%)]\tLoss: 1.33\n","Train Epoch: 1 [37184/87490 (43%)]\tLoss: 1.33\n","Train Epoch: 1 [37248/87490 (43%)]\tLoss: 1.32\n","Train Epoch: 1 [37312/87490 (43%)]\tLoss: 1.32\n","Train Epoch: 1 [37376/87490 (43%)]\tLoss: 1.32\n","Train Epoch: 1 [37440/87490 (43%)]\tLoss: 1.32\n","Train Epoch: 1 [37504/87490 (43%)]\tLoss: 1.32\n","Train Epoch: 1 [37568/87490 (43%)]\tLoss: 1.31\n","Train Epoch: 1 [37632/87490 (43%)]\tLoss: 1.31\n","Train Epoch: 1 [37696/87490 (43%)]\tLoss: 1.31\n","Train Epoch: 1 [37760/87490 (43%)]\tLoss: 1.31\n","Train Epoch: 1 [37824/87490 (43%)]\tLoss: 1.30\n","Train Epoch: 1 [37888/87490 (43%)]\tLoss: 1.30\n","Train Epoch: 1 [37952/87490 (43%)]\tLoss: 1.30\n","Train Epoch: 1 [38016/87490 (43%)]\tLoss: 1.30\n","Train Epoch: 1 [38080/87490 (44%)]\tLoss: 1.30\n","Train Epoch: 1 [38144/87490 (44%)]\tLoss: 1.30\n","Train Epoch: 1 [38208/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38272/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38336/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38400/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38464/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38528/87490 (44%)]\tLoss: 1.29\n","Train Epoch: 1 [38592/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38656/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38720/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38784/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38848/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38912/87490 (44%)]\tLoss: 1.28\n","Train Epoch: 1 [38976/87490 (45%)]\tLoss: 1.27\n","Train Epoch: 1 [39040/87490 (45%)]\tLoss: 1.27\n","Train Epoch: 1 [39104/87490 (45%)]\tLoss: 1.27\n","Train Epoch: 1 [39168/87490 (45%)]\tLoss: 1.27\n","Train Epoch: 1 [39232/87490 (45%)]\tLoss: 1.27\n","Train Epoch: 1 [39296/87490 (45%)]\tLoss: 1.26\n","Train Epoch: 1 [39360/87490 (45%)]\tLoss: 1.26\n","Train Epoch: 1 [39424/87490 (45%)]\tLoss: 1.26\n","Train Epoch: 1 [39488/87490 (45%)]\tLoss: 1.26\n","Train Epoch: 1 [39552/87490 (45%)]\tLoss: 1.26\n","Train Epoch: 1 [39616/87490 (45%)]\tLoss: 1.25\n","Train Epoch: 1 [39680/87490 (45%)]\tLoss: 1.25\n","Train Epoch: 1 [39744/87490 (45%)]\tLoss: 1.25\n","Train Epoch: 1 [39808/87490 (46%)]\tLoss: 1.25\n","Train Epoch: 1 [39872/87490 (46%)]\tLoss: 1.25\n","Train Epoch: 1 [39936/87490 (46%)]\tLoss: 1.25\n","Train Epoch: 1 [40000/87490 (46%)]\tLoss: 1.24\n","Train Epoch: 1 [40064/87490 (46%)]\tLoss: 1.24\n","Train Epoch: 1 [40128/87490 (46%)]\tLoss: 1.24\n","Train Epoch: 1 [40192/87490 (46%)]\tLoss: 1.24\n","Train Epoch: 1 [40256/87490 (46%)]\tLoss: 1.24\n","Train Epoch: 1 [40320/87490 (46%)]\tLoss: 1.23\n","Train Epoch: 1 [40384/87490 (46%)]\tLoss: 1.23\n","Train Epoch: 1 [40448/87490 (46%)]\tLoss: 1.23\n","Train Epoch: 1 [40512/87490 (46%)]\tLoss: 1.23\n","Train Epoch: 1 [40576/87490 (46%)]\tLoss: 1.23\n","Train Epoch: 1 [40640/87490 (46%)]\tLoss: 1.22\n","Train Epoch: 1 [40704/87490 (47%)]\tLoss: 1.22\n","Train Epoch: 1 [40768/87490 (47%)]\tLoss: 1.22\n","Train Epoch: 1 [40832/87490 (47%)]\tLoss: 1.22\n","Train Epoch: 1 [40896/87490 (47%)]\tLoss: 1.22\n","Train Epoch: 1 [40960/87490 (47%)]\tLoss: 1.22\n","Train Epoch: 1 [41024/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41088/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41152/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41216/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41280/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41344/87490 (47%)]\tLoss: 1.21\n","Train Epoch: 1 [41408/87490 (47%)]\tLoss: 1.20\n","Train Epoch: 1 [41472/87490 (47%)]\tLoss: 1.20\n","Train Epoch: 1 [41536/87490 (47%)]\tLoss: 1.20\n","Train Epoch: 1 [41600/87490 (48%)]\tLoss: 1.20\n","Train Epoch: 1 [41664/87490 (48%)]\tLoss: 1.20\n","Train Epoch: 1 [41728/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [41792/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [41856/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [41920/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [41984/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [42048/87490 (48%)]\tLoss: 1.19\n","Train Epoch: 1 [42112/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42176/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42240/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42304/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42368/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42432/87490 (48%)]\tLoss: 1.18\n","Train Epoch: 1 [42496/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42560/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42624/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42688/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42752/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42816/87490 (49%)]\tLoss: 1.17\n","Train Epoch: 1 [42880/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [42944/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [43008/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [43072/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [43136/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [43200/87490 (49%)]\tLoss: 1.16\n","Train Epoch: 1 [43264/87490 (49%)]\tLoss: 1.15\n","Train Epoch: 1 [43328/87490 (50%)]\tLoss: 1.15\n","Train Epoch: 1 [43392/87490 (50%)]\tLoss: 1.15\n","Train Epoch: 1 [43456/87490 (50%)]\tLoss: 1.15\n","Train Epoch: 1 [43520/87490 (50%)]\tLoss: 1.15\n","Train Epoch: 1 [43584/87490 (50%)]\tLoss: 1.15\n","Train Epoch: 1 [43648/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [43712/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [43776/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [43840/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [43904/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [43968/87490 (50%)]\tLoss: 1.14\n","Train Epoch: 1 [44032/87490 (50%)]\tLoss: 1.13\n","Train Epoch: 1 [44096/87490 (50%)]\tLoss: 1.13\n","Train Epoch: 1 [44160/87490 (50%)]\tLoss: 1.13\n","Train Epoch: 1 [44224/87490 (51%)]\tLoss: 1.13\n","Train Epoch: 1 [44288/87490 (51%)]\tLoss: 1.13\n","Train Epoch: 1 [44352/87490 (51%)]\tLoss: 1.13\n","Train Epoch: 1 [44416/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44480/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44544/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44608/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44672/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44736/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44800/87490 (51%)]\tLoss: 1.12\n","Train Epoch: 1 [44864/87490 (51%)]\tLoss: 1.11\n","Train Epoch: 1 [44928/87490 (51%)]\tLoss: 1.11\n","Train Epoch: 1 [44992/87490 (51%)]\tLoss: 1.11\n","Train Epoch: 1 [45056/87490 (51%)]\tLoss: 1.11\n","Train Epoch: 1 [45120/87490 (52%)]\tLoss: 1.11\n","Train Epoch: 1 [45184/87490 (52%)]\tLoss: 1.11\n","Train Epoch: 1 [45248/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45312/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45376/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45440/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45504/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45568/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45632/87490 (52%)]\tLoss: 1.10\n","Train Epoch: 1 [45696/87490 (52%)]\tLoss: 1.09\n","Train Epoch: 1 [45760/87490 (52%)]\tLoss: 1.09\n","Train Epoch: 1 [45824/87490 (52%)]\tLoss: 1.09\n","Train Epoch: 1 [45888/87490 (52%)]\tLoss: 1.09\n","Train Epoch: 1 [45952/87490 (53%)]\tLoss: 1.09\n","Train Epoch: 1 [46016/87490 (53%)]\tLoss: 1.09\n","Train Epoch: 1 [46080/87490 (53%)]\tLoss: 1.09\n","Train Epoch: 1 [46144/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46208/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46272/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46336/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46400/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46464/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46528/87490 (53%)]\tLoss: 1.08\n","Train Epoch: 1 [46592/87490 (53%)]\tLoss: 1.07\n","Train Epoch: 1 [46656/87490 (53%)]\tLoss: 1.07\n","Train Epoch: 1 [46720/87490 (53%)]\tLoss: 1.07\n","Train Epoch: 1 [46784/87490 (53%)]\tLoss: 1.07\n","Train Epoch: 1 [46848/87490 (54%)]\tLoss: 1.07\n","Train Epoch: 1 [46912/87490 (54%)]\tLoss: 1.07\n","Train Epoch: 1 [46976/87490 (54%)]\tLoss: 1.07\n","Train Epoch: 1 [47040/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47104/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47168/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47232/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47296/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47360/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47424/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47488/87490 (54%)]\tLoss: 1.06\n","Train Epoch: 1 [47552/87490 (54%)]\tLoss: 1.05\n","Train Epoch: 1 [47616/87490 (54%)]\tLoss: 1.05\n","Train Epoch: 1 [47680/87490 (54%)]\tLoss: 1.05\n","Train Epoch: 1 [47744/87490 (55%)]\tLoss: 1.05\n","Train Epoch: 1 [47808/87490 (55%)]\tLoss: 1.05\n","Train Epoch: 1 [47872/87490 (55%)]\tLoss: 1.05\n","Train Epoch: 1 [47936/87490 (55%)]\tLoss: 1.05\n","Train Epoch: 1 [48000/87490 (55%)]\tLoss: 1.05\n","Train Epoch: 1 [48064/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48128/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48192/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48256/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48320/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48384/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48448/87490 (55%)]\tLoss: 1.04\n","Train Epoch: 1 [48512/87490 (55%)]\tLoss: 1.03\n","Train Epoch: 1 [48576/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48640/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48704/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48768/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48832/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48896/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [48960/87490 (56%)]\tLoss: 1.03\n","Train Epoch: 1 [49024/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49088/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49152/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49216/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49280/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49344/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49408/87490 (56%)]\tLoss: 1.02\n","Train Epoch: 1 [49472/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49536/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49600/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49664/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49728/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49792/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49856/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49920/87490 (57%)]\tLoss: 1.01\n","Train Epoch: 1 [49984/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50048/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50112/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50176/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50240/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50304/87490 (57%)]\tLoss: 1.00\n","Train Epoch: 1 [50368/87490 (58%)]\tLoss: 1.00\n","Train Epoch: 1 [50432/87490 (58%)]\tLoss: 1.00\n","Train Epoch: 1 [50496/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50560/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50624/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50688/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50752/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50816/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50880/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [50944/87490 (58%)]\tLoss: 0.99\n","Train Epoch: 1 [51008/87490 (58%)]\tLoss: 0.98\n","Train Epoch: 1 [51072/87490 (58%)]\tLoss: 0.98\n","Train Epoch: 1 [51136/87490 (58%)]\tLoss: 0.98\n","Train Epoch: 1 [51200/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51264/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51328/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51392/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51456/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51520/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51584/87490 (59%)]\tLoss: 0.98\n","Train Epoch: 1 [51648/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [51712/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [51776/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [51840/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [51904/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [51968/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [52032/87490 (59%)]\tLoss: 0.97\n","Train Epoch: 1 [52096/87490 (60%)]\tLoss: 0.97\n","Train Epoch: 1 [52160/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52224/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52288/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52352/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52416/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52480/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52544/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52608/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52672/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52736/87490 (60%)]\tLoss: 0.96\n","Train Epoch: 1 [52800/87490 (60%)]\tLoss: 0.95\n","Train Epoch: 1 [52864/87490 (60%)]\tLoss: 0.95\n","Train Epoch: 1 [52928/87490 (60%)]\tLoss: 0.95\n","Train Epoch: 1 [52992/87490 (61%)]\tLoss: 0.95\n","Train Epoch: 1 [53056/87490 (61%)]\tLoss: 0.95\n","Train Epoch: 1 [53120/87490 (61%)]\tLoss: 0.95\n","Train Epoch: 1 [53184/87490 (61%)]\tLoss: 0.95\n","Train Epoch: 1 [53248/87490 (61%)]\tLoss: 0.95\n","Train Epoch: 1 [53312/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53376/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53440/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53504/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53568/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53632/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53696/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53760/87490 (61%)]\tLoss: 0.94\n","Train Epoch: 1 [53824/87490 (62%)]\tLoss: 0.94\n","Train Epoch: 1 [53888/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [53952/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54016/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54080/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54144/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54208/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54272/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54336/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54400/87490 (62%)]\tLoss: 0.93\n","Train Epoch: 1 [54464/87490 (62%)]\tLoss: 0.92\n","Train Epoch: 1 [54528/87490 (62%)]\tLoss: 0.92\n","Train Epoch: 1 [54592/87490 (62%)]\tLoss: 0.92\n","Train Epoch: 1 [54656/87490 (62%)]\tLoss: 0.92\n","Train Epoch: 1 [54720/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [54784/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [54848/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [54912/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [54976/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [55040/87490 (63%)]\tLoss: 0.92\n","Train Epoch: 1 [55104/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55168/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55232/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55296/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55360/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55424/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55488/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55552/87490 (63%)]\tLoss: 0.91\n","Train Epoch: 1 [55616/87490 (64%)]\tLoss: 0.91\n","Train Epoch: 1 [55680/87490 (64%)]\tLoss: 0.91\n","Train Epoch: 1 [55744/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [55808/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [55872/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [55936/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56000/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56064/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56128/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56192/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56256/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56320/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56384/87490 (64%)]\tLoss: 0.90\n","Train Epoch: 1 [56448/87490 (65%)]\tLoss: 0.90\n","Train Epoch: 1 [56512/87490 (65%)]\tLoss: 0.90\n","Train Epoch: 1 [56576/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56640/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56704/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56768/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56832/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56896/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [56960/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [57024/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [57088/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [57152/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [57216/87490 (65%)]\tLoss: 0.89\n","Train Epoch: 1 [57280/87490 (65%)]\tLoss: 0.88\n","Train Epoch: 1 [57344/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57408/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57472/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57536/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57600/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57664/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57728/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57792/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57856/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57920/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [57984/87490 (66%)]\tLoss: 0.88\n","Train Epoch: 1 [58048/87490 (66%)]\tLoss: 0.87\n","Train Epoch: 1 [58112/87490 (66%)]\tLoss: 0.87\n","Train Epoch: 1 [58176/87490 (66%)]\tLoss: 0.87\n","Train Epoch: 1 [58240/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58304/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58368/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58432/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58496/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58560/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58624/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58688/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58752/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58816/87490 (67%)]\tLoss: 0.87\n","Train Epoch: 1 [58880/87490 (67%)]\tLoss: 0.86\n","Train Epoch: 1 [58944/87490 (67%)]\tLoss: 0.86\n","Train Epoch: 1 [59008/87490 (67%)]\tLoss: 0.86\n","Train Epoch: 1 [59072/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59136/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59200/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59264/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59328/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59392/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59456/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59520/87490 (68%)]\tLoss: 0.86\n","Train Epoch: 1 [59584/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59648/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59712/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59776/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59840/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59904/87490 (68%)]\tLoss: 0.85\n","Train Epoch: 1 [59968/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60032/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60096/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60160/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60224/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60288/87490 (69%)]\tLoss: 0.85\n","Train Epoch: 1 [60352/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60416/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60480/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60544/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60608/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60672/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60736/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60800/87490 (69%)]\tLoss: 0.84\n","Train Epoch: 1 [60864/87490 (70%)]\tLoss: 0.84\n","Train Epoch: 1 [60928/87490 (70%)]\tLoss: 0.84\n","Train Epoch: 1 [60992/87490 (70%)]\tLoss: 0.84\n","Train Epoch: 1 [61056/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61120/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61184/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61248/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61312/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61376/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61440/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61504/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61568/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61632/87490 (70%)]\tLoss: 0.83\n","Train Epoch: 1 [61696/87490 (71%)]\tLoss: 0.83\n","Train Epoch: 1 [61760/87490 (71%)]\tLoss: 0.83\n","Train Epoch: 1 [61824/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [61888/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [61952/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62016/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62080/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62144/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62208/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62272/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62336/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62400/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62464/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62528/87490 (71%)]\tLoss: 0.82\n","Train Epoch: 1 [62592/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62656/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62720/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62784/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62848/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62912/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [62976/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63040/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63104/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63168/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63232/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63296/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63360/87490 (72%)]\tLoss: 0.81\n","Train Epoch: 1 [63424/87490 (72%)]\tLoss: 0.80\n","Train Epoch: 1 [63488/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63552/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63616/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63680/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63744/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63808/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63872/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [63936/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [64000/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [64064/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [64128/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [64192/87490 (73%)]\tLoss: 0.80\n","Train Epoch: 1 [64256/87490 (73%)]\tLoss: 0.79\n","Train Epoch: 1 [64320/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64384/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64448/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64512/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64576/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64640/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64704/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64768/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64832/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64896/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [64960/87490 (74%)]\tLoss: 0.79\n","Train Epoch: 1 [65024/87490 (74%)]\tLoss: 0.78\n","Train Epoch: 1 [65088/87490 (74%)]\tLoss: 0.78\n","Train Epoch: 1 [65152/87490 (74%)]\tLoss: 0.78\n","Train Epoch: 1 [65216/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65280/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65344/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65408/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65472/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65536/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65600/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65664/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65728/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65792/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65856/87490 (75%)]\tLoss: 0.78\n","Train Epoch: 1 [65920/87490 (75%)]\tLoss: 0.77\n","Train Epoch: 1 [65984/87490 (75%)]\tLoss: 0.77\n","Train Epoch: 1 [66048/87490 (75%)]\tLoss: 0.77\n","Train Epoch: 1 [66112/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66176/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66240/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66304/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66368/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66432/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66496/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66560/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66624/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66688/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66752/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66816/87490 (76%)]\tLoss: 0.77\n","Train Epoch: 1 [66880/87490 (76%)]\tLoss: 0.76\n","Train Epoch: 1 [66944/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67008/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67072/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67136/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67200/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67264/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67328/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67392/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67456/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67520/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67584/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67648/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67712/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67776/87490 (77%)]\tLoss: 0.76\n","Train Epoch: 1 [67840/87490 (78%)]\tLoss: 0.76\n","Train Epoch: 1 [67904/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [67968/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68032/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68096/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68160/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68224/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68288/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68352/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68416/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68480/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68544/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68608/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68672/87490 (78%)]\tLoss: 0.75\n","Train Epoch: 1 [68736/87490 (79%)]\tLoss: 0.75\n","Train Epoch: 1 [68800/87490 (79%)]\tLoss: 0.75\n","Train Epoch: 1 [68864/87490 (79%)]\tLoss: 0.75\n","Train Epoch: 1 [68928/87490 (79%)]\tLoss: 0.75\n","Train Epoch: 1 [68992/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69056/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69120/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69184/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69248/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69312/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69376/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69440/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69504/87490 (79%)]\tLoss: 0.74\n","Train Epoch: 1 [69568/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69632/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69696/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69760/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69824/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69888/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [69952/87490 (80%)]\tLoss: 0.74\n","Train Epoch: 1 [70016/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70080/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70144/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70208/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70272/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70336/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70400/87490 (80%)]\tLoss: 0.73\n","Train Epoch: 1 [70464/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70528/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70592/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70656/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70720/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70784/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70848/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70912/87490 (81%)]\tLoss: 0.73\n","Train Epoch: 1 [70976/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71040/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71104/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71168/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71232/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71296/87490 (81%)]\tLoss: 0.72\n","Train Epoch: 1 [71360/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71424/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71488/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71552/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71616/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71680/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71744/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71808/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71872/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [71936/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [72000/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [72064/87490 (82%)]\tLoss: 0.72\n","Train Epoch: 1 [72128/87490 (82%)]\tLoss: 0.71\n","Train Epoch: 1 [72192/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72256/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72320/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72384/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72448/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72512/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72576/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72640/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72704/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72768/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72832/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72896/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [72960/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [73024/87490 (83%)]\tLoss: 0.71\n","Train Epoch: 1 [73088/87490 (84%)]\tLoss: 0.71\n","Train Epoch: 1 [73152/87490 (84%)]\tLoss: 0.71\n","Train Epoch: 1 [73216/87490 (84%)]\tLoss: 0.71\n","Train Epoch: 1 [73280/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73344/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73408/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73472/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73536/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73600/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73664/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73728/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73792/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73856/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73920/87490 (84%)]\tLoss: 0.70\n","Train Epoch: 1 [73984/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74048/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74112/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74176/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74240/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74304/87490 (85%)]\tLoss: 0.70\n","Train Epoch: 1 [74368/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74432/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74496/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74560/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74624/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74688/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74752/87490 (85%)]\tLoss: 0.69\n","Train Epoch: 1 [74816/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [74880/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [74944/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75008/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75072/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75136/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75200/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75264/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75328/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75392/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75456/87490 (86%)]\tLoss: 0.69\n","Train Epoch: 1 [75520/87490 (86%)]\tLoss: 0.68\n","Train Epoch: 1 [75584/87490 (86%)]\tLoss: 0.68\n","Train Epoch: 1 [75648/87490 (86%)]\tLoss: 0.68\n","Train Epoch: 1 [75712/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [75776/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [75840/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [75904/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [75968/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76032/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76096/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76160/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76224/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76288/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76352/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76416/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76480/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76544/87490 (87%)]\tLoss: 0.68\n","Train Epoch: 1 [76608/87490 (88%)]\tLoss: 0.68\n","Train Epoch: 1 [76672/87490 (88%)]\tLoss: 0.68\n","Train Epoch: 1 [76736/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [76800/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [76864/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [76928/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [76992/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77056/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77120/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77184/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77248/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77312/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77376/87490 (88%)]\tLoss: 0.67\n","Train Epoch: 1 [77440/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77504/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77568/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77632/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77696/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77760/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77824/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77888/87490 (89%)]\tLoss: 0.67\n","Train Epoch: 1 [77952/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78016/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78080/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78144/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78208/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78272/87490 (89%)]\tLoss: 0.66\n","Train Epoch: 1 [78336/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78400/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78464/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78528/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78592/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78656/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78720/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78784/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78848/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78912/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [78976/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [79040/87490 (90%)]\tLoss: 0.66\n","Train Epoch: 1 [79104/87490 (90%)]\tLoss: 0.65\n","Train Epoch: 1 [79168/87490 (90%)]\tLoss: 0.65\n","Train Epoch: 1 [79232/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79296/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79360/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79424/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79488/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79552/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79616/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79680/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79744/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79808/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79872/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [79936/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [80000/87490 (91%)]\tLoss: 0.65\n","Train Epoch: 1 [80064/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80128/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80192/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80256/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80320/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80384/87490 (92%)]\tLoss: 0.65\n","Train Epoch: 1 [80448/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80512/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80576/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80640/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80704/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80768/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80832/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80896/87490 (92%)]\tLoss: 0.64\n","Train Epoch: 1 [80960/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81024/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81088/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81152/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81216/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81280/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81344/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81408/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81472/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81536/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81600/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81664/87490 (93%)]\tLoss: 0.64\n","Train Epoch: 1 [81728/87490 (93%)]\tLoss: 0.63\n","Train Epoch: 1 [81792/87490 (93%)]\tLoss: 0.63\n","Train Epoch: 1 [81856/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [81920/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [81984/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82048/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82112/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82176/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82240/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82304/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82368/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82432/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82496/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82560/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82624/87490 (94%)]\tLoss: 0.63\n","Train Epoch: 1 [82688/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [82752/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [82816/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [82880/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [82944/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [83008/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [83072/87490 (95%)]\tLoss: 0.63\n","Train Epoch: 1 [83136/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83200/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83264/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83328/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83392/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83456/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83520/87490 (95%)]\tLoss: 0.62\n","Train Epoch: 1 [83584/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83648/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83712/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83776/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83840/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83904/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [83968/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84032/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84096/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84160/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84224/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84288/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84352/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84416/87490 (96%)]\tLoss: 0.62\n","Train Epoch: 1 [84480/87490 (97%)]\tLoss: 0.62\n","Train Epoch: 1 [84544/87490 (97%)]\tLoss: 0.62\n","Train Epoch: 1 [84608/87490 (97%)]\tLoss: 0.62\n","Train Epoch: 1 [84672/87490 (97%)]\tLoss: 0.62\n","Train Epoch: 1 [84736/87490 (97%)]\tLoss: 0.62\n","Train Epoch: 1 [84800/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [84864/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [84928/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [84992/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [85056/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [85120/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [85184/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [85248/87490 (97%)]\tLoss: 0.61\n","Train Epoch: 1 [85312/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85376/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85440/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85504/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85568/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85632/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85696/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85760/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85824/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85888/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [85952/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [86016/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [86080/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [86144/87490 (98%)]\tLoss: 0.61\n","Train Epoch: 1 [86208/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86272/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86336/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86400/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86464/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86528/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86592/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86656/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86720/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86784/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86848/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86912/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [86976/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [87040/87490 (99%)]\tLoss: 0.61\n","Train Epoch: 1 [87104/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87168/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87232/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87296/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87360/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87424/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [87488/87490 (100%)]\tLoss: 0.61\n","Train Epoch: 1 [2736/87490 (3%)]\tLoss: 0.62\n","evaluating trained model ...\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 63/64 (98%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 60/64 (94%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 60/64 (94%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 60/64 (94%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 63/64 (98%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 63/64 (98%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 61/64 (95%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 53/64 (83%) ; correctly guess bot: 55/64 (86%)\n","Train Epoch: 2 [64/87490 (0%)]\tLoss: 0.49\n","Train Epoch: 2 [128/87490 (0%)]\tLoss: 0.45\n","Train Epoch: 2 [192/87490 (0%)]\tLoss: 0.52\n","Train Epoch: 2 [256/87490 (0%)]\tLoss: 0.49\n","Train Epoch: 2 [320/87490 (0%)]\tLoss: 0.43\n","Train Epoch: 2 [384/87490 (0%)]\tLoss: 0.40\n","Train Epoch: 2 [448/87490 (1%)]\tLoss: 0.39\n","Train Epoch: 2 [512/87490 (1%)]\tLoss: 0.41\n","Train Epoch: 2 [576/87490 (1%)]\tLoss: 0.40\n","Train Epoch: 2 [640/87490 (1%)]\tLoss: 0.39\n","Train Epoch: 2 [704/87490 (1%)]\tLoss: 0.38\n","Train Epoch: 2 [768/87490 (1%)]\tLoss: 0.36\n","Train Epoch: 2 [832/87490 (1%)]\tLoss: 0.35\n","Train Epoch: 2 [896/87490 (1%)]\tLoss: 0.33\n","Train Epoch: 2 [960/87490 (1%)]\tLoss: 0.32\n","Train Epoch: 2 [1024/87490 (1%)]\tLoss: 0.30\n","Train Epoch: 2 [1088/87490 (1%)]\tLoss: 0.31\n","Train Epoch: 2 [1152/87490 (1%)]\tLoss: 0.30\n","Train Epoch: 2 [1216/87490 (1%)]\tLoss: 0.29\n","Train Epoch: 2 [1280/87490 (1%)]\tLoss: 0.28\n","Train Epoch: 2 [1344/87490 (2%)]\tLoss: 0.27\n","Train Epoch: 2 [1408/87490 (2%)]\tLoss: 0.26\n","Train Epoch: 2 [1472/87490 (2%)]\tLoss: 0.25\n","Train Epoch: 2 [1536/87490 (2%)]\tLoss: 0.24\n","Train Epoch: 2 [1600/87490 (2%)]\tLoss: 0.23\n","Train Epoch: 2 [1664/87490 (2%)]\tLoss: 0.23\n","Train Epoch: 2 [1728/87490 (2%)]\tLoss: 0.22\n","Train Epoch: 2 [1792/87490 (2%)]\tLoss: 0.21\n","Train Epoch: 2 [1856/87490 (2%)]\tLoss: 0.21\n","Train Epoch: 2 [1920/87490 (2%)]\tLoss: 0.20\n","Train Epoch: 2 [1984/87490 (2%)]\tLoss: 0.20\n","Train Epoch: 2 [2048/87490 (2%)]\tLoss: 0.19\n","Train Epoch: 2 [2112/87490 (2%)]\tLoss: 0.20\n","Train Epoch: 2 [2176/87490 (2%)]\tLoss: 0.21\n","Train Epoch: 2 [2240/87490 (3%)]\tLoss: 0.20\n","Train Epoch: 2 [2304/87490 (3%)]\tLoss: 0.20\n","Train Epoch: 2 [2368/87490 (3%)]\tLoss: 0.19\n","Train Epoch: 2 [2432/87490 (3%)]\tLoss: 0.19\n","Train Epoch: 2 [2496/87490 (3%)]\tLoss: 0.19\n","Train Epoch: 2 [2560/87490 (3%)]\tLoss: 0.18\n","Train Epoch: 2 [2624/87490 (3%)]\tLoss: 0.18\n","Train Epoch: 2 [2688/87490 (3%)]\tLoss: 0.18\n","Train Epoch: 2 [2752/87490 (3%)]\tLoss: 0.17\n","Train Epoch: 2 [2816/87490 (3%)]\tLoss: 0.17\n","Train Epoch: 2 [2880/87490 (3%)]\tLoss: 0.17\n","Train Epoch: 2 [2944/87490 (3%)]\tLoss: 0.16\n","Train Epoch: 2 [3008/87490 (3%)]\tLoss: 0.16\n","Train Epoch: 2 [3072/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 2 [3136/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 2 [3200/87490 (4%)]\tLoss: 0.15\n","Train Epoch: 2 [3264/87490 (4%)]\tLoss: 0.15\n","Train Epoch: 2 [3328/87490 (4%)]\tLoss: 0.15\n","Train Epoch: 2 [3392/87490 (4%)]\tLoss: 0.15\n","Train Epoch: 2 [3456/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3520/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3584/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3648/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3712/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3776/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3840/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3904/87490 (4%)]\tLoss: 0.14\n","Train Epoch: 2 [3968/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 2 [4032/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4096/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4160/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4224/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4288/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4352/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4416/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4480/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4544/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4608/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4672/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4736/87490 (5%)]\tLoss: 0.12\n","Train Epoch: 2 [4800/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 2 [4864/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [4928/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [4992/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [5056/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [5120/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [5184/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [5248/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 2 [5312/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5376/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5440/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5504/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5568/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5632/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 2 [5696/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 2 [5760/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 2 [5824/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 2 [5888/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [5952/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6016/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6080/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6144/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6208/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6272/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6336/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6400/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6464/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 2 [6528/87490 (7%)]\tLoss: 0.10\n","Train Epoch: 2 [6592/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6656/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6720/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6784/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6848/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6912/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [6976/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7040/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7104/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7168/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7232/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7296/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7360/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7424/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 2 [7488/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 2 [7552/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 2 [7616/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 2 [7680/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 2 [7744/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [7808/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [7872/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [7936/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8000/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8064/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8128/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8192/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8256/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 2 [8320/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8384/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8448/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8512/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8576/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8640/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8704/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8768/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8832/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8896/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [8960/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [9024/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [9088/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 2 [9152/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 2 [9216/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9280/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9344/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9408/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9472/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9536/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9600/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 2 [9664/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 2 [9728/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 2 [9792/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9856/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9920/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [9984/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [10048/87490 (11%)]\tLoss: 0.09\n","Train Epoch: 2 [10112/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10176/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10240/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10304/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10368/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10432/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10496/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10560/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10624/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10688/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10752/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10816/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10880/87490 (12%)]\tLoss: 0.09\n","Train Epoch: 2 [10944/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11008/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11072/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11136/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11200/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11264/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11328/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11392/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11456/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11520/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11584/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11648/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11712/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11776/87490 (13%)]\tLoss: 0.08\n","Train Epoch: 2 [11840/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [11904/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [11968/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12032/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12096/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12160/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12224/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12288/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12352/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12416/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12480/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12544/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12608/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12672/87490 (14%)]\tLoss: 0.08\n","Train Epoch: 2 [12736/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [12800/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [12864/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [12928/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [12992/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [13056/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [13120/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 2 [13184/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13248/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13312/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13376/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13440/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13504/87490 (15%)]\tLoss: 0.08\n","Train Epoch: 2 [13568/87490 (16%)]\tLoss: 0.08\n","Train Epoch: 2 [13632/87490 (16%)]\tLoss: 0.08\n","Train Epoch: 2 [13696/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [13760/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [13824/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [13888/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [13952/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14016/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14080/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14144/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14208/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14272/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14336/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14400/87490 (16%)]\tLoss: 0.07\n","Train Epoch: 2 [14464/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14528/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14592/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14656/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14720/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14784/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14848/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14912/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [14976/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15040/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15104/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15168/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15232/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15296/87490 (17%)]\tLoss: 0.07\n","Train Epoch: 2 [15360/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15424/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15488/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15552/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15616/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15680/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15744/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15808/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15872/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [15936/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [16000/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [16064/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [16128/87490 (18%)]\tLoss: 0.07\n","Train Epoch: 2 [16192/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16256/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16320/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16384/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16448/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16512/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16576/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16640/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16704/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16768/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16832/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16896/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [16960/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [17024/87490 (19%)]\tLoss: 0.07\n","Train Epoch: 2 [17088/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17152/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17216/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17280/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17344/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17408/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17472/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 2 [17536/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17600/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17664/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17728/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17792/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17856/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17920/87490 (20%)]\tLoss: 0.07\n","Train Epoch: 2 [17984/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18048/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18112/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18176/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18240/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18304/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18368/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18432/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18496/87490 (21%)]\tLoss: 0.07\n","Train Epoch: 2 [18560/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18624/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18688/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18752/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 2 [18816/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [18880/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [18944/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19008/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19072/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19136/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19200/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19264/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19328/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19392/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19456/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19520/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19584/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19648/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 2 [19712/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [19776/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [19840/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [19904/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [19968/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20032/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20096/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20160/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20224/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20288/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20352/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20416/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20480/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20544/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 2 [20608/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20672/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20736/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20800/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20864/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20928/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [20992/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21056/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21120/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21184/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21248/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21312/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21376/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 2 [21440/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21504/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21568/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21632/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21696/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21760/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21824/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21888/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [21952/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22016/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22080/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22144/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22208/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22272/87490 (25%)]\tLoss: 0.06\n","Train Epoch: 2 [22336/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22400/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22464/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22528/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22592/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22656/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22720/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22784/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22848/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22912/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [22976/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [23040/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [23104/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [23168/87490 (26%)]\tLoss: 0.06\n","Train Epoch: 2 [23232/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23296/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23360/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23424/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23488/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23552/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23616/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23680/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23744/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23808/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23872/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [23936/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [24000/87490 (27%)]\tLoss: 0.06\n","Train Epoch: 2 [24064/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24128/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24192/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24256/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24320/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24384/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24448/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24512/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24576/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24640/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24704/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24768/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24832/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24896/87490 (28%)]\tLoss: 0.06\n","Train Epoch: 2 [24960/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25024/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25088/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25152/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25216/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25280/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25344/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25408/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25472/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25536/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25600/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25664/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25728/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25792/87490 (29%)]\tLoss: 0.06\n","Train Epoch: 2 [25856/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [25920/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [25984/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26048/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26112/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26176/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26240/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26304/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26368/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26432/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26496/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26560/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26624/87490 (30%)]\tLoss: 0.06\n","Train Epoch: 2 [26688/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [26752/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [26816/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [26880/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [26944/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27008/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27072/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27136/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27200/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27264/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27328/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27392/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27456/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27520/87490 (31%)]\tLoss: 0.06\n","Train Epoch: 2 [27584/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27648/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27712/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27776/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27840/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27904/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [27968/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28032/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28096/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28160/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28224/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28288/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28352/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28416/87490 (32%)]\tLoss: 0.06\n","Train Epoch: 2 [28480/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28544/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28608/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28672/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28736/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28800/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28864/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28928/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [28992/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [29056/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [29120/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [29184/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [29248/87490 (33%)]\tLoss: 0.06\n","Train Epoch: 2 [29312/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29376/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29440/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29504/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29568/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29632/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29696/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29760/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29824/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29888/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [29952/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [30016/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [30080/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [30144/87490 (34%)]\tLoss: 0.06\n","Train Epoch: 2 [30208/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30272/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30336/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30400/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30464/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30528/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30592/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30656/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30720/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30784/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30848/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30912/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [30976/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [31040/87490 (35%)]\tLoss: 0.06\n","Train Epoch: 2 [31104/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31168/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31232/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31296/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31360/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31424/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31488/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31552/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31616/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31680/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31744/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31808/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31872/87490 (36%)]\tLoss: 0.06\n","Train Epoch: 2 [31936/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32000/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32064/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32128/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32192/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32256/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32320/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32384/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32448/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32512/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32576/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32640/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32704/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32768/87490 (37%)]\tLoss: 0.06\n","Train Epoch: 2 [32832/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [32896/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [32960/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33024/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33088/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33152/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33216/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33280/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33344/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33408/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33472/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33536/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33600/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33664/87490 (38%)]\tLoss: 0.06\n","Train Epoch: 2 [33728/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [33792/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [33856/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [33920/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [33984/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34048/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34112/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34176/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34240/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34304/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34368/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34432/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34496/87490 (39%)]\tLoss: 0.06\n","Train Epoch: 2 [34560/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34624/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34688/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34752/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34816/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34880/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [34944/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35008/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35072/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35136/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35200/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35264/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35328/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35392/87490 (40%)]\tLoss: 0.06\n","Train Epoch: 2 [35456/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35520/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35584/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35648/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35712/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35776/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35840/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35904/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [35968/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36032/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36096/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36160/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36224/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36288/87490 (41%)]\tLoss: 0.06\n","Train Epoch: 2 [36352/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36416/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36480/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36544/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36608/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36672/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36736/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36800/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36864/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36928/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [36992/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [37056/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [37120/87490 (42%)]\tLoss: 0.06\n","Train Epoch: 2 [37184/87490 (43%)]\tLoss: 0.06\n","Train Epoch: 2 [37248/87490 (43%)]\tLoss: 0.06\n","Train Epoch: 2 [37312/87490 (43%)]\tLoss: 0.06\n","Train Epoch: 2 [37376/87490 (43%)]\tLoss: 0.06\n","Train Epoch: 2 [37440/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37504/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37568/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37632/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37696/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37760/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37824/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37888/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [37952/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [38016/87490 (43%)]\tLoss: 0.05\n","Train Epoch: 2 [38080/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38144/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38208/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38272/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38336/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38400/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38464/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38528/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38592/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38656/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38720/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38784/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38848/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38912/87490 (44%)]\tLoss: 0.05\n","Train Epoch: 2 [38976/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39040/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39104/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39168/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39232/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39296/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39360/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39424/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39488/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39552/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39616/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39680/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39744/87490 (45%)]\tLoss: 0.05\n","Train Epoch: 2 [39808/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [39872/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [39936/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40000/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40064/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40128/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40192/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40256/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40320/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40384/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40448/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40512/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40576/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40640/87490 (46%)]\tLoss: 0.05\n","Train Epoch: 2 [40704/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [40768/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [40832/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [40896/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [40960/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41024/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41088/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41152/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41216/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41280/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41344/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41408/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41472/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41536/87490 (47%)]\tLoss: 0.05\n","Train Epoch: 2 [41600/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41664/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41728/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41792/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41856/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41920/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [41984/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [42048/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [42112/87490 (48%)]\tLoss: 0.05\n","Train Epoch: 2 [42176/87490 (48%)]\tLoss: 0.06\n","Train Epoch: 2 [42240/87490 (48%)]\tLoss: 0.06\n","Train Epoch: 2 [42304/87490 (48%)]\tLoss: 0.06\n","Train Epoch: 2 [42368/87490 (48%)]\tLoss: 0.06\n","Train Epoch: 2 [42432/87490 (48%)]\tLoss: 0.06\n","Train Epoch: 2 [42496/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [42560/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [42624/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [42688/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [42752/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [42816/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [42880/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [42944/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [43008/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [43072/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [43136/87490 (49%)]\tLoss: 0.06\n","Train Epoch: 2 [43200/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [43264/87490 (49%)]\tLoss: 0.05\n","Train Epoch: 2 [43328/87490 (50%)]\tLoss: 0.05\n","Train Epoch: 2 [43392/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43456/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43520/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43584/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43648/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43712/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43776/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43840/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43904/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [43968/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [44032/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [44096/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [44160/87490 (50%)]\tLoss: 0.06\n","Train Epoch: 2 [44224/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44288/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44352/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44416/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44480/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44544/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44608/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44672/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44736/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44800/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44864/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44928/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [44992/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [45056/87490 (51%)]\tLoss: 0.06\n","Train Epoch: 2 [45120/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45184/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45248/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45312/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45376/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45440/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45504/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45568/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45632/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45696/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45760/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45824/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45888/87490 (52%)]\tLoss: 0.06\n","Train Epoch: 2 [45952/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46016/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46080/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46144/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46208/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46272/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46336/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46400/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46464/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46528/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46592/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46656/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46720/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46784/87490 (53%)]\tLoss: 0.06\n","Train Epoch: 2 [46848/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [46912/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [46976/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47040/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47104/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47168/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47232/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47296/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47360/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47424/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47488/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47552/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47616/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47680/87490 (54%)]\tLoss: 0.06\n","Train Epoch: 2 [47744/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [47808/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [47872/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [47936/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48000/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48064/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48128/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48192/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48256/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48320/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48384/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48448/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48512/87490 (55%)]\tLoss: 0.06\n","Train Epoch: 2 [48576/87490 (56%)]\tLoss: 0.06\n","Train Epoch: 2 [48640/87490 (56%)]\tLoss: 0.06\n","Train Epoch: 2 [48704/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [48768/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [48832/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [48896/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [48960/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49024/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49088/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49152/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49216/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49280/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49344/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49408/87490 (56%)]\tLoss: 0.05\n","Train Epoch: 2 [49472/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49536/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49600/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49664/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49728/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49792/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49856/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49920/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [49984/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50048/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50112/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50176/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50240/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50304/87490 (57%)]\tLoss: 0.05\n","Train Epoch: 2 [50368/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50432/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50496/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50560/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50624/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50688/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50752/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50816/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50880/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [50944/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [51008/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [51072/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [51136/87490 (58%)]\tLoss: 0.05\n","Train Epoch: 2 [51200/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51264/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51328/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51392/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51456/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51520/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51584/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51648/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51712/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51776/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51840/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51904/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [51968/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [52032/87490 (59%)]\tLoss: 0.05\n","Train Epoch: 2 [52096/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52160/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52224/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52288/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52352/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52416/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52480/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52544/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52608/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52672/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52736/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52800/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52864/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52928/87490 (60%)]\tLoss: 0.05\n","Train Epoch: 2 [52992/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53056/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53120/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53184/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53248/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53312/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53376/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53440/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53504/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53568/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53632/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53696/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53760/87490 (61%)]\tLoss: 0.05\n","Train Epoch: 2 [53824/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [53888/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [53952/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54016/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54080/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54144/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54208/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54272/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54336/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54400/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54464/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54528/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54592/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54656/87490 (62%)]\tLoss: 0.05\n","Train Epoch: 2 [54720/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [54784/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [54848/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [54912/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [54976/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55040/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55104/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55168/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55232/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55296/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55360/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55424/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55488/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55552/87490 (63%)]\tLoss: 0.05\n","Train Epoch: 2 [55616/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [55680/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [55744/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [55808/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [55872/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [55936/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56000/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56064/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56128/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56192/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56256/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56320/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56384/87490 (64%)]\tLoss: 0.05\n","Train Epoch: 2 [56448/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56512/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56576/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56640/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56704/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56768/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56832/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56896/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [56960/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57024/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57088/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57152/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57216/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57280/87490 (65%)]\tLoss: 0.05\n","Train Epoch: 2 [57344/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57408/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57472/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57536/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57600/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57664/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57728/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57792/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57856/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57920/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [57984/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [58048/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [58112/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [58176/87490 (66%)]\tLoss: 0.05\n","Train Epoch: 2 [58240/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58304/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58368/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58432/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58496/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58560/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58624/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58688/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58752/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58816/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58880/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [58944/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [59008/87490 (67%)]\tLoss: 0.05\n","Train Epoch: 2 [59072/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59136/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59200/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59264/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59328/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59392/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59456/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59520/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59584/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59648/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59712/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59776/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59840/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59904/87490 (68%)]\tLoss: 0.05\n","Train Epoch: 2 [59968/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60032/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60096/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60160/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60224/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60288/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60352/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60416/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60480/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60544/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60608/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60672/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60736/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60800/87490 (69%)]\tLoss: 0.05\n","Train Epoch: 2 [60864/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [60928/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [60992/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61056/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61120/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61184/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61248/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61312/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61376/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61440/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61504/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61568/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61632/87490 (70%)]\tLoss: 0.05\n","Train Epoch: 2 [61696/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [61760/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [61824/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [61888/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [61952/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62016/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62080/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62144/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62208/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62272/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62336/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62400/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62464/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62528/87490 (71%)]\tLoss: 0.05\n","Train Epoch: 2 [62592/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62656/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62720/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62784/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62848/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62912/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [62976/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63040/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63104/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63168/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63232/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63296/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63360/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63424/87490 (72%)]\tLoss: 0.05\n","Train Epoch: 2 [63488/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63552/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63616/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63680/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63744/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63808/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63872/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [63936/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64000/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64064/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64128/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64192/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64256/87490 (73%)]\tLoss: 0.05\n","Train Epoch: 2 [64320/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64384/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64448/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64512/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64576/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64640/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64704/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64768/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64832/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64896/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [64960/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [65024/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [65088/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [65152/87490 (74%)]\tLoss: 0.05\n","Train Epoch: 2 [65216/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65280/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65344/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65408/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65472/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65536/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65600/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65664/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65728/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65792/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65856/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65920/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [65984/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [66048/87490 (75%)]\tLoss: 0.05\n","Train Epoch: 2 [66112/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66176/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66240/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66304/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66368/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66432/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66496/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66560/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66624/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66688/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66752/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66816/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66880/87490 (76%)]\tLoss: 0.05\n","Train Epoch: 2 [66944/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67008/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67072/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67136/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67200/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67264/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67328/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67392/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67456/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67520/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67584/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67648/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67712/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67776/87490 (77%)]\tLoss: 0.05\n","Train Epoch: 2 [67840/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [67904/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [67968/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68032/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68096/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68160/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68224/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68288/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68352/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68416/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68480/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68544/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68608/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68672/87490 (78%)]\tLoss: 0.05\n","Train Epoch: 2 [68736/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [68800/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [68864/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [68928/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [68992/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69056/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69120/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69184/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69248/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69312/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69376/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69440/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69504/87490 (79%)]\tLoss: 0.05\n","Train Epoch: 2 [69568/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69632/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69696/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69760/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69824/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69888/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [69952/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70016/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70080/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70144/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70208/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70272/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70336/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70400/87490 (80%)]\tLoss: 0.05\n","Train Epoch: 2 [70464/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70528/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70592/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70656/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70720/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70784/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70848/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70912/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [70976/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71040/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71104/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71168/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71232/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71296/87490 (81%)]\tLoss: 0.05\n","Train Epoch: 2 [71360/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71424/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71488/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71552/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71616/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71680/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71744/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71808/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71872/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [71936/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [72000/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [72064/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [72128/87490 (82%)]\tLoss: 0.05\n","Train Epoch: 2 [72192/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72256/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72320/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72384/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72448/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72512/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72576/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72640/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72704/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72768/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72832/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72896/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [72960/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [73024/87490 (83%)]\tLoss: 0.05\n","Train Epoch: 2 [73088/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73152/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73216/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73280/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73344/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73408/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73472/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73536/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73600/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73664/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73728/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73792/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73856/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73920/87490 (84%)]\tLoss: 0.05\n","Train Epoch: 2 [73984/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74048/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74112/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74176/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74240/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74304/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74368/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74432/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74496/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74560/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74624/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74688/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74752/87490 (85%)]\tLoss: 0.05\n","Train Epoch: 2 [74816/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [74880/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [74944/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75008/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75072/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75136/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75200/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75264/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75328/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75392/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75456/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75520/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75584/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75648/87490 (86%)]\tLoss: 0.05\n","Train Epoch: 2 [75712/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [75776/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [75840/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [75904/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [75968/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76032/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76096/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76160/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76224/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76288/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76352/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76416/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76480/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76544/87490 (87%)]\tLoss: 0.05\n","Train Epoch: 2 [76608/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76672/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76736/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76800/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76864/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76928/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [76992/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77056/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77120/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77184/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77248/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77312/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77376/87490 (88%)]\tLoss: 0.05\n","Train Epoch: 2 [77440/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77504/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77568/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77632/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77696/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77760/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77824/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77888/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [77952/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78016/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78080/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78144/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78208/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78272/87490 (89%)]\tLoss: 0.05\n","Train Epoch: 2 [78336/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78400/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78464/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78528/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78592/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78656/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78720/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78784/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78848/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78912/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [78976/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [79040/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [79104/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [79168/87490 (90%)]\tLoss: 0.05\n","Train Epoch: 2 [79232/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79296/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79360/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79424/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79488/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79552/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79616/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79680/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79744/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79808/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79872/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [79936/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [80000/87490 (91%)]\tLoss: 0.05\n","Train Epoch: 2 [80064/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80128/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80192/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80256/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80320/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80384/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80448/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80512/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80576/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80640/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80704/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80768/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80832/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80896/87490 (92%)]\tLoss: 0.05\n","Train Epoch: 2 [80960/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81024/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81088/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81152/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81216/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81280/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81344/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81408/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81472/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81536/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81600/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81664/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81728/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81792/87490 (93%)]\tLoss: 0.05\n","Train Epoch: 2 [81856/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [81920/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [81984/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82048/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82112/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82176/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82240/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82304/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82368/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82432/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82496/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82560/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82624/87490 (94%)]\tLoss: 0.05\n","Train Epoch: 2 [82688/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [82752/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [82816/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [82880/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [82944/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83008/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83072/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83136/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83200/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83264/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83328/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83392/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83456/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83520/87490 (95%)]\tLoss: 0.05\n","Train Epoch: 2 [83584/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83648/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83712/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83776/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83840/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83904/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [83968/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84032/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84096/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84160/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84224/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84288/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84352/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84416/87490 (96%)]\tLoss: 0.05\n","Train Epoch: 2 [84480/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84544/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84608/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84672/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84736/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84800/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84864/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84928/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [84992/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [85056/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [85120/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [85184/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [85248/87490 (97%)]\tLoss: 0.05\n","Train Epoch: 2 [85312/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85376/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85440/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85504/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85568/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85632/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85696/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85760/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85824/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85888/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [85952/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [86016/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [86080/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [86144/87490 (98%)]\tLoss: 0.05\n","Train Epoch: 2 [86208/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86272/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86336/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86400/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86464/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86528/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86592/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86656/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86720/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86784/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86848/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86912/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [86976/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [87040/87490 (99%)]\tLoss: 0.05\n","Train Epoch: 2 [87104/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87168/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87232/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87296/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87360/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87424/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [87488/87490 (100%)]\tLoss: 0.05\n","Train Epoch: 2 [2736/87490 (3%)]\tLoss: 0.06\n","evaluating trained model ...\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 63/64 (98%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 55/64 (86%) ; correctly guess bot: 55/64 (86%)\n","Train Epoch: 3 [64/87490 (0%)]\tLoss: 0.31\n","Train Epoch: 3 [128/87490 (0%)]\tLoss: 0.16\n","Train Epoch: 3 [192/87490 (0%)]\tLoss: 0.14\n","Train Epoch: 3 [256/87490 (0%)]\tLoss: 0.15\n","Train Epoch: 3 [320/87490 (0%)]\tLoss: 0.42\n","Train Epoch: 3 [384/87490 (0%)]\tLoss: 0.39\n","Train Epoch: 3 [448/87490 (1%)]\tLoss: 0.34\n","Train Epoch: 3 [512/87490 (1%)]\tLoss: 0.31\n","Train Epoch: 3 [576/87490 (1%)]\tLoss: 0.28\n","Train Epoch: 3 [640/87490 (1%)]\tLoss: 0.26\n","Train Epoch: 3 [704/87490 (1%)]\tLoss: 0.24\n","Train Epoch: 3 [768/87490 (1%)]\tLoss: 0.22\n","Train Epoch: 3 [832/87490 (1%)]\tLoss: 0.21\n","Train Epoch: 3 [896/87490 (1%)]\tLoss: 0.19\n","Train Epoch: 3 [960/87490 (1%)]\tLoss: 0.18\n","Train Epoch: 3 [1024/87490 (1%)]\tLoss: 0.17\n","Train Epoch: 3 [1088/87490 (1%)]\tLoss: 0.16\n","Train Epoch: 3 [1152/87490 (1%)]\tLoss: 0.16\n","Train Epoch: 3 [1216/87490 (1%)]\tLoss: 0.15\n","Train Epoch: 3 [1280/87490 (1%)]\tLoss: 0.14\n","Train Epoch: 3 [1344/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 3 [1408/87490 (2%)]\tLoss: 0.13\n","Train Epoch: 3 [1472/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 3 [1536/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 3 [1600/87490 (2%)]\tLoss: 0.13\n","Train Epoch: 3 [1664/87490 (2%)]\tLoss: 0.13\n","Train Epoch: 3 [1728/87490 (2%)]\tLoss: 0.13\n","Train Epoch: 3 [1792/87490 (2%)]\tLoss: 0.12\n","Train Epoch: 3 [1856/87490 (2%)]\tLoss: 0.12\n","Train Epoch: 3 [1920/87490 (2%)]\tLoss: 0.11\n","Train Epoch: 3 [1984/87490 (2%)]\tLoss: 0.11\n","Train Epoch: 3 [2048/87490 (2%)]\tLoss: 0.11\n","Train Epoch: 3 [2112/87490 (2%)]\tLoss: 0.11\n","Train Epoch: 3 [2176/87490 (2%)]\tLoss: 0.10\n","Train Epoch: 3 [2240/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 3 [2304/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 3 [2368/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 3 [2432/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2496/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2560/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2624/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2688/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2752/87490 (3%)]\tLoss: 0.08\n","Train Epoch: 3 [2816/87490 (3%)]\tLoss: 0.09\n","Train Epoch: 3 [2880/87490 (3%)]\tLoss: 0.08\n","Train Epoch: 3 [2944/87490 (3%)]\tLoss: 0.08\n","Train Epoch: 3 [3008/87490 (3%)]\tLoss: 0.08\n","Train Epoch: 3 [3072/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3136/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3200/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3264/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3328/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3392/87490 (4%)]\tLoss: 0.07\n","Train Epoch: 3 [3456/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3520/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3584/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3648/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 3 [3712/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 3 [3776/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 3 [3840/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 3 [3904/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 3 [3968/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4032/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4096/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4160/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4224/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4288/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4352/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4416/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4480/87490 (5%)]\tLoss: 0.09\n","Train Epoch: 3 [4544/87490 (5%)]\tLoss: 0.08\n","Train Epoch: 3 [4608/87490 (5%)]\tLoss: 0.08\n","Train Epoch: 3 [4672/87490 (5%)]\tLoss: 0.08\n","Train Epoch: 3 [4736/87490 (5%)]\tLoss: 0.08\n","Train Epoch: 3 [4800/87490 (5%)]\tLoss: 0.08\n","Train Epoch: 3 [4864/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [4928/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [4992/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5056/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5120/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5184/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5248/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5312/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5376/87490 (6%)]\tLoss: 0.09\n","Train Epoch: 3 [5440/87490 (6%)]\tLoss: 0.09\n","Train Epoch: 3 [5504/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5568/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5632/87490 (6%)]\tLoss: 0.08\n","Train Epoch: 3 [5696/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [5760/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [5824/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [5888/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [5952/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6016/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6080/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6144/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6208/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6272/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6336/87490 (7%)]\tLoss: 0.08\n","Train Epoch: 3 [6400/87490 (7%)]\tLoss: 0.07\n","Train Epoch: 3 [6464/87490 (7%)]\tLoss: 0.07\n","Train Epoch: 3 [6528/87490 (7%)]\tLoss: 0.07\n","Train Epoch: 3 [6592/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6656/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6720/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6784/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6848/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6912/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [6976/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7040/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7104/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7168/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7232/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7296/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7360/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7424/87490 (8%)]\tLoss: 0.07\n","Train Epoch: 3 [7488/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7552/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7616/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7680/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7744/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7808/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7872/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [7936/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [8000/87490 (9%)]\tLoss: 0.06\n","Train Epoch: 3 [8064/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [8128/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [8192/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [8256/87490 (9%)]\tLoss: 0.07\n","Train Epoch: 3 [8320/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8384/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8448/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [8512/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [8576/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [8640/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8704/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8768/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8832/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8896/87490 (10%)]\tLoss: 0.07\n","Train Epoch: 3 [8960/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [9024/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [9088/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [9152/87490 (10%)]\tLoss: 0.06\n","Train Epoch: 3 [9216/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9280/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9344/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9408/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9472/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9536/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9600/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9664/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9728/87490 (11%)]\tLoss: 0.06\n","Train Epoch: 3 [9792/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 3 [9856/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 3 [9920/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 3 [9984/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 3 [10048/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 3 [10112/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10176/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10240/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10304/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10368/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10432/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10496/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10560/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10624/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10688/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10752/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10816/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10880/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 3 [10944/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11008/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11072/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11136/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11200/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11264/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11328/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11392/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11456/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11520/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11584/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 3 [11648/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11712/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11776/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 3 [11840/87490 (14%)]\tLoss: 0.07\n","Train Epoch: 3 [11904/87490 (14%)]\tLoss: 0.07\n","Train Epoch: 3 [11968/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12032/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12096/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12160/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12224/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12288/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12352/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12416/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12480/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12544/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12608/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12672/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 3 [12736/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [12800/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [12864/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [12928/87490 (15%)]\tLoss: 0.07\n","Train Epoch: 3 [12992/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13056/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13120/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13184/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13248/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13312/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13376/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13440/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13504/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 3 [13568/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13632/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13696/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13760/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13824/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13888/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [13952/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14016/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14080/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14144/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14208/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14272/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14336/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14400/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 3 [14464/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14528/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14592/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14656/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14720/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14784/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14848/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14912/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [14976/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15040/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15104/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15168/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15232/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15296/87490 (17%)]\tLoss: 0.06\n","Train Epoch: 3 [15360/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15424/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15488/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15552/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15616/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15680/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15744/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15808/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15872/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [15936/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [16000/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [16064/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [16128/87490 (18%)]\tLoss: 0.06\n","Train Epoch: 3 [16192/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16256/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16320/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16384/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16448/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16512/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16576/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16640/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16704/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16768/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16832/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16896/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [16960/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [17024/87490 (19%)]\tLoss: 0.06\n","Train Epoch: 3 [17088/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17152/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17216/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17280/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17344/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17408/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17472/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17536/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17600/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17664/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17728/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17792/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17856/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17920/87490 (20%)]\tLoss: 0.06\n","Train Epoch: 3 [17984/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18048/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18112/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18176/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18240/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18304/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18368/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18432/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18496/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18560/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18624/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18688/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18752/87490 (21%)]\tLoss: 0.06\n","Train Epoch: 3 [18816/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [18880/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [18944/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19008/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19072/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19136/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19200/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19264/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19328/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19392/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19456/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19520/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19584/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19648/87490 (22%)]\tLoss: 0.06\n","Train Epoch: 3 [19712/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [19776/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [19840/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [19904/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [19968/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20032/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20096/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20160/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20224/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20288/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20352/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20416/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20480/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20544/87490 (23%)]\tLoss: 0.06\n","Train Epoch: 3 [20608/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 3 [20672/87490 (24%)]\tLoss: 0.06\n","Train Epoch: 3 [20736/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [20800/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [20864/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [20928/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [20992/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21056/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21120/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21184/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21248/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21312/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21376/87490 (24%)]\tLoss: 0.05\n","Train Epoch: 3 [21440/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21504/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21568/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21632/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21696/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21760/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21824/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21888/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [21952/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22016/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22080/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22144/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22208/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22272/87490 (25%)]\tLoss: 0.05\n","Train Epoch: 3 [22336/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22400/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22464/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22528/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22592/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22656/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22720/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22784/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22848/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22912/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [22976/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [23040/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [23104/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [23168/87490 (26%)]\tLoss: 0.05\n","Train Epoch: 3 [23232/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23296/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23360/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23424/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23488/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23552/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23616/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23680/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23744/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23808/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23872/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [23936/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [24000/87490 (27%)]\tLoss: 0.05\n","Train Epoch: 3 [24064/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24128/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24192/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24256/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24320/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24384/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24448/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24512/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24576/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24640/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24704/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24768/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24832/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24896/87490 (28%)]\tLoss: 0.05\n","Train Epoch: 3 [24960/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25024/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25088/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25152/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25216/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25280/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25344/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25408/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25472/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25536/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25600/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25664/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25728/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25792/87490 (29%)]\tLoss: 0.05\n","Train Epoch: 3 [25856/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [25920/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [25984/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26048/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26112/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26176/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26240/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26304/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26368/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26432/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26496/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26560/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26624/87490 (30%)]\tLoss: 0.05\n","Train Epoch: 3 [26688/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [26752/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [26816/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [26880/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [26944/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27008/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27072/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27136/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27200/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27264/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27328/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27392/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27456/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27520/87490 (31%)]\tLoss: 0.04\n","Train Epoch: 3 [27584/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27648/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27712/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27776/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27840/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27904/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [27968/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28032/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28096/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28160/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28224/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28288/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28352/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28416/87490 (32%)]\tLoss: 0.04\n","Train Epoch: 3 [28480/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28544/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28608/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28672/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28736/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28800/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28864/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28928/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [28992/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [29056/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [29120/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [29184/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [29248/87490 (33%)]\tLoss: 0.04\n","Train Epoch: 3 [29312/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29376/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29440/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29504/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29568/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29632/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29696/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29760/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29824/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29888/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [29952/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [30016/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [30080/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [30144/87490 (34%)]\tLoss: 0.04\n","Train Epoch: 3 [30208/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30272/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30336/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30400/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30464/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30528/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30592/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30656/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30720/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30784/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30848/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30912/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [30976/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [31040/87490 (35%)]\tLoss: 0.04\n","Train Epoch: 3 [31104/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31168/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31232/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31296/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31360/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31424/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31488/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31552/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31616/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31680/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31744/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31808/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31872/87490 (36%)]\tLoss: 0.04\n","Train Epoch: 3 [31936/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32000/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32064/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32128/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32192/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32256/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32320/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32384/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32448/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32512/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32576/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32640/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32704/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32768/87490 (37%)]\tLoss: 0.04\n","Train Epoch: 3 [32832/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [32896/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [32960/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33024/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33088/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33152/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33216/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33280/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33344/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33408/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33472/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33536/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33600/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33664/87490 (38%)]\tLoss: 0.04\n","Train Epoch: 3 [33728/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [33792/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [33856/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [33920/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [33984/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34048/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34112/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34176/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34240/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34304/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34368/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34432/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34496/87490 (39%)]\tLoss: 0.04\n","Train Epoch: 3 [34560/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34624/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34688/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34752/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34816/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34880/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [34944/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35008/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35072/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35136/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35200/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35264/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35328/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35392/87490 (40%)]\tLoss: 0.04\n","Train Epoch: 3 [35456/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35520/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35584/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35648/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35712/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35776/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35840/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35904/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [35968/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36032/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36096/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36160/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36224/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36288/87490 (41%)]\tLoss: 0.04\n","Train Epoch: 3 [36352/87490 (42%)]\tLoss: 0.04\n","Train Epoch: 3 [36416/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36480/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36544/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36608/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36672/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36736/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36800/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36864/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36928/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [36992/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [37056/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [37120/87490 (42%)]\tLoss: 0.03\n","Train Epoch: 3 [37184/87490 (43%)]\tLoss: 0.03\n","Train Epoch: 3 [37248/87490 (43%)]\tLoss: 0.03\n","Train Epoch: 3 [37312/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37376/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37440/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37504/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37568/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37632/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37696/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37760/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37824/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37888/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [37952/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [38016/87490 (43%)]\tLoss: 0.04\n","Train Epoch: 3 [38080/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38144/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38208/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38272/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38336/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38400/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38464/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38528/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38592/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38656/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38720/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38784/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38848/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38912/87490 (44%)]\tLoss: 0.04\n","Train Epoch: 3 [38976/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39040/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39104/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39168/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39232/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39296/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39360/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39424/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39488/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39552/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39616/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39680/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39744/87490 (45%)]\tLoss: 0.04\n","Train Epoch: 3 [39808/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [39872/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [39936/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40000/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40064/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40128/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40192/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40256/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40320/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40384/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40448/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40512/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40576/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40640/87490 (46%)]\tLoss: 0.04\n","Train Epoch: 3 [40704/87490 (47%)]\tLoss: 0.04\n","Train Epoch: 3 [40768/87490 (47%)]\tLoss: 0.04\n","Train Epoch: 3 [40832/87490 (47%)]\tLoss: 0.04\n","Train Epoch: 3 [40896/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [40960/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41024/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41088/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41152/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41216/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41280/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41344/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41408/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41472/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41536/87490 (47%)]\tLoss: 0.03\n","Train Epoch: 3 [41600/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41664/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41728/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41792/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41856/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41920/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [41984/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42048/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42112/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42176/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42240/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42304/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42368/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42432/87490 (48%)]\tLoss: 0.03\n","Train Epoch: 3 [42496/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42560/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42624/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42688/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42752/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42816/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42880/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [42944/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43008/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43072/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43136/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43200/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43264/87490 (49%)]\tLoss: 0.03\n","Train Epoch: 3 [43328/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43392/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43456/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43520/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43584/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43648/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43712/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43776/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43840/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43904/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [43968/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [44032/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [44096/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [44160/87490 (50%)]\tLoss: 0.03\n","Train Epoch: 3 [44224/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44288/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44352/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44416/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44480/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44544/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44608/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44672/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44736/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44800/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44864/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44928/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [44992/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [45056/87490 (51%)]\tLoss: 0.03\n","Train Epoch: 3 [45120/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45184/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45248/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45312/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45376/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45440/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45504/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45568/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45632/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45696/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45760/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45824/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45888/87490 (52%)]\tLoss: 0.03\n","Train Epoch: 3 [45952/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46016/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46080/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46144/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46208/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46272/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46336/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46400/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46464/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46528/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46592/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46656/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46720/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46784/87490 (53%)]\tLoss: 0.03\n","Train Epoch: 3 [46848/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [46912/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [46976/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47040/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47104/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47168/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47232/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47296/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47360/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47424/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47488/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47552/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47616/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47680/87490 (54%)]\tLoss: 0.03\n","Train Epoch: 3 [47744/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [47808/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [47872/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [47936/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48000/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48064/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48128/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48192/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48256/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48320/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48384/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48448/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48512/87490 (55%)]\tLoss: 0.03\n","Train Epoch: 3 [48576/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48640/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48704/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48768/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48832/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48896/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [48960/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49024/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49088/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49152/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49216/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49280/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49344/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49408/87490 (56%)]\tLoss: 0.03\n","Train Epoch: 3 [49472/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49536/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49600/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49664/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49728/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49792/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49856/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49920/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [49984/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50048/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50112/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50176/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50240/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50304/87490 (57%)]\tLoss: 0.03\n","Train Epoch: 3 [50368/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50432/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50496/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50560/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50624/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50688/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50752/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50816/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50880/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [50944/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [51008/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [51072/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [51136/87490 (58%)]\tLoss: 0.03\n","Train Epoch: 3 [51200/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51264/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51328/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51392/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51456/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51520/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51584/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51648/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51712/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51776/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51840/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51904/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [51968/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [52032/87490 (59%)]\tLoss: 0.03\n","Train Epoch: 3 [52096/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52160/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52224/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52288/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52352/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52416/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52480/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52544/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52608/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52672/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52736/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52800/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52864/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52928/87490 (60%)]\tLoss: 0.03\n","Train Epoch: 3 [52992/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53056/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53120/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53184/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53248/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53312/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53376/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53440/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53504/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53568/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53632/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53696/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53760/87490 (61%)]\tLoss: 0.03\n","Train Epoch: 3 [53824/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [53888/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [53952/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54016/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54080/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54144/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54208/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54272/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54336/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54400/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54464/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54528/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54592/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54656/87490 (62%)]\tLoss: 0.03\n","Train Epoch: 3 [54720/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [54784/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [54848/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [54912/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [54976/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55040/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55104/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55168/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55232/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55296/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55360/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55424/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55488/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55552/87490 (63%)]\tLoss: 0.03\n","Train Epoch: 3 [55616/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [55680/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [55744/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [55808/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [55872/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [55936/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56000/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56064/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56128/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56192/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56256/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56320/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56384/87490 (64%)]\tLoss: 0.03\n","Train Epoch: 3 [56448/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56512/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56576/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56640/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56704/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56768/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56832/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56896/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [56960/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57024/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57088/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57152/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57216/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57280/87490 (65%)]\tLoss: 0.03\n","Train Epoch: 3 [57344/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57408/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57472/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57536/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57600/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57664/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57728/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57792/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57856/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57920/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [57984/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [58048/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [58112/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [58176/87490 (66%)]\tLoss: 0.03\n","Train Epoch: 3 [58240/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58304/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58368/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58432/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58496/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58560/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58624/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58688/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58752/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58816/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58880/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [58944/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [59008/87490 (67%)]\tLoss: 0.03\n","Train Epoch: 3 [59072/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59136/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59200/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59264/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59328/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59392/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59456/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59520/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59584/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59648/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59712/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59776/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59840/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59904/87490 (68%)]\tLoss: 0.03\n","Train Epoch: 3 [59968/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60032/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60096/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60160/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60224/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60288/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60352/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60416/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60480/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60544/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60608/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60672/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60736/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60800/87490 (69%)]\tLoss: 0.03\n","Train Epoch: 3 [60864/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [60928/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [60992/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61056/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61120/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61184/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61248/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61312/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61376/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61440/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61504/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61568/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61632/87490 (70%)]\tLoss: 0.03\n","Train Epoch: 3 [61696/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [61760/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [61824/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [61888/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [61952/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62016/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62080/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62144/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62208/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62272/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62336/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62400/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62464/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62528/87490 (71%)]\tLoss: 0.03\n","Train Epoch: 3 [62592/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62656/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62720/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62784/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62848/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62912/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [62976/87490 (72%)]\tLoss: 0.03\n","Train Epoch: 3 [63040/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63104/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63168/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63232/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63296/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63360/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63424/87490 (72%)]\tLoss: 0.02\n","Train Epoch: 3 [63488/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63552/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63616/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63680/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63744/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63808/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63872/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [63936/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64000/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64064/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64128/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64192/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64256/87490 (73%)]\tLoss: 0.02\n","Train Epoch: 3 [64320/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64384/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64448/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64512/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64576/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64640/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64704/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64768/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64832/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64896/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [64960/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [65024/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [65088/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [65152/87490 (74%)]\tLoss: 0.02\n","Train Epoch: 3 [65216/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65280/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65344/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65408/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65472/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65536/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65600/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65664/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65728/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65792/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65856/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65920/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [65984/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [66048/87490 (75%)]\tLoss: 0.02\n","Train Epoch: 3 [66112/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66176/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66240/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66304/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66368/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66432/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66496/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66560/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66624/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66688/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66752/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66816/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66880/87490 (76%)]\tLoss: 0.02\n","Train Epoch: 3 [66944/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67008/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67072/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67136/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67200/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67264/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67328/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67392/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67456/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67520/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67584/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67648/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67712/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67776/87490 (77%)]\tLoss: 0.02\n","Train Epoch: 3 [67840/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [67904/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [67968/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68032/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68096/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68160/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68224/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68288/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68352/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68416/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68480/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68544/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68608/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68672/87490 (78%)]\tLoss: 0.02\n","Train Epoch: 3 [68736/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [68800/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [68864/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [68928/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [68992/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69056/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69120/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69184/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69248/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69312/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69376/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69440/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69504/87490 (79%)]\tLoss: 0.02\n","Train Epoch: 3 [69568/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69632/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69696/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69760/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69824/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69888/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [69952/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70016/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70080/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70144/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70208/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70272/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70336/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70400/87490 (80%)]\tLoss: 0.02\n","Train Epoch: 3 [70464/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70528/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70592/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70656/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70720/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70784/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70848/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70912/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [70976/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71040/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71104/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71168/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71232/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71296/87490 (81%)]\tLoss: 0.02\n","Train Epoch: 3 [71360/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71424/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71488/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71552/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71616/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71680/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71744/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71808/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71872/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [71936/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [72000/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [72064/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [72128/87490 (82%)]\tLoss: 0.02\n","Train Epoch: 3 [72192/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72256/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72320/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72384/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72448/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72512/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72576/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72640/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72704/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72768/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72832/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72896/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [72960/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [73024/87490 (83%)]\tLoss: 0.02\n","Train Epoch: 3 [73088/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73152/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73216/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73280/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73344/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73408/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73472/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73536/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73600/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73664/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73728/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73792/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73856/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73920/87490 (84%)]\tLoss: 0.02\n","Train Epoch: 3 [73984/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74048/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74112/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74176/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74240/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74304/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74368/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74432/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74496/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74560/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74624/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74688/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74752/87490 (85%)]\tLoss: 0.02\n","Train Epoch: 3 [74816/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [74880/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [74944/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75008/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75072/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75136/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75200/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75264/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75328/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75392/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75456/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75520/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75584/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75648/87490 (86%)]\tLoss: 0.02\n","Train Epoch: 3 [75712/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [75776/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [75840/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [75904/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [75968/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76032/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76096/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76160/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76224/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76288/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76352/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76416/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76480/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76544/87490 (87%)]\tLoss: 0.02\n","Train Epoch: 3 [76608/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76672/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76736/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76800/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76864/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76928/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [76992/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77056/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77120/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77184/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77248/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77312/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77376/87490 (88%)]\tLoss: 0.02\n","Train Epoch: 3 [77440/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77504/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77568/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77632/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77696/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77760/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77824/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77888/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [77952/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78016/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78080/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78144/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78208/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78272/87490 (89%)]\tLoss: 0.02\n","Train Epoch: 3 [78336/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78400/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78464/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78528/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78592/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78656/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78720/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78784/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78848/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78912/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [78976/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [79040/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [79104/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [79168/87490 (90%)]\tLoss: 0.02\n","Train Epoch: 3 [79232/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79296/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79360/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79424/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79488/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79552/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79616/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79680/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79744/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79808/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79872/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [79936/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [80000/87490 (91%)]\tLoss: 0.02\n","Train Epoch: 3 [80064/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80128/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80192/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80256/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80320/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80384/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80448/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80512/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80576/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80640/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80704/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80768/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80832/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80896/87490 (92%)]\tLoss: 0.02\n","Train Epoch: 3 [80960/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81024/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81088/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81152/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81216/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81280/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81344/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81408/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81472/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81536/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81600/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81664/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81728/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81792/87490 (93%)]\tLoss: 0.02\n","Train Epoch: 3 [81856/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [81920/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [81984/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82048/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82112/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82176/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82240/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82304/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82368/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82432/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82496/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82560/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82624/87490 (94%)]\tLoss: 0.02\n","Train Epoch: 3 [82688/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [82752/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [82816/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [82880/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [82944/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83008/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83072/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83136/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83200/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83264/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83328/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83392/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83456/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83520/87490 (95%)]\tLoss: 0.02\n","Train Epoch: 3 [83584/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83648/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83712/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83776/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83840/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83904/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [83968/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84032/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84096/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84160/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84224/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84288/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84352/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84416/87490 (96%)]\tLoss: 0.02\n","Train Epoch: 3 [84480/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84544/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84608/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84672/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84736/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84800/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84864/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84928/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [84992/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [85056/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [85120/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [85184/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [85248/87490 (97%)]\tLoss: 0.02\n","Train Epoch: 3 [85312/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85376/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85440/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85504/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85568/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85632/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85696/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85760/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85824/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85888/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [85952/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [86016/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [86080/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [86144/87490 (98%)]\tLoss: 0.02\n","Train Epoch: 3 [86208/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86272/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86336/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86400/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86464/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86528/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86592/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86656/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86720/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86784/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86848/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86912/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [86976/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [87040/87490 (99%)]\tLoss: 0.02\n","Train Epoch: 3 [87104/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87168/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87232/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87296/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87360/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87424/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [87488/87490 (100%)]\tLoss: 0.02\n","Train Epoch: 3 [2736/87490 (3%)]\tLoss: 0.03\n","evaluating trained model ...\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 62/64 (97%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 63/64 (98%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 55/64 (86%) ; correctly guess bot: 55/64 (86%)\n","Train Epoch: 4 [64/87490 (0%)]\tLoss: 0.00\n","Train Epoch: 4 [128/87490 (0%)]\tLoss: 2.77\n","Train Epoch: 4 [192/87490 (0%)]\tLoss: 2.48\n","Train Epoch: 4 [256/87490 (0%)]\tLoss: 1.95\n","Train Epoch: 4 [320/87490 (0%)]\tLoss: 1.57\n","Train Epoch: 4 [384/87490 (0%)]\tLoss: 1.31\n","Train Epoch: 4 [448/87490 (1%)]\tLoss: 1.16\n","Train Epoch: 4 [512/87490 (1%)]\tLoss: 1.02\n","Train Epoch: 4 [576/87490 (1%)]\tLoss: 0.91\n","Train Epoch: 4 [640/87490 (1%)]\tLoss: 0.82\n","Train Epoch: 4 [704/87490 (1%)]\tLoss: 0.75\n","Train Epoch: 4 [768/87490 (1%)]\tLoss: 0.70\n","Train Epoch: 4 [832/87490 (1%)]\tLoss: 0.66\n","Train Epoch: 4 [896/87490 (1%)]\tLoss: 0.62\n","Train Epoch: 4 [960/87490 (1%)]\tLoss: 0.58\n","Train Epoch: 4 [1024/87490 (1%)]\tLoss: 0.54\n","Train Epoch: 4 [1088/87490 (1%)]\tLoss: 0.51\n","Train Epoch: 4 [1152/87490 (1%)]\tLoss: 0.48\n","Train Epoch: 4 [1216/87490 (1%)]\tLoss: 0.46\n","Train Epoch: 4 [1280/87490 (1%)]\tLoss: 0.45\n","Train Epoch: 4 [1344/87490 (2%)]\tLoss: 0.43\n","Train Epoch: 4 [1408/87490 (2%)]\tLoss: 0.41\n","Train Epoch: 4 [1472/87490 (2%)]\tLoss: 0.40\n","Train Epoch: 4 [1536/87490 (2%)]\tLoss: 0.38\n","Train Epoch: 4 [1600/87490 (2%)]\tLoss: 0.37\n","Train Epoch: 4 [1664/87490 (2%)]\tLoss: 0.35\n","Train Epoch: 4 [1728/87490 (2%)]\tLoss: 0.34\n","Train Epoch: 4 [1792/87490 (2%)]\tLoss: 0.33\n","Train Epoch: 4 [1856/87490 (2%)]\tLoss: 0.32\n","Train Epoch: 4 [1920/87490 (2%)]\tLoss: 0.31\n","Train Epoch: 4 [1984/87490 (2%)]\tLoss: 0.30\n","Train Epoch: 4 [2048/87490 (2%)]\tLoss: 0.29\n","Train Epoch: 4 [2112/87490 (2%)]\tLoss: 0.28\n","Train Epoch: 4 [2176/87490 (2%)]\tLoss: 0.27\n","Train Epoch: 4 [2240/87490 (3%)]\tLoss: 0.26\n","Train Epoch: 4 [2304/87490 (3%)]\tLoss: 0.26\n","Train Epoch: 4 [2368/87490 (3%)]\tLoss: 0.25\n","Train Epoch: 4 [2432/87490 (3%)]\tLoss: 0.24\n","Train Epoch: 4 [2496/87490 (3%)]\tLoss: 0.24\n","Train Epoch: 4 [2560/87490 (3%)]\tLoss: 0.23\n","Train Epoch: 4 [2624/87490 (3%)]\tLoss: 0.23\n","Train Epoch: 4 [2688/87490 (3%)]\tLoss: 0.22\n","Train Epoch: 4 [2752/87490 (3%)]\tLoss: 0.22\n","Train Epoch: 4 [2816/87490 (3%)]\tLoss: 0.21\n","Train Epoch: 4 [2880/87490 (3%)]\tLoss: 0.21\n","Train Epoch: 4 [2944/87490 (3%)]\tLoss: 0.20\n","Train Epoch: 4 [3008/87490 (3%)]\tLoss: 0.20\n","Train Epoch: 4 [3072/87490 (4%)]\tLoss: 0.19\n","Train Epoch: 4 [3136/87490 (4%)]\tLoss: 0.19\n","Train Epoch: 4 [3200/87490 (4%)]\tLoss: 0.19\n","Train Epoch: 4 [3264/87490 (4%)]\tLoss: 0.18\n","Train Epoch: 4 [3328/87490 (4%)]\tLoss: 0.18\n","Train Epoch: 4 [3392/87490 (4%)]\tLoss: 0.18\n","Train Epoch: 4 [3456/87490 (4%)]\tLoss: 0.17\n","Train Epoch: 4 [3520/87490 (4%)]\tLoss: 0.17\n","Train Epoch: 4 [3584/87490 (4%)]\tLoss: 0.17\n","Train Epoch: 4 [3648/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 4 [3712/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 4 [3776/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 4 [3840/87490 (4%)]\tLoss: 0.16\n","Train Epoch: 4 [3904/87490 (4%)]\tLoss: 0.15\n","Train Epoch: 4 [3968/87490 (5%)]\tLoss: 0.15\n","Train Epoch: 4 [4032/87490 (5%)]\tLoss: 0.15\n","Train Epoch: 4 [4096/87490 (5%)]\tLoss: 0.15\n","Train Epoch: 4 [4160/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 4 [4224/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 4 [4288/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 4 [4352/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 4 [4416/87490 (5%)]\tLoss: 0.14\n","Train Epoch: 4 [4480/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4544/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4608/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4672/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4736/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4800/87490 (5%)]\tLoss: 0.13\n","Train Epoch: 4 [4864/87490 (6%)]\tLoss: 0.14\n","Train Epoch: 4 [4928/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [4992/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [5056/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [5120/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [5184/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [5248/87490 (6%)]\tLoss: 0.13\n","Train Epoch: 4 [5312/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5376/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5440/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5504/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5568/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5632/87490 (6%)]\tLoss: 0.12\n","Train Epoch: 4 [5696/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 4 [5760/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 4 [5824/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 4 [5888/87490 (7%)]\tLoss: 0.12\n","Train Epoch: 4 [5952/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6016/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6080/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6144/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6208/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6272/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6336/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6400/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6464/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6528/87490 (7%)]\tLoss: 0.11\n","Train Epoch: 4 [6592/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6656/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6720/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6784/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6848/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6912/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [6976/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7040/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7104/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7168/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7232/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7296/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7360/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7424/87490 (8%)]\tLoss: 0.10\n","Train Epoch: 4 [7488/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 4 [7552/87490 (9%)]\tLoss: 0.10\n","Train Epoch: 4 [7616/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [7680/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [7744/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [7808/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [7872/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [7936/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8000/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8064/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8128/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8192/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8256/87490 (9%)]\tLoss: 0.09\n","Train Epoch: 4 [8320/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 4 [8384/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 4 [8448/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 4 [8512/87490 (10%)]\tLoss: 0.09\n","Train Epoch: 4 [8576/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8640/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8704/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8768/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8832/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8896/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [8960/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [9024/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [9088/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [9152/87490 (10%)]\tLoss: 0.08\n","Train Epoch: 4 [9216/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9280/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9344/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9408/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9472/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9536/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9600/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9664/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9728/87490 (11%)]\tLoss: 0.08\n","Train Epoch: 4 [9792/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 4 [9856/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 4 [9920/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 4 [9984/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 4 [10048/87490 (11%)]\tLoss: 0.07\n","Train Epoch: 4 [10112/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10176/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10240/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10304/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10368/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10432/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10496/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10560/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10624/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10688/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10752/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10816/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10880/87490 (12%)]\tLoss: 0.07\n","Train Epoch: 4 [10944/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11008/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11072/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11136/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11200/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11264/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11328/87490 (13%)]\tLoss: 0.07\n","Train Epoch: 4 [11392/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11456/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11520/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11584/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11648/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11712/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11776/87490 (13%)]\tLoss: 0.06\n","Train Epoch: 4 [11840/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [11904/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [11968/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12032/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12096/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12160/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12224/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12288/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12352/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12416/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12480/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12544/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12608/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12672/87490 (14%)]\tLoss: 0.06\n","Train Epoch: 4 [12736/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [12800/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [12864/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [12928/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [12992/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13056/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13120/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13184/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13248/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13312/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13376/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13440/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13504/87490 (15%)]\tLoss: 0.06\n","Train Epoch: 4 [13568/87490 (16%)]\tLoss: 0.06\n","Train Epoch: 4 [13632/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [13696/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [13760/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [13824/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [13888/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [13952/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14016/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14080/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14144/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14208/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14272/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14336/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14400/87490 (16%)]\tLoss: 0.05\n","Train Epoch: 4 [14464/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14528/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14592/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14656/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14720/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14784/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14848/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14912/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [14976/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15040/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15104/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15168/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15232/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15296/87490 (17%)]\tLoss: 0.05\n","Train Epoch: 4 [15360/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15424/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15488/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15552/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15616/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15680/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15744/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15808/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15872/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [15936/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [16000/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [16064/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [16128/87490 (18%)]\tLoss: 0.05\n","Train Epoch: 4 [16192/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16256/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16320/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16384/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16448/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16512/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16576/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16640/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16704/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16768/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16832/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16896/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [16960/87490 (19%)]\tLoss: 0.05\n","Train Epoch: 4 [17024/87490 (19%)]\tLoss: 0.04\n","Train Epoch: 4 [17088/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17152/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17216/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17280/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17344/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17408/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17472/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17536/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17600/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17664/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17728/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17792/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17856/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17920/87490 (20%)]\tLoss: 0.04\n","Train Epoch: 4 [17984/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18048/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18112/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18176/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18240/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18304/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18368/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18432/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18496/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18560/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18624/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18688/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18752/87490 (21%)]\tLoss: 0.04\n","Train Epoch: 4 [18816/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [18880/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [18944/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19008/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19072/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19136/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19200/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19264/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19328/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19392/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19456/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19520/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19584/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19648/87490 (22%)]\tLoss: 0.04\n","Train Epoch: 4 [19712/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [19776/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [19840/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [19904/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [19968/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20032/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20096/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20160/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20224/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20288/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20352/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20416/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20480/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20544/87490 (23%)]\tLoss: 0.04\n","Train Epoch: 4 [20608/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20672/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20736/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20800/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20864/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20928/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [20992/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21056/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21120/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21184/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21248/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21312/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21376/87490 (24%)]\tLoss: 0.04\n","Train Epoch: 4 [21440/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21504/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21568/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21632/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21696/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21760/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21824/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21888/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [21952/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22016/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22080/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22144/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22208/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22272/87490 (25%)]\tLoss: 0.04\n","Train Epoch: 4 [22336/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22400/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22464/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22528/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22592/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22656/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22720/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22784/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22848/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22912/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [22976/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [23040/87490 (26%)]\tLoss: 0.04\n","Train Epoch: 4 [23104/87490 (26%)]\tLoss: 0.03\n","Train Epoch: 4 [23168/87490 (26%)]\tLoss: 0.03\n","Train Epoch: 4 [23232/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23296/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23360/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23424/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23488/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23552/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23616/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23680/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23744/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23808/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23872/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [23936/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [24000/87490 (27%)]\tLoss: 0.03\n","Train Epoch: 4 [24064/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24128/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24192/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24256/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24320/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24384/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24448/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24512/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24576/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24640/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24704/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24768/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24832/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24896/87490 (28%)]\tLoss: 0.03\n","Train Epoch: 4 [24960/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25024/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25088/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25152/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25216/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25280/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25344/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25408/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25472/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25536/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25600/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25664/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25728/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25792/87490 (29%)]\tLoss: 0.03\n","Train Epoch: 4 [25856/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [25920/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [25984/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26048/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26112/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26176/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26240/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26304/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26368/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26432/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26496/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26560/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26624/87490 (30%)]\tLoss: 0.03\n","Train Epoch: 4 [26688/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [26752/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [26816/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [26880/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [26944/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27008/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27072/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27136/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27200/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27264/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27328/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27392/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27456/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27520/87490 (31%)]\tLoss: 0.03\n","Train Epoch: 4 [27584/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27648/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27712/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27776/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27840/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27904/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [27968/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28032/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28096/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28160/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28224/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28288/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28352/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28416/87490 (32%)]\tLoss: 0.03\n","Train Epoch: 4 [28480/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28544/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28608/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28672/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28736/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28800/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28864/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28928/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [28992/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [29056/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [29120/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [29184/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [29248/87490 (33%)]\tLoss: 0.03\n","Train Epoch: 4 [29312/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29376/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29440/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29504/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29568/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29632/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29696/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29760/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29824/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29888/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [29952/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [30016/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [30080/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [30144/87490 (34%)]\tLoss: 0.03\n","Train Epoch: 4 [30208/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30272/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30336/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30400/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30464/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30528/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30592/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30656/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30720/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30784/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30848/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30912/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [30976/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [31040/87490 (35%)]\tLoss: 0.03\n","Train Epoch: 4 [31104/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31168/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31232/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31296/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31360/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31424/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31488/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31552/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31616/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31680/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31744/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31808/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31872/87490 (36%)]\tLoss: 0.03\n","Train Epoch: 4 [31936/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32000/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32064/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32128/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32192/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32256/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32320/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32384/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32448/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32512/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32576/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32640/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32704/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32768/87490 (37%)]\tLoss: 0.03\n","Train Epoch: 4 [32832/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [32896/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [32960/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33024/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33088/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33152/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33216/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33280/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33344/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33408/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33472/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33536/87490 (38%)]\tLoss: 0.03\n","Train Epoch: 4 [33600/87490 (38%)]\tLoss: 0.02\n","Train Epoch: 4 [33664/87490 (38%)]\tLoss: 0.02\n","Train Epoch: 4 [33728/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [33792/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [33856/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [33920/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [33984/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34048/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34112/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34176/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34240/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34304/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34368/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34432/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34496/87490 (39%)]\tLoss: 0.02\n","Train Epoch: 4 [34560/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34624/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34688/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34752/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34816/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34880/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [34944/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35008/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35072/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35136/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35200/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35264/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35328/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35392/87490 (40%)]\tLoss: 0.02\n","Train Epoch: 4 [35456/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35520/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35584/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35648/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35712/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35776/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35840/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35904/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [35968/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36032/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36096/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36160/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36224/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36288/87490 (41%)]\tLoss: 0.02\n","Train Epoch: 4 [36352/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36416/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36480/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36544/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36608/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36672/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36736/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36800/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36864/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36928/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [36992/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [37056/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [37120/87490 (42%)]\tLoss: 0.02\n","Train Epoch: 4 [37184/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37248/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37312/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37376/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37440/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37504/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37568/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37632/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37696/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37760/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37824/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37888/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [37952/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [38016/87490 (43%)]\tLoss: 0.02\n","Train Epoch: 4 [38080/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38144/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38208/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38272/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38336/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38400/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38464/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38528/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38592/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38656/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38720/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38784/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38848/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38912/87490 (44%)]\tLoss: 0.02\n","Train Epoch: 4 [38976/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39040/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39104/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39168/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39232/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39296/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39360/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39424/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39488/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39552/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39616/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39680/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39744/87490 (45%)]\tLoss: 0.02\n","Train Epoch: 4 [39808/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [39872/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [39936/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40000/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40064/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40128/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40192/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40256/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40320/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40384/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40448/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40512/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40576/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40640/87490 (46%)]\tLoss: 0.02\n","Train Epoch: 4 [40704/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [40768/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [40832/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [40896/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [40960/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41024/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41088/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41152/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41216/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41280/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41344/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41408/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41472/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41536/87490 (47%)]\tLoss: 0.02\n","Train Epoch: 4 [41600/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41664/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41728/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41792/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41856/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41920/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [41984/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42048/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42112/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42176/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42240/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42304/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42368/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42432/87490 (48%)]\tLoss: 0.02\n","Train Epoch: 4 [42496/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42560/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42624/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42688/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42752/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42816/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42880/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [42944/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43008/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43072/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43136/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43200/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43264/87490 (49%)]\tLoss: 0.02\n","Train Epoch: 4 [43328/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43392/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43456/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43520/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43584/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43648/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43712/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43776/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43840/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43904/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [43968/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [44032/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [44096/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [44160/87490 (50%)]\tLoss: 0.02\n","Train Epoch: 4 [44224/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44288/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44352/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44416/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44480/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44544/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44608/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44672/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44736/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44800/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44864/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44928/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [44992/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [45056/87490 (51%)]\tLoss: 0.02\n","Train Epoch: 4 [45120/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45184/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45248/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45312/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45376/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45440/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45504/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45568/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45632/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45696/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45760/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45824/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45888/87490 (52%)]\tLoss: 0.02\n","Train Epoch: 4 [45952/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46016/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46080/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46144/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46208/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46272/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46336/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46400/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46464/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46528/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46592/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46656/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46720/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46784/87490 (53%)]\tLoss: 0.02\n","Train Epoch: 4 [46848/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [46912/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [46976/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47040/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47104/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47168/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47232/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47296/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47360/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47424/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47488/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47552/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47616/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47680/87490 (54%)]\tLoss: 0.02\n","Train Epoch: 4 [47744/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [47808/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [47872/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [47936/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48000/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48064/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48128/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48192/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48256/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48320/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48384/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48448/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48512/87490 (55%)]\tLoss: 0.02\n","Train Epoch: 4 [48576/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48640/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48704/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48768/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48832/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48896/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [48960/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49024/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49088/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49152/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49216/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49280/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49344/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49408/87490 (56%)]\tLoss: 0.02\n","Train Epoch: 4 [49472/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49536/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49600/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49664/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49728/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49792/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49856/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49920/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [49984/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50048/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50112/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50176/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50240/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50304/87490 (57%)]\tLoss: 0.02\n","Train Epoch: 4 [50368/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50432/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50496/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50560/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50624/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50688/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50752/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50816/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50880/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [50944/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [51008/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [51072/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [51136/87490 (58%)]\tLoss: 0.02\n","Train Epoch: 4 [51200/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51264/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51328/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51392/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51456/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51520/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51584/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51648/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51712/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51776/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51840/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51904/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [51968/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [52032/87490 (59%)]\tLoss: 0.02\n","Train Epoch: 4 [52096/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52160/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52224/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52288/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52352/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52416/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52480/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52544/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52608/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52672/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52736/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52800/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52864/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52928/87490 (60%)]\tLoss: 0.02\n","Train Epoch: 4 [52992/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53056/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53120/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53184/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53248/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53312/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53376/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53440/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53504/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53568/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53632/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53696/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53760/87490 (61%)]\tLoss: 0.02\n","Train Epoch: 4 [53824/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [53888/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [53952/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54016/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54080/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54144/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54208/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54272/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54336/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54400/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54464/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54528/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54592/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54656/87490 (62%)]\tLoss: 0.02\n","Train Epoch: 4 [54720/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [54784/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [54848/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [54912/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [54976/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55040/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55104/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55168/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55232/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55296/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55360/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55424/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55488/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55552/87490 (63%)]\tLoss: 0.02\n","Train Epoch: 4 [55616/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [55680/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [55744/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [55808/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [55872/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [55936/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56000/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56064/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56128/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56192/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56256/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56320/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56384/87490 (64%)]\tLoss: 0.02\n","Train Epoch: 4 [56448/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56512/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56576/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56640/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56704/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56768/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56832/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56896/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [56960/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57024/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57088/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57152/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57216/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57280/87490 (65%)]\tLoss: 0.02\n","Train Epoch: 4 [57344/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57408/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57472/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57536/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57600/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57664/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57728/87490 (66%)]\tLoss: 0.02\n","Train Epoch: 4 [57792/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [57856/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [57920/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [57984/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [58048/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [58112/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [58176/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 4 [58240/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58304/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58368/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58432/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58496/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58560/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58624/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58688/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58752/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58816/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58880/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [58944/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [59008/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 4 [59072/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59136/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59200/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59264/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59328/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59392/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59456/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59520/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59584/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59648/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59712/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59776/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59840/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59904/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 4 [59968/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60032/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60096/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60160/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60224/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60288/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60352/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60416/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60480/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60544/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60608/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60672/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60736/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60800/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 4 [60864/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [60928/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [60992/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61056/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61120/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61184/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61248/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61312/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61376/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61440/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61504/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61568/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61632/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 4 [61696/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [61760/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [61824/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [61888/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [61952/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62016/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62080/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62144/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62208/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62272/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62336/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62400/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62464/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62528/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 4 [62592/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62656/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62720/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62784/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62848/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62912/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [62976/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63040/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63104/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63168/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63232/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63296/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63360/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63424/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 4 [63488/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63552/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63616/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63680/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63744/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63808/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63872/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [63936/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64000/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64064/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64128/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64192/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64256/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 4 [64320/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64384/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64448/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64512/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64576/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64640/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64704/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64768/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64832/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64896/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [64960/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [65024/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [65088/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [65152/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 4 [65216/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65280/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65344/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65408/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65472/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65536/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65600/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65664/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65728/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65792/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65856/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65920/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [65984/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [66048/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 4 [66112/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66176/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66240/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66304/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66368/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66432/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66496/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66560/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66624/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66688/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66752/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66816/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66880/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 4 [66944/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67008/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67072/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67136/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67200/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67264/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67328/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67392/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67456/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67520/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67584/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67648/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67712/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67776/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 4 [67840/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [67904/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [67968/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68032/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68096/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68160/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68224/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68288/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68352/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68416/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68480/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68544/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68608/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68672/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 4 [68736/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [68800/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [68864/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [68928/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [68992/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69056/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69120/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69184/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69248/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69312/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69376/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69440/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69504/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 4 [69568/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69632/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69696/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69760/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69824/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69888/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [69952/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70016/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70080/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70144/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70208/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70272/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70336/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70400/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 4 [70464/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70528/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70592/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70656/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70720/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70784/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70848/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70912/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [70976/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71040/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71104/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71168/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71232/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71296/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 4 [71360/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71424/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71488/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71552/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71616/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71680/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71744/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71808/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71872/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [71936/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [72000/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [72064/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [72128/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 4 [72192/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72256/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72320/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72384/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72448/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72512/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72576/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72640/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72704/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72768/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72832/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72896/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [72960/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [73024/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 4 [73088/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73152/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73216/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73280/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73344/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73408/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73472/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73536/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73600/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73664/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73728/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73792/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73856/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73920/87490 (84%)]\tLoss: 0.01\n","Train Epoch: 4 [73984/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74048/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74112/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74176/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74240/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74304/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74368/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74432/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74496/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74560/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74624/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74688/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74752/87490 (85%)]\tLoss: 0.01\n","Train Epoch: 4 [74816/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [74880/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [74944/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75008/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75072/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75136/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75200/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75264/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75328/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75392/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75456/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75520/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75584/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75648/87490 (86%)]\tLoss: 0.01\n","Train Epoch: 4 [75712/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [75776/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [75840/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [75904/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [75968/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76032/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76096/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76160/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76224/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76288/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76352/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76416/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76480/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76544/87490 (87%)]\tLoss: 0.01\n","Train Epoch: 4 [76608/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76672/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76736/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76800/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76864/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76928/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [76992/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77056/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77120/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77184/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77248/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77312/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77376/87490 (88%)]\tLoss: 0.01\n","Train Epoch: 4 [77440/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77504/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77568/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77632/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77696/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77760/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77824/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77888/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [77952/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78016/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78080/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78144/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78208/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78272/87490 (89%)]\tLoss: 0.01\n","Train Epoch: 4 [78336/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78400/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78464/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78528/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78592/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78656/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78720/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78784/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78848/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78912/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [78976/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [79040/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [79104/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [79168/87490 (90%)]\tLoss: 0.01\n","Train Epoch: 4 [79232/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79296/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79360/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79424/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79488/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79552/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79616/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79680/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79744/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79808/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79872/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [79936/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [80000/87490 (91%)]\tLoss: 0.01\n","Train Epoch: 4 [80064/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80128/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80192/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80256/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80320/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80384/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80448/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80512/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80576/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80640/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80704/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80768/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80832/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80896/87490 (92%)]\tLoss: 0.01\n","Train Epoch: 4 [80960/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81024/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81088/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81152/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81216/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81280/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81344/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81408/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81472/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81536/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81600/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81664/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81728/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81792/87490 (93%)]\tLoss: 0.01\n","Train Epoch: 4 [81856/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [81920/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [81984/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82048/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82112/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82176/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82240/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82304/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82368/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82432/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82496/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82560/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82624/87490 (94%)]\tLoss: 0.01\n","Train Epoch: 4 [82688/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [82752/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [82816/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [82880/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [82944/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83008/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83072/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83136/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83200/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83264/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83328/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83392/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83456/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83520/87490 (95%)]\tLoss: 0.01\n","Train Epoch: 4 [83584/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83648/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83712/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83776/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83840/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83904/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [83968/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84032/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84096/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84160/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84224/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84288/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84352/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84416/87490 (96%)]\tLoss: 0.01\n","Train Epoch: 4 [84480/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84544/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84608/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84672/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84736/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84800/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84864/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84928/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [84992/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [85056/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [85120/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [85184/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [85248/87490 (97%)]\tLoss: 0.01\n","Train Epoch: 4 [85312/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85376/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85440/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85504/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85568/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85632/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85696/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85760/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85824/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85888/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [85952/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [86016/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [86080/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [86144/87490 (98%)]\tLoss: 0.01\n","Train Epoch: 4 [86208/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86272/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86336/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86400/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86464/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86528/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86592/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86656/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86720/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86784/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86848/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86912/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [86976/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [87040/87490 (99%)]\tLoss: 0.01\n","Train Epoch: 4 [87104/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87168/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87232/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87296/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87360/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87424/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [87488/87490 (100%)]\tLoss: 0.01\n","Train Epoch: 4 [2736/87490 (3%)]\tLoss: 0.02\n","evaluating trained model ...\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 64/64 (100%) ; correctly guess bot: 64/64 (100%)\n","\n","Test set accuracy: correctly guess human: 55/64 (86%) ; correctly guess bot: 55/64 (86%)\n","Train Epoch: 5 [64/87490 (0%)]\tLoss: 0.00\n","Train Epoch: 5 [128/87490 (0%)]\tLoss: 0.00\n","Train Epoch: 5 [192/87490 (0%)]\tLoss: 0.40\n","Train Epoch: 5 [256/87490 (0%)]\tLoss: 0.63\n","Train Epoch: 5 [320/87490 (0%)]\tLoss: 0.71\n","Train Epoch: 5 [384/87490 (0%)]\tLoss: 0.65\n","Train Epoch: 5 [448/87490 (1%)]\tLoss: 0.59\n","Train Epoch: 5 [512/87490 (1%)]\tLoss: 0.52\n","Train Epoch: 5 [576/87490 (1%)]\tLoss: 0.46\n","Train Epoch: 5 [640/87490 (1%)]\tLoss: 0.42\n","Train Epoch: 5 [704/87490 (1%)]\tLoss: 0.39\n","Train Epoch: 5 [768/87490 (1%)]\tLoss: 0.35\n","Train Epoch: 5 [832/87490 (1%)]\tLoss: 0.33\n","Train Epoch: 5 [896/87490 (1%)]\tLoss: 0.31\n","Train Epoch: 5 [960/87490 (1%)]\tLoss: 0.29\n","Train Epoch: 5 [1024/87490 (1%)]\tLoss: 0.27\n","Train Epoch: 5 [1088/87490 (1%)]\tLoss: 0.26\n","Train Epoch: 5 [1152/87490 (1%)]\tLoss: 0.24\n","Train Epoch: 5 [1216/87490 (1%)]\tLoss: 0.23\n","Train Epoch: 5 [1280/87490 (1%)]\tLoss: 0.22\n","Train Epoch: 5 [1344/87490 (2%)]\tLoss: 0.21\n","Train Epoch: 5 [1408/87490 (2%)]\tLoss: 0.20\n","Train Epoch: 5 [1472/87490 (2%)]\tLoss: 0.19\n","Train Epoch: 5 [1536/87490 (2%)]\tLoss: 0.18\n","Train Epoch: 5 [1600/87490 (2%)]\tLoss: 0.18\n","Train Epoch: 5 [1664/87490 (2%)]\tLoss: 0.17\n","Train Epoch: 5 [1728/87490 (2%)]\tLoss: 0.16\n","Train Epoch: 5 [1792/87490 (2%)]\tLoss: 0.16\n","Train Epoch: 5 [1856/87490 (2%)]\tLoss: 0.15\n","Train Epoch: 5 [1920/87490 (2%)]\tLoss: 0.15\n","Train Epoch: 5 [1984/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 5 [2048/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 5 [2112/87490 (2%)]\tLoss: 0.14\n","Train Epoch: 5 [2176/87490 (2%)]\tLoss: 0.13\n","Train Epoch: 5 [2240/87490 (3%)]\tLoss: 0.13\n","Train Epoch: 5 [2304/87490 (3%)]\tLoss: 0.12\n","Train Epoch: 5 [2368/87490 (3%)]\tLoss: 0.12\n","Train Epoch: 5 [2432/87490 (3%)]\tLoss: 0.12\n","Train Epoch: 5 [2496/87490 (3%)]\tLoss: 0.11\n","Train Epoch: 5 [2560/87490 (3%)]\tLoss: 0.11\n","Train Epoch: 5 [2624/87490 (3%)]\tLoss: 0.11\n","Train Epoch: 5 [2688/87490 (3%)]\tLoss: 0.11\n","Train Epoch: 5 [2752/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 5 [2816/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 5 [2880/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 5 [2944/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 5 [3008/87490 (3%)]\tLoss: 0.10\n","Train Epoch: 5 [3072/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3136/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3200/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3264/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3328/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3392/87490 (4%)]\tLoss: 0.09\n","Train Epoch: 5 [3456/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3520/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3584/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3648/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3712/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3776/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3840/87490 (4%)]\tLoss: 0.08\n","Train Epoch: 5 [3904/87490 (4%)]\tLoss: 0.07\n","Train Epoch: 5 [3968/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4032/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4096/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4160/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4224/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4288/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4352/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4416/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4480/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4544/87490 (5%)]\tLoss: 0.06\n","Train Epoch: 5 [4608/87490 (5%)]\tLoss: 0.06\n","Train Epoch: 5 [4672/87490 (5%)]\tLoss: 0.06\n","Train Epoch: 5 [4736/87490 (5%)]\tLoss: 0.06\n","Train Epoch: 5 [4800/87490 (5%)]\tLoss: 0.07\n","Train Epoch: 5 [4864/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [4928/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [4992/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5056/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5120/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5184/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5248/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5312/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5376/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5440/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5504/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5568/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5632/87490 (6%)]\tLoss: 0.06\n","Train Epoch: 5 [5696/87490 (7%)]\tLoss: 0.06\n","Train Epoch: 5 [5760/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [5824/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [5888/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [5952/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6016/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6080/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6144/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6208/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6272/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6336/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6400/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6464/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6528/87490 (7%)]\tLoss: 0.05\n","Train Epoch: 5 [6592/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6656/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6720/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6784/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6848/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6912/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [6976/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [7040/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [7104/87490 (8%)]\tLoss: 0.05\n","Train Epoch: 5 [7168/87490 (8%)]\tLoss: 0.04\n","Train Epoch: 5 [7232/87490 (8%)]\tLoss: 0.04\n","Train Epoch: 5 [7296/87490 (8%)]\tLoss: 0.04\n","Train Epoch: 5 [7360/87490 (8%)]\tLoss: 0.04\n","Train Epoch: 5 [7424/87490 (8%)]\tLoss: 0.04\n","Train Epoch: 5 [7488/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7552/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7616/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7680/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7744/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7808/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7872/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [7936/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8000/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8064/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8128/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8192/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8256/87490 (9%)]\tLoss: 0.04\n","Train Epoch: 5 [8320/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8384/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8448/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8512/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8576/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8640/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8704/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8768/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8832/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8896/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [8960/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [9024/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [9088/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [9152/87490 (10%)]\tLoss: 0.04\n","Train Epoch: 5 [9216/87490 (11%)]\tLoss: 0.04\n","Train Epoch: 5 [9280/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9344/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9408/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9472/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9536/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9600/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9664/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9728/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9792/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9856/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9920/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [9984/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [10048/87490 (11%)]\tLoss: 0.03\n","Train Epoch: 5 [10112/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10176/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10240/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10304/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10368/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10432/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10496/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10560/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10624/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10688/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10752/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10816/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10880/87490 (12%)]\tLoss: 0.03\n","Train Epoch: 5 [10944/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11008/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11072/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11136/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11200/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11264/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11328/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11392/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11456/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11520/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11584/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11648/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11712/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11776/87490 (13%)]\tLoss: 0.03\n","Train Epoch: 5 [11840/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [11904/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [11968/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12032/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12096/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12160/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12224/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12288/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12352/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12416/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12480/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12544/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12608/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12672/87490 (14%)]\tLoss: 0.03\n","Train Epoch: 5 [12736/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [12800/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [12864/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [12928/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [12992/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [13056/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [13120/87490 (15%)]\tLoss: 0.03\n","Train Epoch: 5 [13184/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13248/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13312/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13376/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13440/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13504/87490 (15%)]\tLoss: 0.02\n","Train Epoch: 5 [13568/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13632/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13696/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13760/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13824/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13888/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [13952/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14016/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14080/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14144/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14208/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14272/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14336/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14400/87490 (16%)]\tLoss: 0.02\n","Train Epoch: 5 [14464/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14528/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14592/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14656/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14720/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14784/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14848/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14912/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [14976/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15040/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15104/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15168/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15232/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15296/87490 (17%)]\tLoss: 0.02\n","Train Epoch: 5 [15360/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15424/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15488/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15552/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15616/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15680/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15744/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15808/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15872/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [15936/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [16000/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [16064/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [16128/87490 (18%)]\tLoss: 0.02\n","Train Epoch: 5 [16192/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16256/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16320/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16384/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16448/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16512/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16576/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16640/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16704/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16768/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16832/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16896/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [16960/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [17024/87490 (19%)]\tLoss: 0.02\n","Train Epoch: 5 [17088/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17152/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17216/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17280/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17344/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17408/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17472/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17536/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17600/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17664/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17728/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17792/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17856/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17920/87490 (20%)]\tLoss: 0.02\n","Train Epoch: 5 [17984/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18048/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18112/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18176/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18240/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18304/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18368/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18432/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18496/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18560/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18624/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18688/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18752/87490 (21%)]\tLoss: 0.02\n","Train Epoch: 5 [18816/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [18880/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [18944/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19008/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19072/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19136/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19200/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19264/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19328/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19392/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19456/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19520/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19584/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19648/87490 (22%)]\tLoss: 0.02\n","Train Epoch: 5 [19712/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [19776/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [19840/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [19904/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [19968/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20032/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20096/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20160/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20224/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20288/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20352/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20416/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20480/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20544/87490 (23%)]\tLoss: 0.02\n","Train Epoch: 5 [20608/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20672/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20736/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20800/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20864/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20928/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [20992/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21056/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21120/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21184/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21248/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21312/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21376/87490 (24%)]\tLoss: 0.02\n","Train Epoch: 5 [21440/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21504/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21568/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21632/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21696/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21760/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21824/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21888/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [21952/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22016/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22080/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22144/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22208/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22272/87490 (25%)]\tLoss: 0.02\n","Train Epoch: 5 [22336/87490 (26%)]\tLoss: 0.02\n","Train Epoch: 5 [22400/87490 (26%)]\tLoss: 0.02\n","Train Epoch: 5 [22464/87490 (26%)]\tLoss: 0.02\n","Train Epoch: 5 [22528/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22592/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22656/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22720/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22784/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22848/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22912/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [22976/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [23040/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [23104/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [23168/87490 (26%)]\tLoss: 0.01\n","Train Epoch: 5 [23232/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23296/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23360/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23424/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23488/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23552/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23616/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23680/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23744/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23808/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23872/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [23936/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [24000/87490 (27%)]\tLoss: 0.01\n","Train Epoch: 5 [24064/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24128/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24192/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24256/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24320/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24384/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24448/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24512/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24576/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24640/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24704/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24768/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24832/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24896/87490 (28%)]\tLoss: 0.01\n","Train Epoch: 5 [24960/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25024/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25088/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25152/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25216/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25280/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25344/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25408/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25472/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25536/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25600/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25664/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25728/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25792/87490 (29%)]\tLoss: 0.01\n","Train Epoch: 5 [25856/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [25920/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [25984/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26048/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26112/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26176/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26240/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26304/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26368/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26432/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26496/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26560/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26624/87490 (30%)]\tLoss: 0.01\n","Train Epoch: 5 [26688/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [26752/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [26816/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [26880/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [26944/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27008/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27072/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27136/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27200/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27264/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27328/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27392/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27456/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27520/87490 (31%)]\tLoss: 0.01\n","Train Epoch: 5 [27584/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27648/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27712/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27776/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27840/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27904/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [27968/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28032/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28096/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28160/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28224/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28288/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28352/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28416/87490 (32%)]\tLoss: 0.01\n","Train Epoch: 5 [28480/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28544/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28608/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28672/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28736/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28800/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28864/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28928/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [28992/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [29056/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [29120/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [29184/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [29248/87490 (33%)]\tLoss: 0.01\n","Train Epoch: 5 [29312/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29376/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29440/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29504/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29568/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29632/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29696/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29760/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29824/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29888/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [29952/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [30016/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [30080/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [30144/87490 (34%)]\tLoss: 0.01\n","Train Epoch: 5 [30208/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30272/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30336/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30400/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30464/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30528/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30592/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30656/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30720/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30784/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30848/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30912/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [30976/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [31040/87490 (35%)]\tLoss: 0.01\n","Train Epoch: 5 [31104/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31168/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31232/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31296/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31360/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31424/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31488/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31552/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31616/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31680/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31744/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31808/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31872/87490 (36%)]\tLoss: 0.01\n","Train Epoch: 5 [31936/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32000/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32064/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32128/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32192/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32256/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32320/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32384/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32448/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32512/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32576/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32640/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32704/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32768/87490 (37%)]\tLoss: 0.01\n","Train Epoch: 5 [32832/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [32896/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [32960/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33024/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33088/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33152/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33216/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33280/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33344/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33408/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33472/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33536/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33600/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33664/87490 (38%)]\tLoss: 0.01\n","Train Epoch: 5 [33728/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [33792/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [33856/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [33920/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [33984/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34048/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34112/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34176/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34240/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34304/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34368/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34432/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34496/87490 (39%)]\tLoss: 0.01\n","Train Epoch: 5 [34560/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34624/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34688/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34752/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34816/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34880/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [34944/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35008/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35072/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35136/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35200/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35264/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35328/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35392/87490 (40%)]\tLoss: 0.01\n","Train Epoch: 5 [35456/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35520/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35584/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35648/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35712/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35776/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35840/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35904/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [35968/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36032/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36096/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36160/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36224/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36288/87490 (41%)]\tLoss: 0.01\n","Train Epoch: 5 [36352/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36416/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36480/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36544/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36608/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36672/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36736/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36800/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36864/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36928/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [36992/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [37056/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [37120/87490 (42%)]\tLoss: 0.01\n","Train Epoch: 5 [37184/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37248/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37312/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37376/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37440/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37504/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37568/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37632/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37696/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37760/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37824/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37888/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [37952/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [38016/87490 (43%)]\tLoss: 0.01\n","Train Epoch: 5 [38080/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38144/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38208/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38272/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38336/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38400/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38464/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38528/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38592/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38656/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38720/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38784/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38848/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38912/87490 (44%)]\tLoss: 0.01\n","Train Epoch: 5 [38976/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39040/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39104/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39168/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39232/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39296/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39360/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39424/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39488/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39552/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39616/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39680/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39744/87490 (45%)]\tLoss: 0.01\n","Train Epoch: 5 [39808/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [39872/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [39936/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40000/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40064/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40128/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40192/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40256/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40320/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40384/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40448/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40512/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40576/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40640/87490 (46%)]\tLoss: 0.01\n","Train Epoch: 5 [40704/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [40768/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [40832/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [40896/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [40960/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41024/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41088/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41152/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41216/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41280/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41344/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41408/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41472/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41536/87490 (47%)]\tLoss: 0.01\n","Train Epoch: 5 [41600/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41664/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41728/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41792/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41856/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41920/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [41984/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42048/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42112/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42176/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42240/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42304/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42368/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42432/87490 (48%)]\tLoss: 0.01\n","Train Epoch: 5 [42496/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42560/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42624/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42688/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42752/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42816/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42880/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [42944/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43008/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43072/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43136/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43200/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43264/87490 (49%)]\tLoss: 0.01\n","Train Epoch: 5 [43328/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43392/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43456/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43520/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43584/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43648/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43712/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43776/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43840/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43904/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [43968/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [44032/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [44096/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [44160/87490 (50%)]\tLoss: 0.01\n","Train Epoch: 5 [44224/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44288/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44352/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44416/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44480/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44544/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44608/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44672/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44736/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44800/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44864/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44928/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [44992/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [45056/87490 (51%)]\tLoss: 0.01\n","Train Epoch: 5 [45120/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45184/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45248/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45312/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45376/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45440/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45504/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45568/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45632/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45696/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45760/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45824/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45888/87490 (52%)]\tLoss: 0.01\n","Train Epoch: 5 [45952/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46016/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46080/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46144/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46208/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46272/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46336/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46400/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46464/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46528/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46592/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46656/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46720/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46784/87490 (53%)]\tLoss: 0.01\n","Train Epoch: 5 [46848/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [46912/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [46976/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47040/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47104/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47168/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47232/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47296/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47360/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47424/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47488/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47552/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47616/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47680/87490 (54%)]\tLoss: 0.01\n","Train Epoch: 5 [47744/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [47808/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [47872/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [47936/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48000/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48064/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48128/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48192/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48256/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48320/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48384/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48448/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48512/87490 (55%)]\tLoss: 0.01\n","Train Epoch: 5 [48576/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48640/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48704/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48768/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48832/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48896/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [48960/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49024/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49088/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49152/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49216/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49280/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49344/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49408/87490 (56%)]\tLoss: 0.01\n","Train Epoch: 5 [49472/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49536/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49600/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49664/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49728/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49792/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49856/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49920/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [49984/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50048/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50112/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50176/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50240/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50304/87490 (57%)]\tLoss: 0.01\n","Train Epoch: 5 [50368/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50432/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50496/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50560/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50624/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50688/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50752/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50816/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50880/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [50944/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [51008/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [51072/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [51136/87490 (58%)]\tLoss: 0.01\n","Train Epoch: 5 [51200/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51264/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51328/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51392/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51456/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51520/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51584/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51648/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51712/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51776/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51840/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51904/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [51968/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [52032/87490 (59%)]\tLoss: 0.01\n","Train Epoch: 5 [52096/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52160/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52224/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52288/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52352/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52416/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52480/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52544/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52608/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52672/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52736/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52800/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52864/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52928/87490 (60%)]\tLoss: 0.01\n","Train Epoch: 5 [52992/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53056/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53120/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53184/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53248/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53312/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53376/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53440/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53504/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53568/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53632/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53696/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53760/87490 (61%)]\tLoss: 0.01\n","Train Epoch: 5 [53824/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [53888/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [53952/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54016/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54080/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54144/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54208/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54272/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54336/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54400/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54464/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54528/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54592/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54656/87490 (62%)]\tLoss: 0.01\n","Train Epoch: 5 [54720/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [54784/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [54848/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [54912/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [54976/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55040/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55104/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55168/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55232/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55296/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55360/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55424/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55488/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55552/87490 (63%)]\tLoss: 0.01\n","Train Epoch: 5 [55616/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [55680/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [55744/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [55808/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [55872/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [55936/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56000/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56064/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56128/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56192/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56256/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56320/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56384/87490 (64%)]\tLoss: 0.01\n","Train Epoch: 5 [56448/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56512/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56576/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56640/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56704/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56768/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56832/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56896/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [56960/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57024/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57088/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57152/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57216/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57280/87490 (65%)]\tLoss: 0.01\n","Train Epoch: 5 [57344/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57408/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57472/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57536/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57600/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57664/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57728/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57792/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57856/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57920/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [57984/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [58048/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [58112/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [58176/87490 (66%)]\tLoss: 0.01\n","Train Epoch: 5 [58240/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58304/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58368/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58432/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58496/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58560/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58624/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58688/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58752/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58816/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58880/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [58944/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [59008/87490 (67%)]\tLoss: 0.01\n","Train Epoch: 5 [59072/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59136/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59200/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59264/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59328/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59392/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59456/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59520/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59584/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59648/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59712/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59776/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59840/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59904/87490 (68%)]\tLoss: 0.01\n","Train Epoch: 5 [59968/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60032/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60096/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60160/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60224/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60288/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60352/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60416/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60480/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60544/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60608/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60672/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60736/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60800/87490 (69%)]\tLoss: 0.01\n","Train Epoch: 5 [60864/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [60928/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [60992/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61056/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61120/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61184/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61248/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61312/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61376/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61440/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61504/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61568/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61632/87490 (70%)]\tLoss: 0.01\n","Train Epoch: 5 [61696/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [61760/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [61824/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [61888/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [61952/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62016/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62080/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62144/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62208/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62272/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62336/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62400/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62464/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62528/87490 (71%)]\tLoss: 0.01\n","Train Epoch: 5 [62592/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62656/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62720/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62784/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62848/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62912/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [62976/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63040/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63104/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63168/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63232/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63296/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63360/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63424/87490 (72%)]\tLoss: 0.01\n","Train Epoch: 5 [63488/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63552/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63616/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63680/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63744/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63808/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63872/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [63936/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64000/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64064/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64128/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64192/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64256/87490 (73%)]\tLoss: 0.01\n","Train Epoch: 5 [64320/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64384/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64448/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64512/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64576/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64640/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64704/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64768/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64832/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64896/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [64960/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [65024/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [65088/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [65152/87490 (74%)]\tLoss: 0.01\n","Train Epoch: 5 [65216/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65280/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65344/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65408/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65472/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65536/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65600/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65664/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65728/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65792/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65856/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65920/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [65984/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [66048/87490 (75%)]\tLoss: 0.01\n","Train Epoch: 5 [66112/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66176/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66240/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66304/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66368/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66432/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66496/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66560/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66624/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66688/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66752/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66816/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66880/87490 (76%)]\tLoss: 0.01\n","Train Epoch: 5 [66944/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67008/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67072/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67136/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67200/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67264/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67328/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67392/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67456/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67520/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67584/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67648/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67712/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67776/87490 (77%)]\tLoss: 0.01\n","Train Epoch: 5 [67840/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [67904/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [67968/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68032/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68096/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68160/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68224/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68288/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68352/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68416/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68480/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68544/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68608/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68672/87490 (78%)]\tLoss: 0.01\n","Train Epoch: 5 [68736/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [68800/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [68864/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [68928/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [68992/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69056/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69120/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69184/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69248/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69312/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69376/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69440/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69504/87490 (79%)]\tLoss: 0.01\n","Train Epoch: 5 [69568/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69632/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69696/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69760/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69824/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69888/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [69952/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70016/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70080/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70144/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70208/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70272/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70336/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70400/87490 (80%)]\tLoss: 0.01\n","Train Epoch: 5 [70464/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70528/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70592/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70656/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70720/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70784/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70848/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70912/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [70976/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71040/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71104/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71168/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71232/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71296/87490 (81%)]\tLoss: 0.01\n","Train Epoch: 5 [71360/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71424/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71488/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71552/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71616/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71680/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71744/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71808/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71872/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [71936/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [72000/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [72064/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [72128/87490 (82%)]\tLoss: 0.01\n","Train Epoch: 5 [72192/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72256/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72320/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72384/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72448/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72512/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72576/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72640/87490 (83%)]\tLoss: 0.01\n","Train Epoch: 5 [72704/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [72768/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [72832/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [72896/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [72960/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [73024/87490 (83%)]\tLoss: 0.00\n","Train Epoch: 5 [73088/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73152/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73216/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73280/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73344/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73408/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73472/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73536/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73600/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73664/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73728/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73792/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73856/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73920/87490 (84%)]\tLoss: 0.00\n","Train Epoch: 5 [73984/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74048/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74112/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74176/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74240/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74304/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74368/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74432/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74496/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74560/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74624/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74688/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74752/87490 (85%)]\tLoss: 0.00\n","Train Epoch: 5 [74816/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [74880/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [74944/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75008/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75072/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75136/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75200/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75264/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75328/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75392/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75456/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75520/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75584/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75648/87490 (86%)]\tLoss: 0.00\n","Train Epoch: 5 [75712/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [75776/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [75840/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [75904/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [75968/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76032/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76096/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76160/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76224/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76288/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76352/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76416/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76480/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76544/87490 (87%)]\tLoss: 0.00\n","Train Epoch: 5 [76608/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76672/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76736/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76800/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76864/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76928/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [76992/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77056/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77120/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77184/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77248/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77312/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77376/87490 (88%)]\tLoss: 0.00\n","Train Epoch: 5 [77440/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77504/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77568/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77632/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77696/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77760/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77824/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77888/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [77952/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78016/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78080/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78144/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78208/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78272/87490 (89%)]\tLoss: 0.00\n","Train Epoch: 5 [78336/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78400/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78464/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78528/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78592/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78656/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78720/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78784/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78848/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78912/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [78976/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [79040/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [79104/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [79168/87490 (90%)]\tLoss: 0.00\n","Train Epoch: 5 [79232/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79296/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79360/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79424/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79488/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79552/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79616/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79680/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79744/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79808/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79872/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [79936/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [80000/87490 (91%)]\tLoss: 0.00\n","Train Epoch: 5 [80064/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80128/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80192/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80256/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80320/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80384/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80448/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80512/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80576/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80640/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80704/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80768/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80832/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80896/87490 (92%)]\tLoss: 0.00\n","Train Epoch: 5 [80960/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81024/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81088/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81152/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81216/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81280/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81344/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81408/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81472/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81536/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81600/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81664/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81728/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81792/87490 (93%)]\tLoss: 0.00\n","Train Epoch: 5 [81856/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [81920/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [81984/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82048/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82112/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82176/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82240/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82304/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82368/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82432/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82496/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82560/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82624/87490 (94%)]\tLoss: 0.00\n","Train Epoch: 5 [82688/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [82752/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [82816/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [82880/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [82944/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83008/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83072/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83136/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83200/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83264/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83328/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83392/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83456/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83520/87490 (95%)]\tLoss: 0.00\n","Train Epoch: 5 [83584/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83648/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83712/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83776/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83840/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83904/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [83968/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84032/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84096/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84160/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84224/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84288/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84352/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84416/87490 (96%)]\tLoss: 0.00\n","Train Epoch: 5 [84480/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84544/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84608/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84672/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84736/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84800/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84864/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84928/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [84992/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [85056/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [85120/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [85184/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [85248/87490 (97%)]\tLoss: 0.00\n","Train Epoch: 5 [85312/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85376/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85440/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85504/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85568/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85632/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85696/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85760/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85824/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85888/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [85952/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [86016/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [86080/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [86144/87490 (98%)]\tLoss: 0.00\n","Train Epoch: 5 [86208/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86272/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86336/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86400/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86464/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86528/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86592/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86656/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86720/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86784/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86848/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86912/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [86976/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [87040/87490 (99%)]\tLoss: 0.00\n","Train Epoch: 5 [87104/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87168/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87232/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87296/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87360/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87424/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [87488/87490 (100%)]\tLoss: 0.00\n","Train Epoch: 5 [2736/87490 (3%)]\tLoss: 0.01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VFmjf1FbZzy_","colab_type":"code","outputId":"e28328e2-b71b-4f47-edf8-054d3637c459","executionInfo":{"status":"ok","timestamp":1571959261802,"user_tz":-780,"elapsed":2889799,"user":{"displayName":"Chester Holt","photoUrl":"","userId":"05242144140561707601"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run.py train rl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4000_checkpoint.tar\n","Building encoder and decoder ...\n","Models built and ready to go!\n","Building optimizers ...\n","Optimizers built and ready to go!\n","Added valid_rare.json to dataset\n","0 pairs trimmed, 7913 remain\n","Added train.json to dataset\n","0 pairs trimmed, 134565 remain\n","Added test_freq.json to dataset\n","0 pairs trimmed, 142521 remain\n","Added valid_freq.json to dataset\n","0 pairs trimmed, 150238 remain\n","Added test_rare.json to dataset\n","0 pairs trimmed, 158155 remain\n","Initialising Environment...\n","50_epochs (1).tar\n","Building ADEM model ...\n","2_epochs.tar\n","Building Adversarial_Discriminator model ...\n","==============421==============\n","Training for 10000 episodes...\n","/content/gdrive/My Drive/RLChat/reinforcement_learning/train.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  state_tensor[idx, :seq_len] = torch.tensor(seq, device=device, dtype=torch.long)\n","Episode 100 completed, lasted 11 turns -- Total Reward : 0.06934411404654384 -- Average DQN Loss : 0.00011118319962406531\n","Episode 200 completed, lasted 11 turns -- Total Reward : 0.06824290950316936 -- Average DQN Loss : 0.00013314626703504474\n","Episode 300 completed, lasted 11 turns -- Total Reward : 0.06559310900047421 -- Average DQN Loss : 0.0001221777230966836\n","Episode 400 completed, lasted 11 turns -- Total Reward : 0.0070780194364488125 -- Average DQN Loss : 0.00017711424006847665\n","Episode 500 completed, lasted 11 turns -- Total Reward : 0.006237988825887442 -- Average DQN Loss : 0.00012172878487035633\n","Saving model state as 500 checkpoint\n","Episode 600 completed, lasted 11 turns -- Total Reward : 0.007918050047010183 -- Average DQN Loss : 0.00015187095850706102\n","Episode 700 completed, lasted 11 turns -- Total Reward : 0.06888998788781464 -- Average DQN Loss : 0.00012645266542676836\n","Episode 800 completed, lasted 11 turns -- Total Reward : 0.006517912144772708 -- Average DQN Loss : 0.00017319055914413183\n","Episode 900 completed, lasted 11 turns -- Total Reward : 0.034388191415928304 -- Average DQN Loss : 0.00012287299032323062\n","Episode 1000 completed, lasted 11 turns -- Total Reward : 0.06604723515920341 -- Average DQN Loss : 8.838972862577066e-05\n","Saving model state as 1000 checkpoint\n","Episode 1100 completed, lasted 11 turns -- Total Reward : 0.04266672790981829 -- Average DQN Loss : 0.00017015829216688872\n","Episode 1200 completed, lasted 11 turns -- Total Reward : 0.06690627580974251 -- Average DQN Loss : 8.077031816355884e-05\n","Episode 1300 completed, lasted 11 turns -- Total Reward : 0.004534200066700578 -- Average DQN Loss : 0.0001287918261368759\n","Episode 1400 completed, lasted 11 turns -- Total Reward : 0.03638619603589177 -- Average DQN Loss : 0.00016980418295133858\n","Episode 1500 completed, lasted 11 turns -- Total Reward : 0.06599802349228412 -- Average DQN Loss : 9.225961985066533e-05\n","Saving model state as 1500 checkpoint\n","Episode 1600 completed, lasted 11 turns -- Total Reward : 0.09914889372885227 -- Average DQN Loss : 9.015629475470633e-05\n","Episode 1700 completed, lasted 11 turns -- Total Reward : 0.035932069877162576 -- Average DQN Loss : 7.832986011635512e-05\n","Episode 1800 completed, lasted 11 turns -- Total Reward : 0.03768507030326873 -- Average DQN Loss : 8.899524109438062e-05\n","Episode 1900 completed, lasted 11 turns -- Total Reward : 0.03563313651829958 -- Average DQN Loss : 8.419094810960815e-05\n","Episode 2000 completed, lasted 11 turns -- Total Reward : 0.009515857673250139 -- Average DQN Loss : 6.682092498522252e-05\n","Saving model state as 2000 checkpoint\n","Episode 2100 completed, lasted 11 turns -- Total Reward : 0.036193243809975684 -- Average DQN Loss : 5.523279687622562e-05\n","Episode 2200 completed, lasted 11 turns -- Total Reward : 0.07113487389869988 -- Average DQN Loss : 5.93801356444601e-05\n","Episode 2300 completed, lasted 11 turns -- Total Reward : 0.006673104944638908 -- Average DQN Loss : 5.446223949547857e-05\n","Episode 2400 completed, lasted 11 turns -- Total Reward : 0.006517912144772708 -- Average DQN Loss : 3.669694960990455e-05\n","Episode 2500 completed, lasted 11 turns -- Total Reward : 0.006972038303501904 -- Average DQN Loss : 4.3565894156927246e-05\n","Saving model state as 2500 checkpoint\n","Episode 2600 completed, lasted 11 turns -- Total Reward : 0.06859105452895164 -- Average DQN Loss : 3.678476423374377e-05\n","Episode 2700 completed, lasted 11 turns -- Total Reward : 0.006779086077585816 -- Average DQN Loss : 5.569311979343183e-05\n","Episode 2800 completed, lasted 11 turns -- Total Reward : 0.006388464127667248 -- Average DQN Loss : 4.961576778441668e-05\n","Episode 2900 completed, lasted 11 turns -- Total Reward : 0.00940987654030323 -- Average DQN Loss : 3.7906040961388496e-05\n","Episode 3000 completed, lasted 11 turns -- Total Reward : 0.03787802252918482 -- Average DQN Loss : 4.122306272620335e-05\n","Saving model state as 3000 checkpoint\n","Episode 3100 completed, lasted 11 turns -- Total Reward : 0.006972038303501904 -- Average DQN Loss : 3.668628996820189e-05\n","Episode 3200 completed, lasted 11 turns -- Total Reward : 0.006218978785909712 -- Average DQN Loss : 3.700091110658832e-05\n","Episode 3300 completed, lasted 11 turns -- Total Reward : 0.0036751594161614776 -- Average DQN Loss : 4.4611385237658394e-05\n","Episode 3400 completed, lasted 11 turns -- Total Reward : 0.03712496301159263 -- Average DQN Loss : 3.271727400715463e-05\n","Episode 3500 completed, lasted 11 turns -- Total Reward : 0.07038181438110769 -- Average DQN Loss : 3.262889476900455e-05\n","Saving model state as 3500 checkpoint\n","Episode 3600 completed, lasted 11 turns -- Total Reward : 0.06794397614430636 -- Average DQN Loss : 3.014876347151585e-05\n","Episode 3700 completed, lasted 11 turns -- Total Reward : 0.06888998788781464 -- Average DQN Loss : 2.8001800819765778e-05\n","Episode 3800 completed, lasted 11 turns -- Total Reward : 0.03798400366213173 -- Average DQN Loss : 1.81827610504115e-05\n","Episode 3900 completed, lasted 11 turns -- Total Reward : 0.004727152292616665 -- Average DQN Loss : 2.553300219005905e-05\n","Episode 4000 completed, lasted 11 turns -- Total Reward : 0.011760743684135377 -- Average DQN Loss : 3.2502787144039756e-05\n","Saving model state as 4000 checkpoint\n","Episode 4100 completed, lasted 11 turns -- Total Reward : 0.03977476351428777 -- Average DQN Loss : 2.0339811453595757e-05\n","Episode 4200 completed, lasted 11 turns -- Total Reward : 0.03535321319941431 -- Average DQN Loss : 1.664141473156633e-05\n","Episode 4300 completed, lasted 11 turns -- Total Reward : 0.0663461685180664 -- Average DQN Loss : 2.3859632165113e-05\n","Episode 4400 completed, lasted 11 turns -- Total Reward : 0.03798400366213173 -- Average DQN Loss : 1.7832092635217122e-05\n","Episode 4500 completed, lasted 11 turns -- Total Reward : 0.0036751594161614776 -- Average DQN Loss : 2.148489984392654e-05\n","Saving model state as 4500 checkpoint\n","Episode 4600 completed, lasted 11 turns -- Total Reward : 0.005374230677261949 -- Average DQN Loss : 1.487727313360665e-05\n","Episode 4700 completed, lasted 11 turns -- Total Reward : 0.011760743684135377 -- Average DQN Loss : 1.3566582310886588e-05\n","Episode 4800 completed, lasted 11 turns -- Total Reward : 0.03843812982086092 -- Average DQN Loss : 1.0651536194927758e-05\n","Episode 4900 completed, lasted 11 turns -- Total Reward : 0.03798400366213173 -- Average DQN Loss : 1.9645688735181465e-05\n","Episode 5000 completed, lasted 11 turns -- Total Reward : 0.009061731514520943 -- Average DQN Loss : 1.587574697623495e-05\n","Saving model state as 5000 checkpoint\n","Episode 5100 completed, lasted 11 turns -- Total Reward : 0.03798400366213173 -- Average DQN Loss : 1.2837858957936987e-05\n","Episode 5200 completed, lasted 11 turns -- Total Reward : 0.06625919742509723 -- Average DQN Loss : 1.5011186769697816e-05\n","Episode 5300 completed, lasted 11 turns -- Total Reward : 0.0355461654253304 -- Average DQN Loss : 1.0474409373273375e-05\n","Episode 5400 completed, lasted 11 turns -- Total Reward : 0.03966878238134086 -- Average DQN Loss : 1.0747611850092653e-05\n","Episode 5500 completed, lasted 11 turns -- Total Reward : 0.03787802252918482 -- Average DQN Loss : 1.535768315079622e-05\n","Saving model state as 5500 checkpoint\n","Episode 5600 completed, lasted 11 turns -- Total Reward : 0.03563313651829958 -- Average DQN Loss : 1.9668280219775625e-05\n","Episode 5700 completed, lasted 11 turns -- Total Reward : 0.008868779288604856 -- Average DQN Loss : 8.731449634069577e-06\n","Episode 5800 completed, lasted 11 turns -- Total Reward : 0.06888998788781464 -- Average DQN Loss : 1.511117079644464e-05\n","Episode 5900 completed, lasted 11 turns -- Total Reward : 0.005480211810208857 -- Average DQN Loss : 1.625670520297717e-05\n","Episode 6000 completed, lasted 11 turns -- Total Reward : 0.037230944144539535 -- Average DQN Loss : 1.2666245675063692e-05\n","Saving model state as 6000 checkpoint\n","Episode 6100 completed, lasted 11 turns -- Total Reward : 0.004727152292616665 -- Average DQN Loss : 1.349811482214136e-05\n","Episode 6200 completed, lasted 11 turns -- Total Reward : 0.03768507030326873 -- Average DQN Loss : 1.2077288920409046e-05\n","Episode 6300 completed, lasted 11 turns -- Total Reward : 0.035141250933520496 -- Average DQN Loss : 1.091255780920619e-05\n"],"name":"stdout"}]}]}